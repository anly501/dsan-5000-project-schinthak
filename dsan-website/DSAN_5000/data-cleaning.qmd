---
title: Data Cleaning
---

```{css, echo=FALSE}
img{
  border: 3px solid #78c2ae; /* Minty color, adjust as needed */
  padding: 2px; /* Adjust padding as needed */
}
```

Prior to analyzing our data, we first need to clean it. Data cleaning plays a pivotal role in the realm of data science as it serves as the foundation for robust and reliable analyses. Raw data is often messy, meaning that it contains errors, inconsistencies, and missing values that can impede accurate insights and decision-making. The process of data cleaning involves identifying and rectifying these anomalies, ensuring that the dataset is accurate, consistent, and complete. By  addressing issues such as outliers, duplicates, and inaccuracies, data scientists enhance the quality of their analyses and models. Ultimately, the success of any data science endeavor hinges on the data cleaning phase, as it lays the groundwork for informed, trustworthy, and impactful data-driven conclusions.

## CDC Long Covid Survey
As we previously mentioned, this data originally comes from survey data sourced by the CDC. Initially, we can obtain the macros of the data as follows: 

---

```{r}
#| echo: false
#| warning: false
library(tidyverse)
library(stringr)
library(reticulate)
library(tidyverse)
library(readxl)
library(janitor)

use_python("/usr/local/bin/python3", require = T)
knitr::knit_engines$set(python = reticulate::eng_python)
```

```{r, echo=FALSE, fig.cap="Output in a Box"}

cdc <- read.csv("../../data/00-raw-data/cdc_raw.csv")
output_data <- glimpse(cdc)
```

---

From this description of the data, we can see issues with NA values, data types and string data. In order to clean the data, I did the following. You can find the documented code for this cleaning process below. 

1. First, I dropped the columns related to quartiles (`quartile_range`, `quartile_number`, and `suppression_flag`) as they are irrelevant to our future analysis. 

2. Since the values in the column `indicator` are quite long, I decided to create a numerical key system where each number corresponds to a unique values of the `indicator`. This is helpful for future categorization techinques as well as with readability.  

3. Next, I dropped interval columns (`time_period_label` and `confidence_interval`) since the time interval information is already represented in the columns `time_period_start_date` and `time_period_end_date` and the confidence interval is not needed for future analysis. 

4. In order to utilize time, I needed to convert the current start and end time columns into datatime data types. 

5. We can see that within the `group` column, serveral of the data points begin within the phrase "By ". Thus, we will remove it from all values in that column. This will improve any efforts on clustering/decision trees as well as make the data more readable. 

6. Lastly, I decided to remove all rows where the value column was NA, thus eliminating all NA values in the dataset. 12.5% of the original dataset was filtered out. I made this decision since the column `value` is our main focus of analysis and replacing any NAs with zeros would be compromising the integrity of the data and the representation of certain demographics. 

```{r}
#| code-fold: true
#| code-summary: "CDC Data Cleaning"
#| eval: false
#Let's drop the last three columns related to quartiles since it is unnessesary. 
cdc <- cdc %>%
  select(-quartile_range, -quartile_number, -suppression_flag)

#Now, let's check the unique values of indicator to condense the values since they're a little too long
cdc_keys <- cdc %>%
  select(indicator) %>%
  distinct()

cdc_keys <- cdc_keys %>%
  mutate(key = 1:nrow(cdc_keys))

#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. 
cdc <- cdc %>%
  right_join(cdc_keys, by = "indicator")

#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns 
cdc <- cdc %>%
  select(-time_period_label, -confidence_interval)

#Now, let's make the time start and time end datetime variables
cdc <- cdc %>%
  mutate(time_period_start_date = as.Date(time_period_start_date)) %>%
  mutate(time_period_end_date = as.Date(time_period_end_date))

#Let's look at the unique values for the group column and adjust as needed:

cdc <- cdc %>%
  mutate(group = str_replace(group, "^By\\s", ""))

#We also need to check for any na values 

#cdc %>% summarise_all(~ sum(is.na(.)))

#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. 

cdc <- cdc %>%
  filter(!is.na(value))

# cdc %>% summarise_all(~ sum(is.na(.)))
#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset. 

#Lastly, we'll get rid of the indicator column and show the cleaned data:
cdc <- cdc %>%
  select(-indicator)

write.csv(cdc, "../../data/01-modified-data/cdc_clean.csv")
```

As described in step two, I have created a key for the descriptive categories within the dataset. The chart below shows the keys and their respective description. 

![CDC Keys](images/cdc_keys_clean.png){width=70% fig-align='center'}

**[CDC Cleaned Data](https://github.com/anly501/dsan-5000-project-schinthak/blob/main/data/01-modified-data/cdc_clean.csv)**


## UK Symptom Survey
Amongst the data obtained through the UK survey, I will be focusing on survey data regarding symptoms related to prior health, ethnicity, age and sex,  as well as employement status. The raw data would be classified as very messy, with most of the column names unreadable and data types unclear. Thus, I did the following for all four datasets in order to clean the data. 

1. Since the formatting of the excel sheet is not in a traditional record format, we'll need to remove some empty columns. Thus, I removed the first four rows, since they were empty. 

2. Next, we'll remove the top row of the dataframe and make those values the column names. 

3. Using the janitor package in R, we'll also make the column names "tidy". This means the names will be lower case without any spaces. 

4. The columns `estimate`, `lower_confidence_bound`, and `upper_confidence_bound` need to be datatype double, so we will type cast those columns. 

5. Next, we will check for NA values. If there are NA values in the estimate column, this means that that group's data was not collected in the survey. Thus, we'll drop those rows and report the loss of data (please see commented code for the exact percentage of data lost on each dataframe). 

6. Thus, we'll save the cleaned datasets into csv files. 

7. I repeated steps 1-6 for all four sheets of the data since they were all formatted in the same manner. 

8. Lastly, since all four datasets have the same columns, I'll bind all four dataframes into one large dataset to represent the long covid survey statistics from CRIS in the United Kingdom. 

```{r}
#| code-fold: true
#| code-summary: "UK Survey Data Cleaning"
#| output: false
#| eval: false

long_covid_uk_age_sex <- read_excel("../../data/00-raw-data/longcovid_uk.xlsx", sheet = 'Table 1')
long_covid_uk_ethnic <- read_excel("../../data/00-raw-data/longcovid_uk.xlsx", sheet = 'Table 2')
long_covid_uk_health <- read_excel("../../data/00-raw-data/longcovid_uk.xlsx", sheet = 'Table 3')
long_covid_uk_job <- read_excel("../../data/00-raw-data/longcovid_uk.xlsx", sheet = 'Table 4')

#For Long Covid with regards to Health 

#First, we'll remove the first three rows since they're all empty. 
long_covid_uk_health <- long_covid_uk_health %>%
  filter(!row_number() %in% c(1, 2, 3, 4))

#Next, we'll make top row the column names:
long_covid_uk_health <- long_covid_uk_health %>%
  purrr::set_names(as.character(slice(., 1))) %>%
  slice(-1)

#Now, we need to name the column names tidy:
long_covid_uk_health <- long_covid_uk_health %>%
  clean_names()

#Next, we'll need to change the datatypes of certain columns. 
long_covid_uk_health <- long_covid_uk_health %>%
  mutate(estimate = as.double(estimate)) %>%
  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %>%
  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))

#Checking for NA values:
long_covid_uk_health %>%
  summarise_all(~ sum(is.na(.)))
#No NA values! 

#Let's see the cleaned data: 
head(long_covid_uk_health)
write.csv(long_covid_uk_health, "../../data/01-modified-data/long_covid_uk_health_clean.csv")

#For Long Covid with regards to Employement Status:

#Now, we'll do the same cleaning for employement status data since it's the same format:
long_covid_uk_job <- long_covid_uk_job %>%
  filter(!row_number() %in% c(1, 2, 3, 4))

#Next, we'll make top row the column names:
long_covid_uk_job <- long_covid_uk_job %>%
  purrr::set_names(as.character(slice(., 1))) %>%
  slice(-1)

#Now, we need to name the column names tidy:
long_covid_uk_job <- long_covid_uk_job %>%
  clean_names()

#Next, we'll need to change the datatypes of certain columns. 
long_covid_uk_job <- long_covid_uk_job %>%
  mutate(estimate = as.double(estimate)) %>%
  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %>%
  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))

#Checking for NA values:
long_covid_uk_job %>%
  summarise_all(~ sum(is.na(.)))
#There are a significant number of NA values for estimate and the confidence intervals. 
#Since these are for specific groups, the NA's mean that data was not collected for these groups. 
#Since we can't use "other" groups to estimate this data, we will drop these rows. 
#This results is a loss of 13% of the original dataset. 
long_covid_uk_job <- long_covid_uk_job %>%
  filter(!is.na(estimate))

head(long_covid_uk_job)
write.csv(long_covid_uk_job, "../../data/01-modified-data/long_covid_uk_job_clean.csv")

#Now, we'll do the same cleaning for employement status data since it's the same format:
long_covid_uk_age_sex <- long_covid_uk_age_sex %>%
  filter(!row_number() %in% c(1, 2, 3, 4))

#Next, we'll make top row the column names:
long_covid_uk_age_sex <- long_covid_uk_age_sex %>%
  purrr::set_names(as.character(slice(., 1))) %>%
  slice(-1)

#Now, we need to name the column names tidy:
long_covid_uk_age_sex <- long_covid_uk_age_sex %>%
  clean_names()

#Next, we'll need to change the datatypes of certain columns. 
long_covid_uk_age_sex <- long_covid_uk_age_sex %>%
  mutate(estimate = as.double(estimate)) %>%
  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %>%
  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))

#Checking for NA values:
long_covid_uk_age_sex %>%
  summarise_all(~ sum(is.na(.)))

#There are a significant number of NA values for estimate and the confidence intervals. 
#Since these are for specific groups, the NA's mean that data was not collected for these groups. 
#Since we can't use "other" groups to estimate this data, we will drop these rows. 
#This results is a loss of 16.9% of the original dataset. 
long_covid_uk_age_sex <- long_covid_uk_age_sex %>%
  filter(!is.na(estimate))

head(long_covid_uk_age_sex)
write.csv(long_covid_uk_age_sex, "../../data/01-modified-data/long_covid_uk_age_sex_clean.csv")

#Now, we'll do the same cleaning for employement status data since it's the same format:
long_covid_uk_ethnic <- long_covid_uk_ethnic %>%
  filter(!row_number() %in% c(1, 2, 3, 4))

#Next, we'll make top row the column names:
long_covid_uk_ethnic <- long_covid_uk_ethnic %>%
  purrr::set_names(as.character(slice(., 1))) %>%
  slice(-1)

#Now, we need to name the column names tidy:
long_covid_uk_ethnic <- long_covid_uk_ethnic %>%
  clean_names()

#Next, we'll need to change the datatypes of certain columns. 
long_covid_uk_ethnic <- long_covid_uk_ethnic %>%
  mutate(estimate = as.double(estimate)) %>%
  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %>%
  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))

#Checking for NA values:
long_covid_uk_ethnic %>%
  summarise_all(~ sum(is.na(.)))
#There are a significant number of NA values for estimate and the confidence intervals. 
#Since these are for specific groups, the NA's mean that data was not collected for these groups. 
#Since we can't use "other" groups to estimate this data, we will drop these rows. 
#This results is a loss of 1.8% of the original dataset. 
long_covid_uk_ethnic <- long_covid_uk_ethnic %>%
  filter(!is.na(estimate))

head(long_covid_uk_ethnic)
write.csv(long_covid_uk_ethnic, "../../data/01-modified-data/long_covid_uk_ethnic_clean.csv")

#Merging the uk data together:
long_covid_uk <- bind_rows(long_covid_uk_age_sex, long_covid_uk_ethnic, 
                           long_covid_uk_health, long_covid_uk_job)
write.csv(long_covid_uk, "../../data/01-modified-data/long_covid_uk_clean.csv")
```

---

![Long Covid UK Survey Data - Cleaned](images/symptom_cleaned_job.png){width=70% fig-align='center'}

**[UK Survey Employement Status Cleaned Data](https://github.com/anly501/dsan-5000-project-schinthak/blob/main/data/01-modified-data/long_covid_uk_clean.csv)**


## Harvard University Survey
Supervised by Harvard University, this survey data represents results from those with Long Covid regarding their demographic information as well as a plethora of symptoms they've experienced. The raw data is quite messy, with several collumns of duplicate data, some data indecipherable, and the several NA values. Thus, I made the following changes to clean the data. Please note that for NA values within symptom data, I chose to replace them with zeros since it's under assumption that if someone chose not to answer regarding a symptom, they did not experience that symptom. More information would be needed in future study for more accurate results. 

1. First, I selected columns that were to do with demographic statistics and all the symptoms recorded in the survey. To do this, I first subset a range of columns in the dataset and then hypertuned those columns by removing specific columns that were duplicated. 

2. Next, I filtered out the data where demographic information such as `sex`, `CovidPositive`, `BMI`, `Hospital_admission`, and `days_acuteCovid` was NA since those values could not be estimated. 

3. I then used the janitor package in R as well as the function rename_all() to make sure the column names were tidy. 

4. Lastly, I replaced all remaining NA values (found in the symptom column) with zeros since it's under assumption that if someone chose not to answer regarding a symptom, they did not experience that symptom. After that was complete I saved the cleaned data into a csv. 


```{r}
#| code-fold: true
#| code-summary: "Harvard Survey Cleaning"
#| echo: true
#| warning: false
#| eval: false

harvard_covid <- read_csv("../../data/00-raw-data/harvard_long_covid.csv")

harvard_covid <- harvard_covid %>%
  select(status:Hospital_admission, age:Headache_covid) %>%
  select(-country, -income, -ethnicity, -PE2, -BMICat, -Vaccination_status, 
         -No_Doses, -Type_vaccine, -agegroup, -agegroup2, -DVT3, -DVT2, 
         -Median_followupTime, -Median_followupTime2, -kidney3, -Heart_attack3) %>%
  filter(!is.na(sex) & !is.na(CovidPositive) & !is.na(BMI) & 
           !is.na(Hospital_admission) & !is.na(days_acuteCovid)) %>%
  clean_names() %>%
  rename_all(~sub("2$", "", .)) %>%
  rename_all(~sub("3$", "", .)) %>%
  rename_all(~sub("_now$", "", .)) %>%
  rename_all(~sub("_pastweek$", "", .)) %>%
  replace(is.na(.), 0)

write.csv(harvard_covid, "../../data/01-modified-data/harvard_long_covid_clean.csv")
```

![Harvard Survey Data - Cleaned](images/harvard_data_cleaned.png){width=70% fig-align='center'}

**[Harvard Survey Cleaned Data](https://github.com/anly501/dsan-5000-project-schinthak/blob/main/data/01-modified-data/harvard_long_covid_clean.csv)**

## Long Covid News
Another pillar of my analysis is to measure the media coverage on long covid and its sentiment. I used the NewsAPI to collect current news information surrounding long covid, as well as coverage on chickenpox and influenza. Since the data came from an API, there isn't too much cleaning needed to be done on the columns, however, the text needs to be cleaned. 

In the code below, I created a clean text function that removes any non alphabetic-numeric charcaters, removed any leading or trailing white spaces, removed any singular letters, nonsensical words, and ellipses. After removed those characters, I merged all text in the dataframe into one string in order to visualize the cleaned text in the word cloud below. This word cloud contains media coverage for long covid, influeza, and chickenpox.  

```{python}
#| echo: false
#| warning: false
#| output: false

import pandas as pd 
import numpy as np
import wordcloud 
import string 
import nltk
import re

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')
```

```{python}
#| warning: false
#| code-fold: true
#| code-summary: "News Cleaning"

news = pd.read_csv('../../data/00-raw-data/news_raw.csv')
news = news[news['content'] != '[Removed]']

news['all_text'] = news['title'] + ' ' + news['description'] + ' ' + news['content']

def clean_text(text):
  # Add your text cleaning logic here
  cleaned_text = text.lower()  # Example: Convert to lowercase
  cleaned_text = cleaned_text.strip()  # Remove leading and trailing spaces
  cleaned_text = re.sub(r'\[.*?\]', '', cleaned_text) 
  cleaned_text = ' '.join(cleaned_text.split()[:-1])
  cleaned_text = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned_text)  # Remove special characters
  cleaned_text = re.sub(r'\.\.\.+', '', cleaned_text)
  cleaned_text = cleaned_text.replace('...', '')  # Remove ellipses
  cleaned_text = cleaned_text.replace('doziernurphotoshutterstock', '')  # Remove specifc word
  cleaned_text = cleaned_text.replace('/', ' ')  # Remove '/'
  cleaned_text = ' '.join([word for word in cleaned_text.split() if len(word) > 1 or word.lower() in {'i', 'a'}])

  
  return cleaned_text

news['all_text'] = news['all_text'].apply(clean_text)

news.to_csv('../../data/01-modified-data/news_clean.csv', index = False)


text = news['all_text'].tolist()
text = ' '.join([str(elem) for elem in text])

# #FILTER OUT UNWANTED CHARACTERS
new_text=""
for character in text:
    if character in string.printable:
        new_text+=character
text=new_text

# #FILTER OUT UNWANTED WORDS
new_text=""
for word in nltk.tokenize.word_tokenize(text):
    if word not in nltk.corpus.stopwords.words('english'):
        if word in [".",",","!","?",":",";"]:
            #remove the last space
            new_text=new_text[0:-1]+word+" "
        else: #add a space
            new_text+=word.lower()+" "
text=new_text

def generate_word_cloud(my_text):
    from wordcloud import WordCloud, STOPWORDS
    import matplotlib.pyplot as plt
    # exit()
    # Import package
    # Define a function to plot word cloud
    def plot_cloud(wordcloud):
        # Set figure size
        plt.figure(figsize=(40, 30))
        # Display image
        plt.imshow(wordcloud) 
        # No axis details
        plt.axis("off");

    # Generate word cloud
    wordcloud = WordCloud(
        width = 3000,
        height = 2000, 
        random_state=1, 
        background_color='salmon', 
        colormap='Pastel1', 
        collocations=False,
        stopwords = STOPWORDS).generate(my_text)
    plot_cloud(wordcloud)
    plt.show()

generate_word_cloud(text)
``` 

In this word cloud, we can see stand out topics like Long Covid, of course, as well as respiratory, China, virus, vaccine, and hospital. These words suggest that the media coverage on all three illnesses is medical in nature as well as explanatory of the origins of Covid. For further analysis, we'll look to the EDA section for sentiment analysis. 

**[News Cleaned Data](https://github.com/anly501/dsan-5000-project-schinthak/blob/main/data/01-modified-data/long_covid_news_clean.txt)**

```{python}
#| echo: false
#| output: false
with open('../../data/01-modified-data/news_string_clean.txt', 'w') as file:
    # Write the list as a string with elements separated by spaces and add a newline at the end
    file.write(''.join(map(str, text)) + '\n')
```

# Presidential Address
The raw data of the president's address in stored in a text file. Thus, we'll clean the text data and visualize frequencies of words. Later on, we will also conduct sentiment analysis on the data. 

In order to clean this data such that it is prepared for sentiment analysis, we will first convert the string into a dataframe such that each row is a paragraph of text within the memorandum. Next came the cleaning of the text. Unlike the News API, which was scrapping articles from the internet, the Presidental Memorandum is written and publically published from the White House. Thus, we only need to clean the text such that non-alpha numerica characters are removed, headers such as "Section 1" or "Subject:" are removed, and leading and trailing white space is eliminated. Once that is done, the last step is to combine all the text within the dataframe into a single string once again, run through a cleaning process one time to eliminate punctuation, and visualize the cleaned data via the word cloud below. 

```{python}
#| code-fold: true
#| code-summary: "White House Address Text Cleaning"
#| warning: false

my_file = open('../../data/00-raw-data/white_house_statement_raw.txt', "r") 
  
# reading the file 
data = my_file.read() 
  
paragraphs = data.split('\n\n')

# Iterate through paragraphs, identify and remove lines starting with numbers
cleaned_paragraphs = [re.sub(r'^\d+\.\s*', '', paragraph.strip()) for paragraph in paragraphs]

# Create a DataFrame with cleaned text and numbered index starting at 1
df = pd.DataFrame({'Text': cleaned_paragraphs}, index=range(1, len(cleaned_paragraphs)+1))

def clean_text(text):
    # Add your text cleaning logic here
    cleaned_text = text.lower()  # Example: Convert to lowercase
    cleaned_text = cleaned_text.strip()  # Remove leading and trailing spaces
    cleaned_text = re.sub(r'\(.*?\)', '', cleaned_text) 
    cleaned_text = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned_text)  # Remove special characters
    cleaned_text = re.sub(r'\.\.\.+', '', cleaned_text)
    cleaned_text = cleaned_text.replace('subject:', '')  # Remove ellipses
    cleaned_text = cleaned_text.replace('sec.', '')  # Remove specifc word
    cleaned_text = cleaned_text.replace('/', ' ')  # Remove '/'
    cleaned_text = ' '.join([word for word in cleaned_text.split() if len(word) > 1 or word.lower() in {'i', 'a'}])
    cleaned_text = cleaned_text.strip() 
    
    return cleaned_text

df['Text'] = df['Text'].apply(clean_text)

df.to_csv('../../data/01-modified-data/white_house_df_clean.csv', index = False)

text = df['Text'].tolist()
text = ' '.join([str(elem) for elem in text])


# #FILTER OUT UNWANTED CHARACTERS
new_text=""
for character in text:
    if character in string.printable:
        new_text+=character
text=new_text

# #FILTER OUT UNWANTED WORDS
new_text=""
for word in nltk.tokenize.word_tokenize(text):
    if word not in nltk.corpus.stopwords.words('english'):
        if word in [".",",","!","?",":",";"]:
            #remove the last space
            new_text=new_text[0:-1]+word+" "
        else: #add a space
            new_text+=word.lower()+" "
text=new_text

def generate_word_cloud(my_text):
    from wordcloud import WordCloud, STOPWORDS
    import matplotlib.pyplot as plt
    # exit()
    # Import package
    # Define a function to plot word cloud
    def plot_cloud(wordcloud):
        # Set figure size
        plt.figure(figsize=(40, 30))
        # Display image
        plt.imshow(wordcloud) 
        # No axis details
        plt.axis("off");

    # Generate word cloud
    wordcloud = WordCloud(
        width = 3000,
        height = 2000, 
        random_state=1, 
        background_color='black', 
        colormap='Pastel1', 
        collocations=False,
        stopwords = STOPWORDS).generate(my_text)
    plot_cloud(wordcloud)
    plt.show()

generate_word_cloud(text)
```

From the word cloud, we can see that the memorandum focused on Long Covid, while also discussing indviduals, longterm solutions, as well as disabilities. This means that we could potentially see a positive sentiment in the memorandum throughout the page, however, we will dicuss that moving forward in the EDA section. 

**[White House Statement Cleaned Data](https://github.com/anly501/dsan-5000-project-schinthak/blob/main/data/01-modified-data/white_house_statement_cleaned.txt)**


```{python}
#| echo: false
#| output: false
with open('../../data/01-modified-data/white_house_statement_cleaned.txt', 'w') as file:
    # Write the list as a string with elements separated by spaces and add a newline at the end
    file.write(''.join(map(str, text)) + '\n')
```