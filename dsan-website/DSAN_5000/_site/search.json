[
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Our Long Covid dataset consists of survey statistics from a long covid survey conducted by the CDC. In this survery, we can see that users were asked to fill out a mutliple of demographic questions and then choose which long covid experience best decribed them. Let’s take a look at the cleaned data.\n\n\nCode\ncdc = pd.read_csv(\"../../data/01-modified-data/cdc_clean.csv\")\ncdc.drop('Unnamed: 0', axis = 1, inplace= True)\ncdc.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9977 entries, 0 to 9976\nData columns (total 11 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   group                   9977 non-null   object \n 1   state                   9977 non-null   object \n 2   subgroup                9977 non-null   object \n 3   phase                   9977 non-null   float64\n 4   time_period             9977 non-null   int64  \n 5   time_period_start_date  9977 non-null   object \n 6   time_period_end_date    9977 non-null   object \n 7   value                   9977 non-null   float64\n 8   lowci                   9977 non-null   float64\n 9   highci                  9977 non-null   float64\n 10  key                     9977 non-null   int64  \ndtypes: float64(4), int64(2), object(5)\nmemory usage: 857.5+ KB\n\n\n\n\n\n\n\n\n\n\n\ngroup\nstate\nsubgroup\nphase\ntime_period\ntime_period_start_date\ntime_period_end_date\nvalue\nlowci\nhighci\nkey\n\n\n\n\n0\nNational Estimate\nUnited States\nUnited States\n3.5\n46\n2022-06-01\n2022-06-13\n14.0\n13.5\n14.5\n1\n\n\n1\nAge\nUnited States\n18 - 29 years\n3.5\n46\n2022-06-01\n2022-06-13\n17.8\n15.9\n19.8\n1\n\n\n2\nAge\nUnited States\n30 - 39 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.2\n14.1\n16.2\n1\n\n\n3\nAge\nUnited States\n40 - 49 years\n3.5\n46\n2022-06-01\n2022-06-13\n16.9\n15.7\n18.3\n1\n\n\n4\nAge\nUnited States\n50 - 59 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.3\n14.1\n16.7\n1\n\n\n\n\n\n\n\nThus far, we can see some of the important columns to take a further look at are group, subgroup, value, and key. Firstly, let’s look at the distribution of the value (percent having Long Covid based on the key’s defintion) grouped by each key.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=\"key\", y=\"value\",\n            hue=\"key\", palette=\"Set2\",\n            data=cdc)\nplt.show()\n\n\n\n\n\nFrom the boxplots, we can see that the distibution across keys is varying, with the highest percentage going to key 6, representing the percentage of adults with any activity limitations from long Covid who also currently have long Covid. The lowest range of values goes to key 9, which represents the percentage of adults of significant activity limitations.\nThus, to understand this data further, we’ll take a look at the additional groups affected by Long Covid in a number of ways.\n\n\nCode\nfig, axs = plt.subplots(4, 2, figsize=(15, 25))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(1,9), axs.ravel()):\n    if i == 8:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", ax = ax, errorbar=None)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n        ax.tick_params(axis='x', rotation=45)\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", estimator = \"mean\", ax = ax, errorbar=None)\n        for j in ax.containers:\n            ax.bar_label(j,)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n\nplt.show()\n\n\n\n\n\nLet’s take a look at the data through each group and it’s given sugroups. Primarily, ammongst all Long Covid patients surveyed through the CDC, we can see that the largest age group surveyed was those age 40-49. Additionally, more females responded to the survey than males. In terms of gender and sexuality, cis-gender females and transgender people had a higher experience with long Covid than cis-gender males and bisexual individuals had a greater percentage of long covid experiences. A few other things to note, those in the category “non-hsopanic, other races and multiple races” were highest amongst the ethnicity demographics and not suprisingly, the highest disparity amongst the subcategories was between those who were disabled verses not disabled. Those with a disability had a 12.2% increase in long covid experiences than those who do not have a disability. The is very important to understanding Long Covid, as it has been known to effect those with disabilities more than it say with non-immunocompromised people.\nLastly, we’ll take a look at the overall distributions for each key value in order to understand the survey in greater detail.\n\n\nCode\nfig, axs = plt.subplots(5, 2, figsize=(15, 25))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(10), axs.ravel()):\n    if i == 9:\n        sns.histplot(cdc, x=\"value\", ax = ax, kde = True)\n        # chart formatting\n        ax.set_title('All')\n        ax.set_xlabel(\"\")\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.histplot(analysis, x=\"value\", ax = ax, kde = True)\n\n        # chart formatting\n        ax.set_title(groups[i])\n\nplt.show()\n\n\n\n\n\nFor all subgroups and the dataset as a whole, we can see that the distributions are all rightly skewed. This indicates that most of the percentages collected all fell closer to zero. Since this dataset is heavily filled with categorical data, we will try to develop a way to pear down the features through naive bayes analysis and feature selection within the next section."
  },
  {
    "objectID": "data-exploration.html#cdc---long-covid",
    "href": "data-exploration.html#cdc---long-covid",
    "title": "Data Exploration",
    "section": "",
    "text": "Our Long Covid dataset consists of survey statistics from a long covid survey conducted by the CDC. In this survery, we can see that users were asked to fill out a mutliple of demographic questions and then choose which long covid experience best decribed them. Let’s take a look at the cleaned data.\n\n\nCode\ncdc = pd.read_csv(\"../../data/01-modified-data/cdc_clean.csv\")\ncdc.drop('Unnamed: 0', axis = 1, inplace= True)\ncdc.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9977 entries, 0 to 9976\nData columns (total 11 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   group                   9977 non-null   object \n 1   state                   9977 non-null   object \n 2   subgroup                9977 non-null   object \n 3   phase                   9977 non-null   float64\n 4   time_period             9977 non-null   int64  \n 5   time_period_start_date  9977 non-null   object \n 6   time_period_end_date    9977 non-null   object \n 7   value                   9977 non-null   float64\n 8   lowci                   9977 non-null   float64\n 9   highci                  9977 non-null   float64\n 10  key                     9977 non-null   int64  \ndtypes: float64(4), int64(2), object(5)\nmemory usage: 857.5+ KB\n\n\n\n\n\n\n\n\n\n\n\ngroup\nstate\nsubgroup\nphase\ntime_period\ntime_period_start_date\ntime_period_end_date\nvalue\nlowci\nhighci\nkey\n\n\n\n\n0\nNational Estimate\nUnited States\nUnited States\n3.5\n46\n2022-06-01\n2022-06-13\n14.0\n13.5\n14.5\n1\n\n\n1\nAge\nUnited States\n18 - 29 years\n3.5\n46\n2022-06-01\n2022-06-13\n17.8\n15.9\n19.8\n1\n\n\n2\nAge\nUnited States\n30 - 39 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.2\n14.1\n16.2\n1\n\n\n3\nAge\nUnited States\n40 - 49 years\n3.5\n46\n2022-06-01\n2022-06-13\n16.9\n15.7\n18.3\n1\n\n\n4\nAge\nUnited States\n50 - 59 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.3\n14.1\n16.7\n1\n\n\n\n\n\n\n\nThus far, we can see some of the important columns to take a further look at are group, subgroup, value, and key. Firstly, let’s look at the distribution of the value (percent having Long Covid based on the key’s defintion) grouped by each key.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=\"key\", y=\"value\",\n            hue=\"key\", palette=\"Set2\",\n            data=cdc)\nplt.show()\n\n\n\n\n\nFrom the boxplots, we can see that the distibution across keys is varying, with the highest percentage going to key 6, representing the percentage of adults with any activity limitations from long Covid who also currently have long Covid. The lowest range of values goes to key 9, which represents the percentage of adults of significant activity limitations.\nThus, to understand this data further, we’ll take a look at the additional groups affected by Long Covid in a number of ways.\n\n\nCode\nfig, axs = plt.subplots(4, 2, figsize=(15, 25))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(1,9), axs.ravel()):\n    if i == 8:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", ax = ax, errorbar=None)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n        ax.tick_params(axis='x', rotation=45)\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", estimator = \"mean\", ax = ax, errorbar=None)\n        for j in ax.containers:\n            ax.bar_label(j,)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n\nplt.show()\n\n\n\n\n\nLet’s take a look at the data through each group and it’s given sugroups. Primarily, ammongst all Long Covid patients surveyed through the CDC, we can see that the largest age group surveyed was those age 40-49. Additionally, more females responded to the survey than males. In terms of gender and sexuality, cis-gender females and transgender people had a higher experience with long Covid than cis-gender males and bisexual individuals had a greater percentage of long covid experiences. A few other things to note, those in the category “non-hsopanic, other races and multiple races” were highest amongst the ethnicity demographics and not suprisingly, the highest disparity amongst the subcategories was between those who were disabled verses not disabled. Those with a disability had a 12.2% increase in long covid experiences than those who do not have a disability. The is very important to understanding Long Covid, as it has been known to effect those with disabilities more than it say with non-immunocompromised people.\nLastly, we’ll take a look at the overall distributions for each key value in order to understand the survey in greater detail.\n\n\nCode\nfig, axs = plt.subplots(5, 2, figsize=(15, 25))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(10), axs.ravel()):\n    if i == 9:\n        sns.histplot(cdc, x=\"value\", ax = ax, kde = True)\n        # chart formatting\n        ax.set_title('All')\n        ax.set_xlabel(\"\")\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.histplot(analysis, x=\"value\", ax = ax, kde = True)\n\n        # chart formatting\n        ax.set_title(groups[i])\n\nplt.show()\n\n\n\n\n\nFor all subgroups and the dataset as a whole, we can see that the distributions are all rightly skewed. This indicates that most of the percentages collected all fell closer to zero. Since this dataset is heavily filled with categorical data, we will try to develop a way to pear down the features through naive bayes analysis and feature selection within the next section."
  },
  {
    "objectID": "data-exploration.html#long-covid-symptoms---uk",
    "href": "data-exploration.html#long-covid-symptoms---uk",
    "title": "Data Exploration",
    "section": "Long Covid Symptoms - UK:",
    "text": "Long Covid Symptoms - UK:\nThe following dataset was measured to UK survey and app data regarding symptoms tracked for patients with Long Covid. In order to understand the distribution of the data and track the symptoms over time, we can take a look at the following.\n\nuk_health = pd.read_csv(\"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\nuk_health.drop('Unnamed: 0', axis = 1, inplace= True)\nuk_health.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 272 entries, 0 to 271\nData columns (total 6 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   symptom                            272 non-null    object \n 1   domain                             272 non-null    object \n 2   group                              272 non-null    object \n 3   estimate                           264 non-null    float64\n 4   lower_95_percent_confidence_limit  264 non-null    float64\n 5   upper_95_percent_confidence_limit  264 non-null    float64\ndtypes: float64(3), object(3)\nmemory usage: 12.9+ KB\n\n\n\nuk_health.head()\n\n\n\n\n\n\n\n\nsymptom\ndomain\ngroup\nestimate\nlower_95_percent_confidence_limit\nupper_95_percent_confidence_limit\n\n\n\n\n0\nAbdominal pain\nAll people\nAll people\n9.56\n8.82\n10.30\n\n\n1\nAbdominal pain\nHealth/disability status\nNo health conditions\n6.96\n6.04\n7.87\n\n\n2\nAbdominal pain\nHealth/disability status\nActivity not limited by health conditions\n6.06\n4.26\n7.85\n\n\n3\nAbdominal pain\nHealth/disability status\nActivity limited a little by health conditions\n11.30\n9.70\n12.91\n\n\n4\nAbdominal pain\nHealth/disability status\nActivity limited a lot by health conditions\n17.20\n14.83\n19.56\n\n\n\n\n\n\n\nSome important features to look at in the data set are symptom, which are the symptoms people tracked in the survey, domain and group, which act similarly to the group and subgroup from the CDC data, and estimate, which is the corresponding percentage of the those who filled out the survey. If we to extrapolate from the survey data, we could also use the lower and upper confidence boundaries.\nIn order to understand the dataset and the symptoms, lets take a look at the symptoms for all people that filled out the survey."
  },
  {
    "objectID": "data-exploration.html#long-covid-news",
    "href": "data-exploration.html#long-covid-news",
    "title": "Data Exploration",
    "section": "Long Covid News:",
    "text": "Long Covid News:\nLong Covid News can be difficult to interpret. Thus, after cleaning the text data, we will interpret the data through frequency testing. We can visualize frequency through a world cloud.\n\n\nCode\nfile = open(\"../../data/01-modified-data/long_covid_news_clean.txt\", \"r\")\ncontent = file.read()\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(15, 10))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ngenerate_word_cloud(content)\n\n\n\n\n\nFrom this world cloud, we can see several words related to long covid and symptom management. In order to further understand the implications that the news and media play into the information spread about long covid and it’s related topics, we will dive further into the text analysis and categorization via Naive Bayes and feature selection."
  },
  {
    "objectID": "data-exploration.html#presidential-address",
    "href": "data-exploration.html#presidential-address",
    "title": "Data Exploration",
    "section": "Presidential Address:",
    "text": "Presidential Address:\nSimilarly to the News atricles on Long Covid, we can take a look at the text data of the White House’s official statement on its efforts to confront and treat long covid through a frequency analysis. Creating a word cloud is one way to visualize the text.\n\n\nCode\nfile = open(\"../../data/01-modified-data/white_house_statement_cleaned.txt\", \"r\")\ncontent = file.read()\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(15, 10))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ngenerate_word_cloud(content)"
  },
  {
    "objectID": "dr.html",
    "href": "dr.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Long Covid"
  },
  {
    "objectID": "introduction.html#summary-of-long-covid",
    "href": "introduction.html#summary-of-long-covid",
    "title": "Introduction",
    "section": "Summary of Long Covid",
    "text": "Summary of Long Covid\nCovid-19, the infectious disease originating from the SARS-Cov-2 virus, was declared a global pandemic in March of 2020. As the illness spread, affecting thousands of people worldwide, a multitude of individuals started reporting lasting symptoms after being cleared of the disease. These varying degrees of symptoms after an extended period of time can be classified as long covid. Specifically, “Long COVID is an often debilitating illness that occurs in at least 10% of severe acute respiratory syndrome coronavirus 2” Davis et al. (2023). Those who have reported long covid face a variety of symptoms ranging from heart, lung, pancreas, immune, and neurological diseases and chronic illnesses. Additionally, those in existing vulnerability groups such as those with type two diabetes, those of the female sex, and those with autoimmune disease to name a few, are also more susceptible to long covid Davis et al. (2023)."
  },
  {
    "objectID": "introduction.html#why-is-it-important",
    "href": "introduction.html#why-is-it-important",
    "title": "Introduction",
    "section": "Why is it Important?",
    "text": "Why is it Important?\nThe study and analysis of long covid across demographics in the United States is vital due to the recency of the pandemic as well as the lack of outreach and knowledge regarding this topic. It is both the biomedical community as well as the public that currently have little knowledge about long covid, the possible symptoms, the vulnerable groups, and the current research in the field Sudre et al. (2021)."
  },
  {
    "objectID": "introduction.html#why-should-you-continue-reading",
    "href": "introduction.html#why-should-you-continue-reading",
    "title": "Introduction",
    "section": "Why should you continue reading?",
    "text": "Why should you continue reading?\nGiven the lack of public knowledge and the widespread impending issue of managing the lasting effects of COVID-19, the study of long covid is essential now more than ever. At the moment, the scientific research in this area is still new and developing, meaning that findings have not made headway for the general audiences. Additionally, there have been very few social science perspectives published regarding the impact of long covid symptoms on the people’s lives or the conversations surrounding this topic in the media. These are topics of conversation which this project hopes to uncover."
  },
  {
    "objectID": "introduction.html#past-works",
    "href": "introduction.html#past-works",
    "title": "Introduction",
    "section": "Past works:",
    "text": "Past works:\nAs previously mentioned, there is still limited research published surrounding long covid in academic and scientific journals. However, the current published works provide a plethora of information surrounding noted symptoms, sentiments on long covid, and more.\nThe authors of Long COVID: major findings, mechanisms and recommendations compiled the findings of several COVID-19 research papers as well as collected data from long covid researchers and clinics in order to create a list of long covid symptoms. Additionally, the authors noted a few challenges in their collection process, one of those being the lack of uniformity in testing, especially during the early stages of COVID-19. They also noted that several non-respiratory symptoms go unlabeled as potential long covid symptoms due to the narrative that COVID is only a respiratory illness Davis et al. (2023). With this being said, the work of Davis et al. (2023) on the topic provides a strong base understanding of long covid and the future research that is needed.\nIn conjunction with the previous research, the authors of Attributes and predictors of long COVID continued research on the documented symptoms of long covid across multiple demographics (age, race, location, sex, etc) and created a random forest predictive model in order to make predictions on symptoms on specific demographics. The primary source of their data was from “The COVID Symptom Study is a mobile application” which allows users to self report “daily information on their health status and symptoms, as well as results of any available COVID-19 test” Sudre et al. (2021). Thus, while they were able to provide an extensive analysis of symptoms, this data is only a small subset of the population using the application."
  },
  {
    "objectID": "introduction.html#what-am-i-exploring",
    "href": "introduction.html#what-am-i-exploring",
    "title": "Introduction",
    "section": "What am I exploring?",
    "text": "What am I exploring?\nFor this project, I would like to focus on creating a succinct understanding of long covid, the demographics affected, and the current knowledge base and media presence in order to provide this information to the public. Through data gathering, exploratory analysis, visualizations, and text analysis, I plan to answer a myriad of questions surrounding long covid and present the current information base in a way that is transparent and accessible."
  },
  {
    "objectID": "introduction.html#questions-to-address",
    "href": "introduction.html#questions-to-address",
    "title": "Introduction",
    "section": "Questions to Address:",
    "text": "Questions to Address:\n\nHow does gender and age affect one’s chances of experiencing long covid symptoms?\nWhich pre-existing chronic illnesses make one more prone to experiencing long covid symptoms?\nIn what ways does race affect the persistence of long covid symptoms?\now might the existing research on long covid symptoms be skewed to respiratory illnesses?\nWhat is the most common collection of symptoms present in someone with long covid?\nWhat is the sentiment of current media on the topic of long covid?\nCan we create a way to predict symptoms of long covid for demographics?\nHow have the reported symptoms of long covid changed over time?\nWhat are the current resources to assist with long covid and are they easily accessible?\nWhat is government opinion (written) on the state of long covid research/information?"
  },
  {
    "objectID": "introduction.html#goals-and-hypotheses",
    "href": "introduction.html#goals-and-hypotheses",
    "title": "Introduction",
    "section": "Goals and Hypotheses:",
    "text": "Goals and Hypotheses:\nThrough this study of long covid across a multitude of demographics, I hope to answer the above questions as well as provide a concise and coherent way to view information on long covid.\nAs of now, I can hypothesize that existing minority and disadvantaged populations experience more severe long covid symptoms due to the lack of public information, biomedical education to service these communities, the lack of aid from the government in assisting those with chronic illness due to these lasting effects of COVID-19."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "eda-code.html",
    "href": "eda-code.html",
    "title": "Long Covid Symptoms - UK",
    "section": "",
    "text": "cdc = pd.read_csv(\"../../data/01-modified-data/cdc_clean.csv\")\ncdc.drop('Unnamed: 0', axis = 1, inplace= True)\ncdc.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9977 entries, 0 to 9976\nData columns (total 11 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   group                   9977 non-null   object \n 1   state                   9977 non-null   object \n 2   subgroup                9977 non-null   object \n 3   phase                   9977 non-null   float64\n 4   time_period             9977 non-null   int64  \n 5   time_period_start_date  9977 non-null   object \n 6   time_period_end_date    9977 non-null   object \n 7   value                   9977 non-null   float64\n 8   lowci                   9977 non-null   float64\n 9   highci                  9977 non-null   float64\n 10  key                     9977 non-null   int64  \ndtypes: float64(4), int64(2), object(5)\nmemory usage: 857.5+ KB\ncdc.head()\n\n\n\n\n\n\n\n\ngroup\nstate\nsubgroup\nphase\ntime_period\ntime_period_start_date\ntime_period_end_date\nvalue\nlowci\nhighci\nkey\n\n\n\n\n0\nNational Estimate\nUnited States\nUnited States\n3.5\n46\n2022-06-01\n2022-06-13\n14.0\n13.5\n14.5\n1\n\n\n1\nAge\nUnited States\n18 - 29 years\n3.5\n46\n2022-06-01\n2022-06-13\n17.8\n15.9\n19.8\n1\n\n\n2\nAge\nUnited States\n30 - 39 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.2\n14.1\n16.2\n1\n\n\n3\nAge\nUnited States\n40 - 49 years\n3.5\n46\n2022-06-01\n2022-06-13\n16.9\n15.7\n18.3\n1\n\n\n4\nAge\nUnited States\n50 - 59 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.3\n14.1\n16.7\n1\nThus far, we can see some of the important columns to take a further look at are group, subgroup, value, and key. Firstly, let’s look at the distribution of the value (percent having Long Covid based on the key’s defintion) grouped by each key.\n# cdc['key'] = cdc['key'].astype('category')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=\"key\", y=\"value\",\n            hue=\"key\", palette=\"Set2\",\n            data=cdc)\nplt.show()\nFrom the boxplots, we can see that the distibution across keys is varying, with the highest percentage going to key 6, representing the percentage of adults with any activity limitations from long Covid who also currently have long Covid. The lowest range of values goes to key 9, which represents the percentage of adults of significant activity limitations.\nThus, to understand this data further, we’ll take a look at the additional groups affected by Long Covid in a number of ways.\nfig, axs = plt.subplots(4, 2, figsize=(40, 30))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(1,9), axs.ravel()):\n    if i == 8:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", ax = ax, errorbar=None)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n        ax.tick_params(axis='x', rotation=45)\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.barplot(analysis, x=\"subgroup\", y=\"value\", hue = \"subgroup\", estimator = \"mean\", ax = ax, errorbar=None)\n        for j in ax.containers:\n            ax.bar_label(j,)\n\n        # chart formatting\n        ax.set_title(groups[i])\n        ax.set_xlabel(\"\")\n\nplt.show()\nLet’s take a look at the data through each group and it’s given sugroups. Primarily, ammongst all Long Covid patients surveyed through the CDC, we can see that the largest age group surveyed was those age 40-49. Additionally, more females responded to the survey than males. In terms of gender and sexuality, cis-gender females and transgender people had a higher experience with long Covid than cis-gender males and bisexual individuals had a greater percentage of long covid experiences. A few other things to note, those in the category “non-hsopanic, other races and multiple races” were highest amongst the ethnicity demographics and not suprisingly, the highest disparity amongst the subcategories was between those who were disabled verses not disabled. Those with a disability had a 12.2% increase in long covid experiences than those who do not have a disability. The is very important to understanding Long Covid, as it has been known to effect those with disabilities more than it say with non-immunocompromised people.\nLastly, we’ll take a look at the overall distributions for each key value in order to understand the survey in greater detail.\nfig, axs = plt.subplots(5, 2, figsize=(40, 30))\n\ngroups = cdc['group'].unique()\nfor i, ax in zip(range(10), axs.ravel()):\n    if i == 9:\n        sns.histplot(cdc, x=\"value\", ax = ax, kde = True)\n        # chart formatting\n        ax.set_title('All')\n        ax.set_xlabel(\"\")\n    else:\n        analysis = cdc[cdc[\"group\"] == groups[i]]\n        sns.histplot(analysis, x=\"value\", ax = ax, kde = True)\n\n        # chart formatting\n        ax.set_title(groups[i])\n\nplt.show()\nFor all subgroups and the dataset as a whole, we can see that the distributions are all rightly skewed. This indicates that most of the percentages collected all fell closer to zero. Since this dataset is heavily filled with categorical data, we will try to develop a way to pear down the features through naive bayes analysis and feature selection within the next section.\nThe following dataset was measured to UK survey and app data regarding symptoms tracked for patients with Long Covid. In order to undertsand the distribution of the data and track the symptoms over time, we can take a look at the following.\nuk_health = pd.read_csv(\"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\nuk_health.drop('Unnamed: 0', axis = 1, inplace= True)\nuk_health.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 272 entries, 0 to 271\nData columns (total 6 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   symptom                            272 non-null    object \n 1   domain                             272 non-null    object \n 2   group                              272 non-null    object \n 3   estimate                           264 non-null    float64\n 4   lower_95_percent_confidence_limit  264 non-null    float64\n 5   upper_95_percent_confidence_limit  264 non-null    float64\ndtypes: float64(3), object(3)\nmemory usage: 12.9+ KB\nuk_health.head()\n\n\n\n\n\n\n\n\nsymptom\ndomain\ngroup\nestimate\nlower_95_percent_confidence_limit\nupper_95_percent_confidence_limit\n\n\n\n\n0\nAbdominal pain\nAll people\nAll people\n9.56\n8.82\n10.30\n\n\n1\nAbdominal pain\nHealth/disability status\nNo health conditions\n6.96\n6.04\n7.87\n\n\n2\nAbdominal pain\nHealth/disability status\nActivity not limited by health conditions\n6.06\n4.26\n7.85\n\n\n3\nAbdominal pain\nHealth/disability status\nActivity limited a little by health conditions\n11.30\n9.70\n12.91\n\n\n4\nAbdominal pain\nHealth/disability status\nActivity limited a lot by health conditions\n17.20\n14.83\n19.56\nSome important features to look at in the data set are symptom, which are the symptoms people tracked in the survey, domain and group, which act similarly to the group and subgroup from the CDC data, and estimate, which is the corresponding percentage of the those who filled out the survey. If we to extrapolate from the survey data, we could also use the lower and upper confidence boundaries.\nIn order to understand the dataset and the symptoms, lets take a look at the symptoms for all people that filled out the survey.\nimport pandas as pd \nimport numpy as np\nimport wordcloud \nimport string \nimport nltk\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words('english')\n\nnews = pd.read_csv('../../data/00-raw-data/long_covid_news_raw.csv')\ntext = news['title'].tolist()\ntext.append(news['content'].tolist())\ntext = ' '.join([str(elem) for elem in text])\n\n# #FILTER OUT UNWANTED CHARACTERS\nnew_text=\"\"\nfor character in text:\n    if character in string.printable:\n        new_text+=character\ntext=new_text\n\n# #FILTER OUT UNWANTED WORDS\nnew_text=\"\"\nfor word in nltk.tokenize.word_tokenize(text):\n    if word not in nltk.corpus.stopwords.words('english'):\n        if word in [\".\",\",\",\"!\",\"?\",\":\",\";\",\"[ removed ]\"]:\n            #remove the last space\n            new_text=new_text[0:-1]+word+\" \"\n        else: #add a space\n            new_text+=word.lower()+\" \"\ntext=new_text\n\nwith open('../../data/01-modified-data/long_covid_news_clean.txt', 'w') as file:\n    # Write the list as a string with elements separated by spaces and add a newline at the end\n    file.write(''.join(map(str, text)) + '\\n')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/shchinthak/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/shchinthak/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/shchinthak/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/shchinthak/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nfile = open(\"../../data/01-modified-data/long_covid_news_clean.txt\", \"r\")\ncontent = file.read()\nprint(content)\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ngenerate_word_cloud(content)\n\n[ removed ] [ removed ] `` long colds '' may exist, researchers find studying long covid [ removed ] [ removed ] [ removed ] threads ban sensitive search terms temporary, instagram head says how to cure 'brain fog ' using these super easy expert tips [ removed ] care home boss shocked speed covid variant american long covid sufferers preyed unregulated stem cell clinics a new possible explanation long covid could lead simple treatment patients long covid research is in its most hopeful phase yet have cold cant seem shake? long covid thingand long cold may long-covid may long-term respiratory illness - study it 's long covid: symptoms linger illnesses, leading 'long colds ' long covid: can plant-based, anti-inflammatory diets help? long colds vs. long covid: the differences, similarities symptoms new research offers clues causes long covid fuelling hope eventual treatments [ removed ] brain fog long covid: low serotonin could play role [ removed ] what causes long covid? the answer might gut [ removed ] [ removed ] [ removed ] [ removed ] canadians travelling mexico lyme disease treatments. that worries health experts researchers find long covid might linked brain damage paxlovid may reduce chance long covid. why dont doctors prescribe? [ removed ] researchers link persistent inflammation long covid patients brain fog & fatigue 'long colds ' show prolonged symptoms similar long covid, claims study do i still need wear mask isolate? heres covid-19 key facts update [ removed ] [ removed ] [ removed ] long covid may linked brain damage, study finds getting covid several times outsize effect black people, doctors say [ removed ] why are death disability rising among young americans? | opinion meta acknowledges blocking covid searches threads israel-hamas war long covid prevalent among bedridden least 7 days infected: study [ removed ] [ removed ] [ removed ] what pots? this strange disorder doubled since pandemic sen moncrieff: some people may never get happened pandemic carnival ruled negligent cruise 662 passengers got covid-19 early pandemic aaron rodgers challenges 'mr. pfizer ' travis kelce duel with rfk jr. the labor shortage one reasonand quiet quitting canada secret economic advantage could lead greater prosperity what expect jobs report what expect friday 's jobs report carnival ruled negligent cruise 662 passengers got covid early pandemic people whove covid least 5 times describe illness changed reinfection carnival ruled negligent cruise 662 passengers got covid-19 early pandemic britain 's covid-19 response inquiry enters second phase political decisions spotlight early results new zealand election indicate christopher luxon poised become prime minister scottish covid inquiry: what investigating work? carnival ruled negligent cruise 662 passengers got covid-19 early pandemic self-silencing is making women sick why new covid shot game-changer ( term booster obsolete ) [ removed ] nobel peace prize winner announced bedbug infestation fears: morning rundown [ removed ] news24 | new zealand 's national party lead new government labour concedes could low serotonin levels contribute long covid? 'long colds ' thing, like long covid say experts long covid research opens door exploration post-viral illness there worry long covid, study shows. you could get long cold, | cnn im back normal, shorter hair - leinster wing vanessa hullon battles back hodgkins lymphoma pfizer plans double price covid medication paxlovid price lifesaving covid-19 antiviral paxlovid expected rise next year, raising concerns access | cnn parents young children say kids left behind updated covid-19 vaccines roll | cnn insurers slam mental health parity plan [ ' [ removed ] ', ' [ removed ] ', 'some people may experience `` long colds, '' long-term symptoms following common colds, flu, pneumonia respiratory illnesses, similar pattern seen in\\xa0long covid, according new stu [ +2964 chars ] ', ' [ removed ] ', ' [ removed ] ', ' [ removed ] ', 'the head instagram tuesday said app threads soon stop blocking search terms like `` covid '' current practice app, meta launched july.\\xa0\\r\\nin exchange threads p [ +4570 chars ] ', 'most people experienced jet lag sluggish feeling brain tired cant really think straight.\\r\\nthe sort fuzzy feeling happen get flu, like [ +3198 chars ] ', ' [ removed ] ', 'ms emsley, 31, said started residents cold flu-like symptoms. \\r\\n '' as day went people became poorly started affecting staff well, '' said.\\r\\nms emsley [ +340 chars ] ', 'dozens health clinics across country selling unauthorized stem cell treatments purported prevent treat side effects covid-19.\\r\\nthirty-eight businesses serving around 60 clini [ +5601 chars ] ', 'the neurologic long covid symptoms patients, like brain fog memory loss, may caused lingering virusin gut, places.thats according new study university pennsylv [ +2811 chars ] ', ' a phenomenal amount research long covidthe name chronic symptoms following case covid-19has published past three years. but scientific advances yet bring relief [ +5857 chars ] ', 'have ever cold couldnt shake weeks endone definitely wasnt covid? long covidlingering symptoms sars-cov-2 infection last weeks, months, even year [ +1955 chars ] ', `` researchers london 's queen mary university found covid may respiratory disease long-term effects, several others also showing evidence long-term aftereffects [ +2073 chars ] '', 'the coronavirus isnt pathogen cause symptoms last months, even years, initial infection overcome, new study published friday the lancets eclinicalmedicine [ +5603 chars ] ', 'for many people, particularly following vaccination, infection sars-cov-2, virus causes covid-19, resolves within days. but others, results long covid, variety ofte [ +10975 chars ] ', ' &lt; ul &gt; &lt; li &gt; researchers say may lasting health impacts non-covid acute respiratory illnesses going unrecognized. &lt; /li &gt; &lt; li &gt; those acute respiratory illness test negati [ +7150 chars ] ', `` this story part cbc health 's second opinion, weekly\\xa0analysis health medical science news emailed subscribers saturday mornings. if n't subscribed yet, [ +9364 chars ] '', ' [ removed ] ', ' &lt; ul &gt; &lt; li &gt; long covid affects around 510 % people became infected sars-cov-2, virus causes covid-19. &lt; /li &gt; &lt; li &gt; there 200 symptoms identified, among [ +7119 chars ] ', ' [ removed ] ', 'nearly one five people covid-19 united states continue suffer symptoms long covid. but people recover completely others remain sick mystery [ +7487 chars ] ', ' [ removed ] ', ' [ removed ] ', ' [ removed ] ', ' [ removed ] ', 'medical tourism drawn foreigners mexico decades, attraction longer limited breast augmentations, porcelain veneers rhinoplasties.\\xa0\\r\\nclinics specializing alt [ +8939 chars ] ', 'new delhi, long covid appear linked direct viral invasion active damage brain, according new study. researchers university gothenburg, sweden, performed blo [ +2232 chars ] ', ' a consensus emerged among experts study treat long covid: paxlovid seems reduce risk lingering symptoms among eligible take it.\\r\\nthe idea intuitive, experts say. pax [ +7024 chars ] ', ' [ removed ] ', 'new delhi, persistent inflammation long covid patients linked depleting serotonin levels, turn, several symptoms brain fog, fatigue memory loss. researchers u [ +2820 chars ] ', `` new delhi, people may experience long-term symptoms -- 'long colds ' -- acute respiratory infections test negative covid-19, according study published the lancet 's eclinicalme [ +2170 chars ] '', `` australia appears cusp eighth covid-19 wave, increase cases across country.\\xa0\\r\\nvictoria 's acting chief health officer suggested melburnians consider donning [ +6157 chars ] '', ' [ removed ] ', ' [ removed ] ', ' [ removed ] ', ' a new study suggests long covid caused direct viral invasion brain damage. researchers university gothenburg sweden analyzed blood cerebrospinal fluid samples fro [ +319 chars ] ', 'after contracting covid third time, may 2022, s. monet wahls noticed usual fall winter cough became perpetual, year-round hacking. respiratory issues made sleeping night [ +5880 chars ] ', ' [ removed ] ', `` america 's labor force facing crisis, one knows exactly. according data bureau labor statistics, number american adults considered unable work grew tha [ +5537 chars ] '', 'threads leaving knot tied covid-related searches foreseeable future.the social media company blocked terms including covid, vaccines long covid focuses resources [ +2136 chars ] ', ' &lt; ul &gt; &lt; li &gt; news &lt; /li &gt; \\r\\n &lt; li &gt; science news &lt; /li &gt; \\r\\n &lt; li &gt; long covid prevalent among bedridden least 7 days infected: study &lt; /li &gt; &lt; /ul &gt; \\r\\nfollow us on social media ', ' [ removed ] ', ' [ removed ] ', ' [ removed ] ', 'in late 2021, 18 months long covid symptoms, oonagh cousins, member great britain rowing team, ready resume training. shed contracted covid-19 early 2020, although ini [ +7872 chars ] ', 'my memory fails regular basis. i dont mean thing walk room suddenly know entered; i mean someone say: remember time? and [ +3410 chars ] ', 'canberra, australia -- a cruise operator failed cancel voyage sydney led major covid-19 outbreak ruled negligent duty care passengers australian class- [ +3195 chars ] ', 'jets quarterback aaron rodgers made waves last week the pat mcafee show coined term `` mr. pfizer '' reference kansas city chiefs tight end travis kelce.\\r\\nkelce recently filmed ad [ +3776 chars ] ', 'one question hovered post-pandemic economy better part three years: where workers? businesses difficulty hiring blamed gamut ills: too-generous un [ +4819 chars ] ', 'commentary: canada among first countries recognize mental health important variable competitiveness\\r\\nimproved brain performance, enhanced creativity, better treatments neurological di [ +8443 chars ] ', 'this time last week, prospects seemed growing dimmer september jobs report would land friday planned. \\r\\na government shutdown would forced bureau labor statisti [ +8550 chars ] ', 'this time last week, prospects seemed growing dimmer september jobs report would land friday planned. \\r\\na government shutdown would forced bureau labor statisti [ +8016 chars ] ', 'canberra, australia a cruise operator failed cancel voyage sydney led major covid-19 outbreak ruled negligent duty care passengers australian class-act [ +3190 chars ] ', 'nearly four years covids emergence, plenty people tested positive least twice. but unlucky group hit reinfection reinfection.\\r\\nive seen patients fiv [ +7991 chars ] ', 'canberra, australia -- a cruise operator failed cancel voyage sydney led major covid-19 outbreak ruled negligent duty care passengers australian class- [ +3195 chars ] ', 'london -- britains inquiry response coronavirus pandemic impact nation entered second phase tuesday, political decision-making around major developments, [ +2720 chars ] ', `` the centre-right national party led christopher luxon held strong lead new zealand 's general election saturday, 20 % vote counted.the national party, currently oppos [ +1613 chars ] '', `` the first main evidence sessions public inquiry investigating scotland 's response pandemic take place coming weeks. \\r\\nin areas inquiry look impact [ +6406 chars ] '', 'canberra, australia -- a cruise operator failed cancel voyage sydney led major covid-19 outbreak ruled negligent duty care passengers australian class- [ +3195 chars ] ', 'be disappointing piece advice people would pay money hear, therapy office, often valuable guidance i give. my clients mostly women, nearly [ +6304 chars ] ', 'unlike earlier pandemic, initial vaccines followed seemingly never-ending stream boosters, recently updated covid vaccine simple: single shot.\\r\\ndeveloped pfiz [ +8970 chars ] ', ' [ removed ] ', 'senators warn republicans conservative speaker wont make dreams come true. new jersey state investigators revisit fatal 2018 crash involving sen. bob menendezs wife. and chicago [ +10352 chars ] ', ' [ removed ] ', `` &lt; ul &gt; &lt; li &gt; new zealand 's national party\\r\\nand act party expected form new government country 's general\\r\\nelection. &lt; /li &gt; &lt; li &gt; this labour leader nz prime minister chris hipkins c [ +2845 chars ] '', `` decreased levels serotonin body may contributing factor development persistence so-called 'long covid ' symptoms, according new study.\\r\\nresearchers suggest, even [ +3726 chars ] '', ' '' long colds '' thing way `` long covid '', people experiencing prolonged symptoms initial infection, according uk study. \\r\\ncommon long cold symptoms inc [ +3999 chars ] ', 'for decades, scientists studying people experience prolonged illnesses wake even mild infections. now, due millions people suffering similar phenomenon [ +5906 chars ] ', 'feeling run-down even kicked infection? it could case long cold, according new study.\\r\\nmuch like long covid, symptoms persist covid-19 infection clear [ +4173 chars ] ', 'vanessa hullon plays form rugby multiple times week, whether international tag rugby, leinster provincial level, rugby union old belvedere rugby league exiles. so wh [ +7789 chars ] ', 'skip content\\r\\npfizer sell covid-19 antiviral treatment twice price sold federal government transitions toward commercial market.\\r\\nsince p [ +2448 chars ] ', 'the price lifesaving covid-19 medication paxlovid likely rise next year patients united states continues transition emergency phase pandemic, sparkin [ +5821 chars ] ', 'parents young children scrambling find still-scarce doses updated covid-19 vaccine, recommended mid-september everyone ages 6 months older.\\r\\nwhen find [ +12101 chars ] ', 'with robert king, megan r. wilson erin schumaker \\r\\ndriving the day\\r\\npresident joe biden wants expand access mental health care, insurers argue proposed rules could uninten [ +11758 chars ] ' ]"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This data originally comes from survey data sourced by the CDC. Initially, we can obtain the macro’s of the data as follows:\n\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nFrom this description of the data, we can see issues with NA values, data types and string data. In order to clean the data, I did the following. You can find the documented code for this cleaning process below.\n\nDrop columns related to quartiles as they are irrelevant\nCreate key values that correspond to the unique values of the indicator column since the descriptions of each category are quite long.\nDropped interval columns since the data was mentioned in other columns\nConvert start and end data to datetime\nRemoved “By” in the group values\nRemove all rows where the value column was NA, thus eliminating all NA values in the dataset. 12.5% of the original dataset was filtered out.\n\n\n\nCode\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n\n#We also need to check for any na values \n\n#cdc %&gt;% summarise_all(~ sum(is.na(.)))\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\n\n# cdc %&gt;% summarise_all(~ sum(is.na(.)))\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset. \n\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")\n\n\n\n\n\nCDC Keys\n\n\nAs described in step two, I have created a key for the descriptive categories within the dataset. The chart above shows the keys and their respective description.\nCDC Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#cdc---long-covid",
    "href": "data-cleaning.html#cdc---long-covid",
    "title": "Data Cleaning",
    "section": "",
    "text": "This data originally comes from survey data sourced by the CDC. Initially, we can obtain the macro’s of the data as follows:\n\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nFrom this description of the data, we can see issues with NA values, data types and string data. In order to clean the data, I did the following. You can find the documented code for this cleaning process below.\n\nDrop columns related to quartiles as they are irrelevant\nCreate key values that correspond to the unique values of the indicator column since the descriptions of each category are quite long.\nDropped interval columns since the data was mentioned in other columns\nConvert start and end data to datetime\nRemoved “By” in the group values\nRemove all rows where the value column was NA, thus eliminating all NA values in the dataset. 12.5% of the original dataset was filtered out.\n\n\n\nCode\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n\n#We also need to check for any na values \n\n#cdc %&gt;% summarise_all(~ sum(is.na(.)))\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\n\n# cdc %&gt;% summarise_all(~ sum(is.na(.)))\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset. \n\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")\n\n\n\n\n\nCDC Keys\n\n\nAs described in step two, I have created a key for the descriptive categories within the dataset. The chart above shows the keys and their respective description.\nCDC Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#long-covid-symptoms---uk",
    "href": "data-cleaning.html#long-covid-symptoms---uk",
    "title": "Data Cleaning",
    "section": "Long Covid Symptoms - UK:",
    "text": "Long Covid Symptoms - UK:\nAmongst the data obtained through the UK survey, I will be focusing on survey data regarding symptoms related to prior health as well as employement status. The raw data would be classified as very messy, with most of the column names unreadable and data types unclear. Thus, I did the following for both datasets in order to clean the data.\n\nSince the formatting of the excel sheet is not in a traditional record format, we’ll need to remove some empty columns. Thus, I removed the first four rows, since they were empty.\nNext, we’ll make the top row of the dataframe and make those values the column names.\nUsing the janitor package in R, we’ll also make the column names “tidy”. This means the names will be lower case without any spaces.\nThe columns estimate, lower confidence bound, and upper confidence bound need to be datatype double, so we will cats those columns.\nChecking for NA values; if there are NA values in the estimate column, this means that that group’s data was not collected in the survey. Thus, we’ll drop those rows and report the loss of data (see code).\nLastly, we’ll save the cleaned datasets into csv files.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\nlong_covid_uk_health &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 3')\nlong_covid_uk_job &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 4')\n\n#For Long Covid with regards to Health \n\n#First, we'll remove the first three rows since they're all empty. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\n#Checking for NA values:\nlong_covid_uk_health %&gt;%\n  summarise_all(~ sum(is.na(.)))\n#No NA values! \n\n#Let's see the cleaned data: \nhead(long_covid_uk_health)\nwrite.csv(long_covid_uk_health, \"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\n\n#For Long Covid with regards to Employement Status:\n\n#Now, we'll do the same cleaning for employement status data since it's the same format:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\n#Checking for NA values:\nlong_covid_uk_job %&gt;%\n  summarise_all(~ sum(is.na(.)))\n#There are a significant number of NA values for estimate and the confidence intervals. \n#Since these are for specific groups, the NA's mean that data was not collected for these groups. \n#Since we can't use \"other\" groups to estimate this data, we will drop these rows. \n#This results is a loss of 13% of the original dataset. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!is.na(estimate))\n\nhead(long_covid_uk_job)\nwrite.csv(long_covid_uk_job, \"../../data/01-modified-data/long_covid_uk_job_clean.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLong Covid UK Survey Data based on Health History - Cleaned\n\n\nUK Survey Health Cleaned Data\n\n\n\nLong Covid UK Survey Data based on Employement Status - Cleaned\n\n\nUK Survey Employement Status Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#long-covid-news",
    "href": "data-cleaning.html#long-covid-news",
    "title": "Data Cleaning",
    "section": "Long Covid News:",
    "text": "Long Covid News:\nI used the NewsAPI to collect current news information surrounding long covid. Since the data came from an API, there isn’t too much cleaning needed to be done. For now, we’ll visualize word frequency amongst titles of news articles.\n\n\nCode\nnews = pd.read_csv('../../data/00-raw-data/long_covid_news_raw.csv')\ntext = news['title'].tolist()\ntext.append(news['content'].tolist())\ntext = ' '.join([str(elem) for elem in text])\n\n# #FILTER OUT UNWANTED CHARACTERS\nnew_text=\"\"\nfor character in text:\n    if character in string.printable:\n        new_text+=character\ntext=new_text\n\n# #FILTER OUT UNWANTED WORDS\nnew_text=\"\"\nfor word in nltk.tokenize.word_tokenize(text):\n    if word not in nltk.corpus.stopwords.words('english'):\n        if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n            #remove the last space\n            new_text=new_text[0:-1]+word+\" \"\n        else: #add a space\n            new_text+=word.lower()+\" \"\ntext=new_text\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ngenerate_word_cloud(text)\n\n\n\n\n\nAfter cleaning the data containing the title and content for each news report, I created a wordcloud displaying the cleaned text frequencies. As we can see, words relating to boosters, symptoms, and overall health are the most frequent after long covid. Further analysis will be needed in order to showcase sentiment within the news surrounding long covid as well as topics for symptoms, if they are reported.\nNews Cleaned Data"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In order to understand long covid, I will be gathering data from a multitude of sources."
  },
  {
    "objectID": "data-gathering.html#record-data-from-cdc---long-covid",
    "href": "data-gathering.html#record-data-from-cdc---long-covid",
    "title": "Data Gathering",
    "section": "Record data from CDC - Long Covid:",
    "text": "Record data from CDC - Long Covid:\nThe CDC released the results of a Household Pulse Survey, where they records results of post covid-19 symptoms across multiple demographics including age, sex, gender, and location. These survey results were recorded over a period of time. Using the open SODA API and the integration in R via RSocrata, I was able to obtain this data. A sample of the raw dataset is show below.\n\n\nCode\nlibrary(RSocrata)\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\ncdc &lt;- read.socrata('https://data.cdc.gov/resource/gsea-w83j.csv')\nwrite.csv(cdc, \"../../data/00-raw-data/cdc_raw.csv\", row.names = FALSE)\n\n\n\n\n\nCDC Survey Data\n\n\nCDC Raw Data"
  },
  {
    "objectID": "data-gathering.html#long-covid-symptoms-in-uk",
    "href": "data-gathering.html#long-covid-symptoms-in-uk",
    "title": "Data Gathering",
    "section": "Long Covid Symptoms in UK",
    "text": "Long Covid Symptoms in UK\nThe COVID-19 and Respiratory Infections Survey (CRIS) was used to obtain self-reported persisting symptoms of long covid within the UK. This dataset was downloaded from the Office of National Statistics (UK).\nClick here to view the source\n\n\n\nUK Long Covid Survey Data Based on Health\n\n\nUK Survey Raw Data"
  },
  {
    "objectID": "data-gathering.html#news-api",
    "href": "data-gathering.html#news-api",
    "title": "Data Gathering",
    "section": "News API:",
    "text": "News API:\nUsing the NewsAPI, I will extract media coverage with the key phrase “long covid” for the past month.\n\n\nCode\nfrom newsapi import NewsApiClient\nimport pandas as pd\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\ndate = date.today()\ndate_past = date - relativedelta(months=1)\n\n# Init\nnewsapi = NewsApiClient(api_key= API_KEY)\n\nsources = newsapi.get_sources()\nsources = pd.DataFrame(sources['sources'])\nsources = sources[(sources['language'] == 'en')]\n\ndf_sources = ', '.join(sources['id'].astype(str))\ndf_domains = ', '.join(sources['url'].astype(str))\n\n\n# /v2/everything\nall_articles = newsapi.get_everything(q='\"long covid\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param= date_past,\n                                      to= date,\n                                      language= 'en',\n                                      sort_by='relevancy')\n\nlong_covid_news = pd.DataFrame(all_articles['articles'])\nlong_covid_news[['id', 'name']] = pd.DataFrame(long_covid_news['source'].tolist())\nlong_covid_news.drop(columns=['source'], inplace=True)\nlong_covid_news = long_covid_news[['id','name','author','title','description','url','publishedAt','content']]\n\n\nlong_covid_news.to_csv('../../data/00-raw-data/long_covid_news_raw.csv', index=False)\n\nlong_covid_news.head()\n\n\n\n\n\nNews API Long Covid Data - Raw\n\n\nNews Raw Data"
  },
  {
    "objectID": "data-gathering.html#white-house-perspective-of-long-covid-and-necessary-steps",
    "href": "data-gathering.html#white-house-perspective-of-long-covid-and-necessary-steps",
    "title": "Data Gathering",
    "section": "White House perspective of Long Covid and necessary steps:",
    "text": "White House perspective of Long Covid and necessary steps:\nThe White House released a statement regarding long covid as well as the nessesary steps to tackle the diagnosis. In order to understand the government’s opinion of long covid, I will conduct a sentiment analysis on this statement. You can find the statement here.\n\n\n\nThe White House\n\n\nWhite House Raw Data"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "cleaning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\ncdc &lt;- read.csv(\"../../data/00-raw-data/cdc_raw.csv\")\nstr(cdc)\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n#We also need to check for any na values \ncdc %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n  indicator group state subgroup phase time_period time_period_start_date\n1         0     0     0        0     0           0                      0\n  time_period_end_date value lowci highci key\n1                    0  1399  1399   1399   0\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\ncdc %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n  indicator group state subgroup phase time_period time_period_start_date\n1         0     0     0        0     0           0                      0\n  time_period_end_date value lowci highci key\n1                    0     0     0      0   0\n\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset.\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nhead(cdc)\n\n              group         state      subgroup phase time_period\n1 National Estimate United States United States   3.5          46\n2               Age United States 18 - 29 years   3.5          46\n3               Age United States 30 - 39 years   3.5          46\n4               Age United States 40 - 49 years   3.5          46\n5               Age United States 50 - 59 years   3.5          46\n6               Age United States 60 - 69 years   3.5          46\n  time_period_start_date time_period_end_date value lowci highci key\n1             2022-06-01           2022-06-13  14.0  13.5   14.5   1\n2             2022-06-01           2022-06-13  17.8  15.9   19.8   1\n3             2022-06-01           2022-06-13  15.2  14.1   16.2   1\n4             2022-06-01           2022-06-13  16.9  15.7   18.3   1\n5             2022-06-01           2022-06-13  15.3  14.1   16.7   1\n6             2022-06-01           2022-06-13  10.9   9.8   12.0   1\n\ncdc_keys\n\n                                                                                                  indicator\n1                                                Ever experienced long COVID, as a percentage of all adults\n2                                 Ever experienced long COVID, as a percentage of adults who ever had COVID\n3                                          Currently experiencing long COVID, as a percentage of all adults\n4                           Currently experiencing long COVID, as a percentage of adults who ever had COVID\n5                                                                                            Ever had COVID\n6         Any activity limitations from long COVID, as a percentage of adults who currently have long COVID\n7                                   Any activity limitations from long COVID, as a percentage of all adults\n8 Significant activity limitations from long COVID, as a percentage of adults who currently have long COVID\n9                           Significant activity limitations from long COVID, as a percentage of all adults\n  key\n1   1\n2   2\n3   3\n4   4\n5   5\n6   6\n7   7\n8   8\n9   9\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")"
  },
  {
    "objectID": "cleaning.html#symptoms",
    "href": "cleaning.html#symptoms",
    "title": "cleaning",
    "section": "Symptoms:",
    "text": "Symptoms:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlong_covid_uk_health &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 3')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n\nlong_covid_uk_job &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 4')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n\n\n\n#For Long Covid with regards to Health \n\n#First, we'll remove the first three rows since they're all empty. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n\n#Next, we'll make top row the column names:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  clean_names()\n\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `estimate = as.double(estimate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `lower_95_percent_confidence_limit =\n  as.double(lower_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `upper_95_percent_confidence_limit =\n  as.double(upper_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n\n#Checking for NA values:\nlong_covid_uk_health %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n# A tibble: 1 × 6\n  symptom domain group estimate lower_95_percent_confid…¹ upper_95_percent_con…²\n    &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;                     &lt;int&gt;                  &lt;int&gt;\n1       0      0     0        8                         8                      8\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\n#No NA values! \n\n#Let's see the cleaned data: \nhead(long_covid_uk_health)\n\n# A tibble: 6 × 6\n  symptom    domain group estimate lower_95_percent_con…¹ upper_95_percent_con…²\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Abdominal… All p… All …     9.56                   8.82                  10.3 \n2 Abdominal… Healt… No h…     6.96                   6.04                   7.87\n3 Abdominal… Healt… Acti…     6.06                   4.26                   7.85\n4 Abdominal… Healt… Acti…    11.3                    9.7                   12.9 \n5 Abdominal… Healt… Acti…    17.2                   14.8                   19.6 \n6 Abdominal… Exten… Not …     2.29                   1.51                   3.07\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\nwrite.csv(long_covid_uk_health, \"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\n\n\n#Now, we'll do the same cleaning for employement status data since it's the same format:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `estimate = as.double(estimate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `lower_95_percent_confidence_limit =\n  as.double(lower_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `upper_95_percent_confidence_limit =\n  as.double(upper_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n#Checking for NA values:\nlong_covid_uk_job %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n# A tibble: 1 × 6\n  symptom domain group estimate lower_95_percent_confid…¹ upper_95_percent_con…²\n    &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;                     &lt;int&gt;                  &lt;int&gt;\n1       0      0     0      111                       111                    111\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\n#There are a significant number of NA values for estimate and the confidence intervals. \n#Since these are for specific groups, the NA's mean that data was not collected for these groups. \n#Since we can't use \"other\" groups to estimate this data, we will drop these rows. \n#This results is a loss of 13% of the original dataset. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!is.na(estimate))\n\nhead(long_covid_uk_job)\n\n# A tibble: 6 × 6\n  symptom    domain group estimate lower_95_percent_con…¹ upper_95_percent_con…²\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Abdominal… All p… All …     9.56                   8.82                  10.3 \n2 Abdominal… Emplo… Empl…     8.9                    7.92                   9.89\n3 Abdominal… Emplo… Inac…    18.6                   15.8                   21.5 \n4 Abdominal… Emplo… Inac…     7.5                    6.34                   8.66\n5 Abdominal… Self-… Empl…     9.04                   7.97                  10.1 \n6 Abdominal… Self-… Self…     8.05                   5.56                  10.6 \n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\nwrite.csv(long_covid_uk_job, \"../../data/01-modified-data/long_covid_uk_job_clean.csv\")"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "naive-bayes.html#preparing-the-data",
    "href": "naive-bayes.html#preparing-the-data",
    "title": "Naive Bayes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nFirst, we’ll need to prepare both the labeled text data and the labeled record data. This includes properly formatting and cleaning the data as well as breaking the dataset into train, validate, and testing sets.\n\nLabeled Text Data\nTo understand the nature of long covid reporting, we will also need to take a look at the differences between long covid reporting and the reporting of other illnesses or diseases. Thus, let’s expand the previous Long-Covid News dataset to also include articles related to covid and influenza. Using this dataset will alllow us to label the articles with the following topics: long covid, coronavirus, and inlfuenza, labeled 1,2, and 3 respectively. We will use these labels within our Naive Bayes classifier in order to accurately predict long covid messaging within the media.\nLet’s first start by agumenting the data as previously described and cleaning the text.\n\n\nCode\nAPI_KEY = '8491eeb10bba463a9665a53da25b1c69'\n\nfrom newsapi import NewsApiClient\nimport pandas as pd\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\ndate = date.today()\ndate_past = date - relativedelta(months=1)\n\n# Init\nnewsapi = NewsApiClient(api_key= API_KEY)\n\nsources = newsapi.get_sources()\nsources = pd.DataFrame(sources['sources'])\nsources = sources[(sources['language'] == 'en')]\n\ndf_sources = ', '.join(sources['id'].astype(str))\ndf_domains = ', '.join(sources['url'].astype(str))\n\n\n# /v2/everything\np1 = newsapi.get_everything(q='\"long covid\"',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\np2 = newsapi.get_everything(q='coronavirus',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\np3 = newsapi.get_everything(q='influenza',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\n#Concating the data\n\nlong_covid_news = pd.DataFrame(p1['articles'])\nlong_covid_news['topic'] = 1\ncovid_news = pd.DataFrame(p2['articles'])\ncovid_news['topic'] = 2\ninfluenza_news = pd.DataFrame(p3['articles'])\ninfluenza_news['topic'] = 3\nall_news = pd.concat([long_covid_news, covid_news, influenza_news], axis=0)\n\nall_news[['id', 'name']] = pd.DataFrame(all_news['source'].tolist())\nall_news.drop(columns=['source'], inplace=True)\nall_news = all_news[['id','name','author','title','description','url','publishedAt','content','topic']]\nall_news = all_news[all_news['content'] != '[Removed]']\n\ndef clean_text(text):\n    # FILTER OUT UNWANTED CHARACTERS\n    new_text = \"\"\n    for character in text:\n        if character in string.printable:\n            new_text += character\n    \n    # FILTER OUT UNWANTED WORDS\n    new_text = \"\"\n    for word in nltk.tokenize.word_tokenize(text):\n        if word not in nltk.corpus.stopwords.words('english'):\n            if word in [\".\", \",\", \"!\", \"?\", \":\", \";\"]:\n                # remove the last space\n                new_text = new_text[0:-1] + word + \" \"\n            else:  # add a space\n                new_text += word.lower() + \" \"\n    \n    return new_text.strip()  # Remove leading/trailing spaces\n\nall_news['title'] = all_news['title'].apply(clean_text)\nall_news['description'] = all_news['description'].apply(clean_text)\nall_news['content'] = all_news['content'].apply(clean_text)\n\nall_news['combined_text'] = all_news.apply(lambda row: ' '.join([str(row['title']), str(row['description']), str(row['content'])]), axis=1)\n\n\n\nFeature Selection for Text Data\nUsing the text data provided from the News API, we’ll first conduct a feature selection on the data in order to determine the words/phrases that are highly correlated with the classification label. The following code goes through this feature selection.\n\n\nCode\nnews = all_news['combined_text'].tolist()\ny = all_news['topic'].tolist()\ny=np.array(y)\n\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(news,MAX_FEATURES=10000)\n\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\ndf2.columns = range(df2.columns.size)\nx=df2.to_numpy()\n\n\nTraining/Testing and Results: Now, we’ll take the selected features and run a Multinomial Navie Bayes model in order to classify the test and determine the accuracy.\n\n\nCode\n#Split to train and test data\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval, y_test, yp_test)\n\n\n#TEST\nacc_train,acc_test,time_train,time_eval,y_test, y_pred =train_MNB_model(x,y,i_print=False)\nprint(\"Testing Accuracy: \", acc_test)\nprint(\"Testing Time: \", time_eval)\n\n\nTesting Accuracy:  76.59574468085107\nTesting Time:  0.0033729999999998483\n\n\nWe can see that the testing accuracy score is fairly high, but it could be better. Let’s analyze this further through some visualizations:\n\n\nCode\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\nCode\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval,y_test,y_pred)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=50, min_index=0, max_index=500)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=10, min_index=500, max_index=5000)\n\n#PLOT-1\nplt.plot(num_features,train_accuracies,'-or')\nplt.plot(num_features,test_accuracies,'-ob')\nplt.xlabel('Number of features')\nplt.ylabel('ACCURACY: Training (blue) and Test (red)')\nplt.show()\n\n# #PLOT-2\nplt.plot(num_features,train_time,'-or')\nplt.plot(num_features,eval_time,'-ob')\nplt.xlabel('Number of features')\nplt.ylabel('Runtime: training time (red) and evaluation time(blue)')\nplt.show()\n\n# #PLOT-3\nplt.plot(np.array(test_accuracies),train_time,'-or')\nplt.plot(np.array(test_accuracies),eval_time,'-ob')\nplt.xlabel('test_accuracies')\nplt.ylabel('Runtime: training time (red) and evaluation time (blue)')\nplt.show()\n\n# #PLOT-3\nplt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\nplt.xlabel('Number of features')\nplt.ylabel('train_accuracies-test_accuracies')\nplt.show()\n\n\n5 50 50 77.17391304347827 76.59574468085107\n10 100 100 78.80434782608695 72.3404255319149\n15 150 150 80.97826086956522 74.46808510638297\n20 200 200 82.06521739130434 72.3404255319149\n25 250 250 83.15217391304348 72.3404255319149\n30 300 300 84.78260869565217 72.3404255319149\n35 350 350 86.95652173913044 74.46808510638297\n40 400 400 88.04347826086956 74.46808510638297\n45 450 450 88.58695652173914 74.46808510638297\n50 500 500 90.21739130434783 76.59574468085107\n5 2750 2750 96.19565217391305 76.59574468085107\n10 5000 2917 96.19565217391305 76.59574468085107\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these images, we can see that the confusion matrix is not very clear in its correlation and the training and testing accuracies are widely different (however, both level out ater 1000 features). This can due to a number of things. One: the data in general is too similar. This may mean that long covid reporting is on par with that of other diseases. Two: the data is too small. The small dataset with varying text can make it difficult for the Multinomial model to accuratly predict classifications. Third: the text is not clear or unclean. While we did do an initial cleaning of the data, it may not have been sufficient for classification. Thus, going forward, we may need to try a different dataset or further clean the text for better results.\n\n\n\nLabeled Record Data\nFirst, let’s prepare the data for Naive Bayes Classification. Please note that since our feature set is very small (n=3), we will use all the features for our classifier. If the feature set was larger, we would use sklearn’s feature selection tool in order to identify the best features to represent the data without being cross-correlated.\n\n\nCode\ncdc = pd.read_csv(\"../../data/01-modified-data/cdc_clean.csv\")\ncdc_naive = cdc[['group', 'subgroup', 'value', 'key']]\ncdc_naive['group'] = cdc_naive['group'].astype('category').cat.codes\ncdc_naive['subgroup'] = cdc_naive['subgroup'].astype('category').cat.codes\n\n\nX_train, X_test, y_train, y_test = train_test_split(cdc_naive[['group', 'subgroup', 'value']], cdc_naive['key'], test_size = 0.2, random_state = 5000)\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_predicted = classifier.predict(X_test)\nprint(\"The accuray of the multinomial naive bayes model is: \", accuracy_score(y_test, y_predicted))\n\n# Classfier report\nprint(classification_report(y_test, y_predicted))\n\n\nThe accuray of the multinomial naive bayes model is:  0.6938877755511023\n              precision    recall  f1-score   support\n\n           1       0.58      0.77      0.66       264\n           2       0.63      0.78      0.70       265\n           3       0.59      0.62      0.60       270\n           4       0.61      0.32      0.42       250\n           5       0.90      0.92      0.91       278\n           6       1.00      0.96      0.98       119\n           7       0.59      0.68      0.63       208\n           8       0.29      0.12      0.17       123\n           9       0.95      0.93      0.94       219\n\n    accuracy                           0.69      1996\n   macro avg       0.68      0.68      0.67      1996\nweighted avg       0.68      0.69      0.68      1996\n\n\n\nAfter training the classifier on our training data and running predictions of the test set, we can see that the Guassian Naive Bayes classifier had an accuracy score of approximately 70%. Thus, given the demographic grouping, subgrouping, and estimated percentage of the survey group, we are able to predict the persons experience with Long Covid with approximately 70% accuracy. Let’s take a closer look at the classifier though visualizations.\n\n\nCode\nconf_matrix = confusion_matrix(y_test, y_predicted)\n\n# Plot the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8'],\n            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\nHere, we can see that the cofusion matrix shows a strong correlation along the diagonal, meaning that the predicted calsses correctly correlate to the actual classes. There are a few cases where the classifier incorrectly classified the objects, such as predicting class 0 instead of class 4 or predicting class 6 instead of class 2. Rather than the classifier itself, this may also be an issue with the ‘closeness’ of these classes.\nWe can also look at the class distribution within the dataset, first overall, and then by feature.\n\n\nCode\nplt.figure(figsize=(6, 4))\ncdc_naive['key'].value_counts().plot(kind='bar', color='green')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('Class Distribution')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\nCode\nfeature_columns = ['group', 'subgroup', 'value']  \nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\n\nfor feature, ax in zip(feature_columns, axs.ravel()):\n    for class_label in cdc_naive['key'].unique():  # Replace 'label_column' with your actual label column name\n        sns.histplot(cdc_naive[cdc_naive['key'] == class_label][feature], kde=True, label=f'Class {class_label}', ax = ax)\n        \n    ax.set_title(f'Distribution of {feature} by Class')\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Density')\n\nplt.show()\n\n\n\n\n\nFrom these visualizations, we can see that class 7 and 8 are the least represented within the daatset, primarily because they represent extreme cases of inactivity due to Long Covid from patients currently experiencing Long Covid and those who no longer experience symptoms. Additionally, and what is more surprising, is that the distributions for each class when viewing through group and subgroup are not nearly as distinct as when viewing classes through the value feature. Thus we can say that the classifier heavily depended on the value feature to predict the classes."
  },
  {
    "objectID": "naive-bayes.html",
    "href": "naive-bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised classification techinque used to classify data based on a training set containing existing labels. At it’s core, Naive Bayes is based on baysian statistics, or more specifically conditional probabilities. Assuming that the features used to describe the data are conditionally independent, Naive Bayes takes the provided class labels and calculates the probability by multiplying the probabilities of each feature occurring given each class. Through this conditional statement, it then chooses the class with the highest probability based on the fetaure values to be the predicted value. Thus, Naive Bayes heavily relies on the features being uncorrelated as well as the training data to accuratly predict the data at hand.\nNavie Bayes is especially useful for uncertain, unknown, or incomplete information. It provides a way to estimate the likelihood of an event based on prior knowledge, making it well-suited for classification tasks. The objective of Naive Bayes classification is to accurately predict the class label of an input data point given its features.\nAdditionally, there are different types of Naive Bayes classifiers tailored for specific types of data. Gaussian Naive Bayes is suited for continuous data, assuming the features are normally distributed. Multinomial Naive Bayes is commonly used for text classification, specifically when dealing to word frequencies. Laslty, Bernoulli Naive Bayes is effective for binary features, making it useful for tasks like spam detection.\nThus, I will be using Naive Bayes (specificaly Gaussian and Multinomial) in order to create classifications for both the CDC survey data as well as media coverage of long covid. Given the nature of Long Covid and its recency, Naive Bayes will be very useful in providing more insight of potentially vulerable groups or grouping of symptoms that should be looked for in people with Long Covid."
  },
  {
    "objectID": "naive-bayes.html#what-is-naive-bayes",
    "href": "naive-bayes.html#what-is-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised classification techinque used to classify data based on a training set containing existing labels. At it’s core, Naive Bayes is based on baysian statistics, or more specifically conditional probabilities. Assuming that the features used to describe the data are conditionally independent, Naive Bayes takes the provided class labels and calculates the probability by multiplying the probabilities of each feature occurring given each class. Through this conditional statement, it then chooses the class with the highest probability based on the fetaure values to be the predicted value. Thus, Naive Bayes heavily relies on the features being uncorrelated as well as the training data to accuratly predict the data at hand.\nNavie Bayes is especially useful for uncertain, unknown, or incomplete information. It provides a way to estimate the likelihood of an event based on prior knowledge, making it well-suited for classification tasks. The objective of Naive Bayes classification is to accurately predict the class label of an input data point given its features.\nAdditionally, there are different types of Naive Bayes classifiers tailored for specific types of data. Gaussian Naive Bayes is suited for continuous data, assuming the features are normally distributed. Multinomial Naive Bayes is commonly used for text classification, specifically when dealing to word frequencies. Laslty, Bernoulli Naive Bayes is effective for binary features, making it useful for tasks like spam detection.\nThus, I will be using Naive Bayes (specificaly Gaussian and Multinomial) in order to create classifications for both the CDC survey data as well as media coverage of long covid. Given the nature of Long Covid and its recency, Naive Bayes will be very useful in providing more insight of potentially vulerable groups or grouping of symptoms that should be looked for in people with Long Covid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nData Science and Analytics (DSAN 5000)\n",
    "section": "",
    "text": "Data Science and Analytics (DSAN 5000)\n\n\nAuthor: Shriya Chinthak\n\n\nNETID: sc2325\n\n\nWelcome to my portfolio for my Data Science and Analytics project! For information about me, please click the About Me link above. On the right, you’ll find a comprehensive list of the projec including an introduction, data sources, processing, and analysis, as well as a conclusion.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "naive-bayes.html#conclude",
    "href": "naive-bayes.html#conclude",
    "title": "Naive Bayes",
    "section": "Conclude",
    "text": "Conclude\nThus, after creating a Guassian and Multinomial Naive Bayes models on CDC and Long Covid media coverage data, we can conclude a few things. Primarily, the news data collected does not seem to be “different” in comparison to coverage of other diseases. This may be due to journalistic style of writing. Thus, we will need to move to more opion based data such as social media text, in order to gauge the varying opinions on the topic. Additionally, for the CDC data, we were able to get fairly accurate results of classification of people with differing long covid experiences based on their demographics. With additional information on symptoms, this classifier could be incredibly useful to help indivduals understand their severity of long covid in comparison to historical records."
  },
  {
    "objectID": "eda-code.html#naive-bayes",
    "href": "eda-code.html#naive-bayes",
    "title": "Long Covid Symptoms - UK",
    "section": "Naive Bayes:",
    "text": "Naive Bayes:\nwe’ll use CDC for record data\n\ncdc = pd.read_csv(\"../../data/01-modified-data/cdc_clean.csv\")\ncdc.drop('Unnamed: 0', axis = 1, inplace= True)\ncdc.head()\n\n\n\n\n\n\n\n\ngroup\nstate\nsubgroup\nphase\ntime_period\ntime_period_start_date\ntime_period_end_date\nvalue\nlowci\nhighci\nkey\n\n\n\n\n0\nNational Estimate\nUnited States\nUnited States\n3.5\n46\n2022-06-01\n2022-06-13\n14.0\n13.5\n14.5\n1\n\n\n1\nAge\nUnited States\n18 - 29 years\n3.5\n46\n2022-06-01\n2022-06-13\n17.8\n15.9\n19.8\n1\n\n\n2\nAge\nUnited States\n30 - 39 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.2\n14.1\n16.2\n1\n\n\n3\nAge\nUnited States\n40 - 49 years\n3.5\n46\n2022-06-01\n2022-06-13\n16.9\n15.7\n18.3\n1\n\n\n4\nAge\nUnited States\n50 - 59 years\n3.5\n46\n2022-06-01\n2022-06-13\n15.3\n14.1\n16.7\n1\n\n\n\n\n\n\n\n\ncdc_naive = cdc[['group', 'subgroup', 'value', 'key']]\ncdc_naive['group'] = cdc_naive['group'].astype('category').cat.codes\ncdc_naive['subgroup'] = cdc_naive['subgroup'].astype('category').cat.codes\n\n/var/folders/hh/g7z_nzzj3b90yyn96_xmf8d40000gn/T/ipykernel_30922/84606103.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cdc_naive['group'] = cdc_naive['group'].astype('category').cat.codes\n/var/folders/hh/g7z_nzzj3b90yyn96_xmf8d40000gn/T/ipykernel_30922/84606103.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cdc_naive['subgroup'] = cdc_naive['subgroup'].astype('category').cat.codes\n\n\n\n\n\n\n\n\n\ngroup\nsubgroup\nvalue\nkey\n\n\n\n\n0\n4\n68\n14.0\n1\n\n\n1\n0\n0\n17.8\n1\n\n\n2\n0\n1\n15.2\n1\n\n\n3\n0\n2\n16.9\n1\n\n\n4\n0\n3\n15.3\n1\n\n\n...\n...\n...\n...\n...\n\n\n9972\n8\n71\n1.7\n9\n\n\n9973\n8\n72\n1.6\n9\n\n\n9974\n8\n73\n2.2\n9\n\n\n9975\n8\n74\n0.8\n9\n\n\n9976\n8\n77\n3.0\n9\n\n\n\n\n9977 rows × 4 columns\n\n\n\n\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nimport sklearn \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(cdc_naive[['group', 'subgroup', 'value']], cdc_naive['key'], test_size = 0.2, random_state = 50)\n\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\ny_predicted = classifier.predict(X_test)\nprint(\"The accuray of the multinomial naive bayes model is: \", accuracy_score(y_test, y_predicted))\n\nThe accuray of the multinomial naive bayes model is:  0.6918837675350702\n\n\nLet’s take a look at the classification report for out model\n\nprint(classification_report(y_test, y_predicted))\n\n              precision    recall  f1-score   support\n\n           1       0.57      0.80      0.67       264\n           2       0.70      0.77      0.74       289\n           3       0.57      0.55      0.56       264\n           4       0.60      0.32      0.42       249\n           5       0.93      0.94      0.93       264\n           6       1.00      0.98      0.99       120\n           7       0.57      0.65      0.61       223\n           8       0.32      0.18      0.23       122\n           9       0.90      0.93      0.91       201\n\n    accuracy                           0.69      1996\n   macro avg       0.68      0.68      0.67      1996\nweighted avg       0.68      0.69      0.68      1996\n\n\n\n\nconf_matrix = confusion_matrix(y_test, y_predicted)\n\n# Plot the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8'],\n            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(6, 4))\ncdc_naive['key'].value_counts().plot(kind='bar', color='green')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('Class Distribution')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\nfeature_columns = ['group', 'subgroup', 'value']  \nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\n\nfor feature, ax in zip(feature_columns, axs.ravel()):\n    for class_label in cdc_naive['key'].unique():  # Replace 'label_column' with your actual label column name\n        sns.histplot(cdc_naive[cdc_naive['key'] == class_label][feature], kde=True, label=f'Class {class_label}', ax = ax)\n        \n    ax.set_title(f'Distribution of {feature} by Class')\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Density')\n\nplt.show()\n\n\n\n\n\n#Let's gather additional data for the textual analysis:\n\nAPI_KEY = '8491eeb10bba463a9665a53da25b1c69'\n\nfrom newsapi import NewsApiClient\nimport pandas as pd\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\ndate = date.today()\ndate_past = date - relativedelta(months=1)\n\n# Init\nnewsapi = NewsApiClient(api_key= API_KEY)\n\nsources = newsapi.get_sources()\nsources = pd.DataFrame(sources['sources'])\nsources = sources[(sources['language'] == 'en')]\n\ndf_sources = ', '.join(sources['id'].astype(str))\ndf_domains = ', '.join(sources['url'].astype(str))\n\n\n# /v2/everything\np1 = newsapi.get_everything(q='\"long covid\"',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\np2 = newsapi.get_everything(q='coronavirus',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\np3 = newsapi.get_everything(q='influenza',\nsources=str(df_sources),\ndomains=str(df_domains),\nfrom_param= date_past,\nto= date,\nlanguage= 'en',\nsort_by='relevancy')\n\n#Concating the data\n\nlong_covid_news = pd.DataFrame(p1['articles'])\nlong_covid_news['topic'] = 1\ncovid_news = pd.DataFrame(p2['articles'])\ncovid_news['topic'] = 2\ninfluenza_news = pd.DataFrame(p3['articles'])\ninfluenza_news['topic'] = 3\nall_news = pd.concat([long_covid_news, covid_news, influenza_news], axis=0)\n\nall_news[['id', 'name']] = pd.DataFrame(all_news['source'].tolist())\nall_news.drop(columns=['source'], inplace=True)\nall_news = all_news[['id','name','author','title','description','url','publishedAt','content','topic']]\nall_news = all_news[all_news['content'] != '[Removed]']\nall_news.head()\n\n\n\n\n\n\n\n\nid\nname\nauthor\ntitle\ndescription\nurl\npublishedAt\ncontent\ntopic\n\n\n\n\n2\ncbs-news\nCBS News\nSara Moniuszko\n\"Long colds\" may exist, researchers find while...\nResearchers found that people with acute respi...\nhttps://www.cbsnews.com/news/long-colds-may-ex...\n2023-10-06T17:27:07Z\nSome people may experience \"long colds,\" or lo...\n1\n\n\n6\ncbs-news\nCBS News\nCaitlin O'Kane\nThreads ban on sensitive search terms is tempo...\nThe list of banned search terms on Threads inc...\nhttps://www.cbsnews.com/news/threads-ban-searc...\n2023-10-18T19:54:56Z\nThe head of Instagram on Tuesday said the app ...\n1\n\n\n7\nusa-today\nUSA Today\nJosie Goodrich\nHow To Cure 'Brain Fog' Using These Super Easy...\nBrain fog is a symptom characterized by feelin...\nhttps://www.usatoday.com/story/life/health-wel...\n2023-10-06T16:40:31Z\nMost people have experienced jet lag that slug...\n1\n\n\n9\nbbc-news\nBBC News\nhttps://www.facebook.com/bbcnews\nCare home boss shocked at speed of Covid variant\nKay Emsley says she was taken by surprised at ...\nhttps://www.bbc.co.uk/news/articles/cqv9j1q7ve7o\n2023-10-16T05:01:02Z\nMs Emsley, 31, said it started with residents ...\n1\n\n\n10\nnewsweek\nNewsweek\nJess Thomson\nAmerican Long Covid Sufferers Preyed on by Unr...\nSixty clinics served by 38 businesses are sell...\nhttps://www.newsweek.com/long-covid-stem-cell-...\n2023-10-26T15:00:01Z\nDozens of health clinics across the country ar...\n1\n\n\n\n\n\n\n\n\nimport string\nimport nltk\n\ndef clean_text(text):\n    # FILTER OUT UNWANTED CHARACTERS\n    new_text = \"\"\n    for character in text:\n        if character in string.printable:\n            new_text += character\n    \n    # FILTER OUT UNWANTED WORDS\n    new_text = \"\"\n    for word in nltk.tokenize.word_tokenize(text):\n        if word not in nltk.corpus.stopwords.words('english'):\n            if word in [\".\", \",\", \"!\", \"?\", \":\", \";\"]:\n                # remove the last space\n                new_text = new_text[0:-1] + word + \" \"\n            else:  # add a space\n                new_text += word.lower() + \" \"\n    \n    return new_text.strip()  # Remove leading/trailing spaces\n\nall_news['title'] = all_news['title'].apply(clean_text)\nall_news['description'] = all_news['description'].apply(clean_text)\nall_news['content'] = all_news['content'].apply(clean_text)\n\n\nall_news['combined_text'] = all_news.apply(lambda row: ' '.join([str(row['title']), str(row['description']), str(row['content'])]), axis=1)\nall_news.tail()\n\n\n\n\n\n\n\n\nid\nname\nauthor\ntitle\ndescription\nurl\npublishedAt\ncontent\ntopic\ncombined_text\n\n\n\n\n95\nabc-news\nABC News\nNone\nnew covid-19 vaccines available oct. 16 n.b., ...\nnew brunswickers book covid-19 vaccine appoint...\nhttps://www.cbc.ca/news/canada/new-brunswick/c...\n2023-10-05T14:23:08Z\nnew brunswickers start booking appointments fr...\n3\nnew covid-19 vaccines available oct. 16 n.b., ...\n\n\n96\nabc-news\nABC News\nNone\ncovid-19 kills 4 new brunswick, virus activity...\ncovid-19 killed least four new brunswickers, h...\nhttps://www.cbc.ca/news/canada/new-brunswick/c...\n2023-10-31T16:15:48Z\ncovid-19 killed least four new brunswickers, h...\n3\ncovid-19 kills 4 new brunswick, virus activity...\n\n\n97\nabc-news\nABC News\nNone\nvaccine fatigue real, says pharmacist, covid-1...\nfall season usually means new season respirato...\nhttps://www.cbc.ca/news/canada/newfoundland-la...\n2023-10-20T20:23:45Z\nthe fall season usually means new season respi...\n3\nvaccine fatigue real, says pharmacist, covid-1...\n\n\n98\nabc-news\nABC News\nDaryl Austin\nnew covid shot game-changer ( term ‘ booster ’...\nlatest vaccine formula available local pharmac...\nhttps://www.nationalgeographic.com/science/art...\n2023-10-03T00:00:00Z\nunlike earlier pandemic, initial vaccines foll...\n3\nnew covid shot game-changer ( term ‘ booster ’...\n\n\n99\nabc-news\nABC News\nhttps://www.facebook.com/bbcnews\nsouth africa egg shortage: poultry products be...\nmillions chickens killed amid one worst bird f...\nhttps://www.bbc.co.uk/news/world-africa-67054197\n2023-10-11T01:37:10Z\neggs currently south africa 's hottest commodi...\n3\nsouth africa egg shortage: poultry products be...\n\n\n\n\n\n\n\n\nnews = all_news['combined_text'].tolist()\ny = all_news['topic'].tolist()\ny=np.array(y)\n\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(news,MAX_FEATURES=10000)\n\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\ndf2.columns = range(df2.columns.size)\nx=df2.to_numpy()\n\n#Split to train and test data\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval, y_test, yp_test)\n\n\n#TEST\nacc_train,acc_test,time_train,time_eval,y_test, y_pred =train_MNB_model(x,y,i_print=False)\nprint(\"Testing Accuracy: \", acc_test)\nprint(\"Testing Time: \", time_eval)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(230, 2890) (230,)\nTesting Accuracy:  73.91304347826086\nTesting Time:  0.003735000000006039"
  }
]