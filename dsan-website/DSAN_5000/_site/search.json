[
  {
    "objectID": "dr.html",
    "href": "dr.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Long Covid"
  },
  {
    "objectID": "introduction.html#summary-of-long-covid",
    "href": "introduction.html#summary-of-long-covid",
    "title": "Introduction",
    "section": "Summary of Long Covid",
    "text": "Summary of Long Covid\nCovid-19, the infectious disease originating from the SARS-Cov-2 virus, was declared a global pandemic in March of 2020. As the illness spread, affecting thousands of people worldwide, a multitude of individuals started reporting lasting symptoms after being cleared of the disease. These varying degrees of symptoms after an extended period of time can be classified as long covid. Specifically, “Long COVID is an often debilitating illness that occurs in at least 10% of severe acute respiratory syndrome coronavirus 2” Davis et al. (2023). Those who have reported long covid face a variety of symptoms ranging from heart, lung, pancreas, immune, and neurological diseases and chronic illnesses. Additionally, those in existing vulnerability groups such as those with type two diabetes, those of the female sex, and those with autoimmune disease to name a few, are also more susceptible to long covid Davis et al. (2023)."
  },
  {
    "objectID": "introduction.html#why-is-it-important",
    "href": "introduction.html#why-is-it-important",
    "title": "Introduction",
    "section": "Why is it Important?",
    "text": "Why is it Important?\nThe study and analysis of long covid across demographics in the United States is vital due to the recency of the pandemic as well as the lack of outreach and knowledge regarding this topic. It is both the biomedical community as well as the public that currently have little knowledge about long covid, the possible symptoms, the vulnerable groups, and the current research in the field Sudre et al. (2021)."
  },
  {
    "objectID": "introduction.html#why-should-you-continue-reading",
    "href": "introduction.html#why-should-you-continue-reading",
    "title": "Introduction",
    "section": "Why should you continue reading?",
    "text": "Why should you continue reading?\nGiven the lack of public knowledge and the widespread impending issue of managing the lasting effects of COVID-19, the study of long covid is essential now more than ever. At the moment, the scientific research in this area is still new and developing, meaning that findings have not made headway for the general audiences. Additionally, there have been very few social science perspectives published regarding the impact of long covid symptoms on the people’s lives or the conversations surrounding this topic in the media. These are topics of conversation which this project hopes to uncover."
  },
  {
    "objectID": "introduction.html#past-works",
    "href": "introduction.html#past-works",
    "title": "Introduction",
    "section": "Past works:",
    "text": "Past works:\nAs previously mentioned, there is still limited research published surrounding long covid in academic and scientific journals. However, the current published works provide a plethora of information surrounding noted symptoms, sentiments on long covid, and more.\nThe authors of Long COVID: major findings, mechanisms and recommendations compiled the findings of several COVID-19 research papers as well as collected data from long covid researchers and clinics in order to create a list of long covid symptoms. Additionally, the authors noted a few challenges in their collection process, one of those being the lack of uniformity in testing, especially during the early stages of COVID-19. They also noted that several non-respiratory symptoms go unlabeled as potential long covid symptoms due to the narrative that COVID is only a respiratory illness Davis et al. (2023). With this being said, the work of Davis et al. (2023) on the topic provides a strong base understanding of long covid and the future research that is needed.\nIn conjunction with the previous research, the authors of Attributes and predictors of long COVID continued research on the documented symptoms of long covid across multiple demographics (age, race, location, sex, etc) and created a random forest predictive model in order to make predictions on symptoms on specific demographics. The primary source of their data was from “The COVID Symptom Study is a mobile application” which allows users to self report “daily information on their health status and symptoms, as well as results of any available COVID-19 test” Sudre et al. (2021). Thus, while they were able to provide an extensive analysis of symptoms, this data is only a small subset of the population using the application."
  },
  {
    "objectID": "introduction.html#what-am-i-exploring",
    "href": "introduction.html#what-am-i-exploring",
    "title": "Introduction",
    "section": "What am I exploring?",
    "text": "What am I exploring?\nFor this project, I would like to focus on creating a succinct understanding of long covid, the demographics affected, and the current knowledge base and media presence in order to provide this information to the public. Through data gathering, exploratory analysis, visualizations, and text analysis, I plan to answer a myriad of questions surrounding long covid and present the current information base in a way that is transparent and accessible."
  },
  {
    "objectID": "introduction.html#questions-to-address",
    "href": "introduction.html#questions-to-address",
    "title": "Introduction",
    "section": "Questions to Address:",
    "text": "Questions to Address:\n\nHow does gender and age affect one’s chances of experiencing long covid symptoms?\nWhich pre-existing chronic illnesses make one more prone to experiencing long covid symptoms?\nIn what ways does race affect the persistence of long covid symptoms?\now might the existing research on long covid symptoms be skewed to respiratory illnesses?\nWhat is the most common collection of symptoms present in someone with long covid?\nWhat is the sentiment of current media on the topic of long covid?\nCan we create a way to predict symptoms of long covid for demographics?\nHow have the reported symptoms of long covid changed over time?\nWhat are the current resources to assist with long covid and are they easily accessible?\nWhat is government opinion (written) on the state of long covid research/information?"
  },
  {
    "objectID": "introduction.html#goals-and-hypotheses",
    "href": "introduction.html#goals-and-hypotheses",
    "title": "Introduction",
    "section": "Goals and Hypotheses:",
    "text": "Goals and Hypotheses:\nThrough this study of long covid across a multitude of demographics, I hope to answer the above questions as well as provide a concise and coherent way to view information on long covid.\nAs of now, I can hypothesize that existing minority and disadvantaged populations experience more severe long covid symptoms due to the lack of public information, biomedical education to service these communities, the lack of aid from the government in assisting those with chronic illness due to these lasting effects of COVID-19."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nData Science and Analytics (DSAN 5000)\n",
    "section": "",
    "text": "Data Science and Analytics (DSAN 5000)\n\n\nAuthor: Shriya Chinthak\n\n\nNETID: sc2325\n\n\nWelcome to my portfolio for my Data Science and Analytics project! For information about me, please click the About Me link above. On the right, you’ll find a comprehensive list of the projec including an introduction, data sources, processing, and analysis, as well as a conclusion.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data-exploration.html#long-covid-symptoms---uk",
    "href": "data-exploration.html#long-covid-symptoms---uk",
    "title": "Data Exploration",
    "section": "Long Covid Symptoms - UK:",
    "text": "Long Covid Symptoms - UK:"
  },
  {
    "objectID": "data-exploration.html#long-covid-news",
    "href": "data-exploration.html#long-covid-news",
    "title": "Data Exploration",
    "section": "Long Covid News:",
    "text": "Long Covid News:"
  },
  {
    "objectID": "data-exploration.html#presidential-address",
    "href": "data-exploration.html#presidential-address",
    "title": "Data Exploration",
    "section": "Presidential Address:",
    "text": "Presidential Address:"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In order to understand long covid, I will be gathering data from a multitude of sources."
  },
  {
    "objectID": "data-gathering.html#record-data-from-cdc---long-covid",
    "href": "data-gathering.html#record-data-from-cdc---long-covid",
    "title": "Data Gathering",
    "section": "Record data from CDC - Long Covid:",
    "text": "Record data from CDC - Long Covid:\nThe CDC released the results of a Household Pulse Survey, where they records results of post covid-19 symptoms across multiple demographics including age, sex, gender, and location. These survey results were recorded over a period of time. Using the open SODA API and the integration in R via RSocrata, I was able to obtain this data. A sample of the raw dataset is show below.\n\n\nCode\nlibrary(RSocrata)\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\ncdc &lt;- read.socrata('https://data.cdc.gov/resource/gsea-w83j.csv')\nwrite.csv(cdc, \"../../data/00-raw-data/cdc_raw.csv\", row.names = FALSE)\n\n\n\n\n\nCDC Survey Data\n\n\nCDC Raw Data"
  },
  {
    "objectID": "data-gathering.html#long-covid-symptoms-in-uk",
    "href": "data-gathering.html#long-covid-symptoms-in-uk",
    "title": "Data Gathering",
    "section": "Long Covid Symptoms in UK",
    "text": "Long Covid Symptoms in UK\nThe COVID-19 and Respiratory Infections Survey (CRIS) was used to obtain self-reported persisting symptoms of long covid within the UK. This dataset was downloaded from the Office of National Statistics (UK).\nClick here to view the source\n\n\n\nUK Long Covid Survey Data Based on Health\n\n\nUK Survey Raw Data"
  },
  {
    "objectID": "data-gathering.html#news-api",
    "href": "data-gathering.html#news-api",
    "title": "Data Gathering",
    "section": "News API:",
    "text": "News API:\nUsing the NewsAPI, I will extract media coverage with the key phrase “long covid” for the past month.\n\n\nCode\nfrom newsapi import NewsApiClient\nimport pandas as pd\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\ndate = date.today()\ndate_past = date - relativedelta(months=1)\n\n# Init\nnewsapi = NewsApiClient(api_key= API_KEY)\n\nsources = newsapi.get_sources()\nsources = pd.DataFrame(sources['sources'])\nsources = sources[(sources['language'] == 'en')]\n\ndf_sources = ', '.join(sources['id'].astype(str))\ndf_domains = ', '.join(sources['url'].astype(str))\n\n\n# /v2/everything\nall_articles = newsapi.get_everything(q='\"long covid\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param= date_past,\n                                      to= date,\n                                      language= 'en',\n                                      sort_by='relevancy')\n\nlong_covid_news = pd.DataFrame(all_articles['articles'])\nlong_covid_news[['id', 'name']] = pd.DataFrame(long_covid_news['source'].tolist())\nlong_covid_news.drop(columns=['source'], inplace=True)\nlong_covid_news = long_covid_news[['id','name','author','title','description','url','publishedAt','content']]\n\n\nlong_covid_news.to_csv('../../data/00-raw-data/long_covid_news_raw.csv', index=False)\n\nlong_covid_news.head()\n\n\n\n\n\nNews API Long Covid Data - Raw\n\n\nNews Raw Data"
  },
  {
    "objectID": "data-gathering.html#white-house-perspective-of-long-covid-and-necessary-steps",
    "href": "data-gathering.html#white-house-perspective-of-long-covid-and-necessary-steps",
    "title": "Data Gathering",
    "section": "White House perspective of Long Covid and necessary steps:",
    "text": "White House perspective of Long Covid and necessary steps:\nThe White House released a statement regarding long covid as well as the nessesary steps to tackle the diagnosis. In order to understand the government’s opinion of long covid, I will conduct a sentiment analysis on this statement. You can find the statement here.\n\n\n\nThe White House\n\n\nWhite House Raw Data"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This data originally comes from survey data sourced by the CDC. Initially, we can obtain the macro’s of the data as follows:\n\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nFrom this description of the data, we can see issues with NA values, data types and string data. In order to clean the data, I did the following. You can find the documented code for this cleaning process below.\n\nDrop columns related to quartiles as they are irrelevant\nCreate key values that correspond to the unique values of the indicator column since the descriptions of each category are quite long.\nDropped interval columns since the data was mentioned in other columns\nConvert start and end data to datetime\nRemoved “By” in the group values\nRemove all rows where the value column was NA, thus eliminating all NA values in the dataset. 12.5% of the original dataset was filtered out.\n\n\n\nCode\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n\n#We also need to check for any na values \n\n#cdc %&gt;% summarise_all(~ sum(is.na(.)))\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\n\n# cdc %&gt;% summarise_all(~ sum(is.na(.)))\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset. \n\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")\n\n\n\n\n\nCDC Keys\n\n\nAs described in step two, I have created a key for the descriptive categories within the dataset. The chart above shows the keys and their respective description.\nCDC Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#cdc---long-covid",
    "href": "data-cleaning.html#cdc---long-covid",
    "title": "Data Cleaning",
    "section": "",
    "text": "This data originally comes from survey data sourced by the CDC. Initially, we can obtain the macro’s of the data as follows:\n\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nFrom this description of the data, we can see issues with NA values, data types and string data. In order to clean the data, I did the following. You can find the documented code for this cleaning process below.\n\nDrop columns related to quartiles as they are irrelevant\nCreate key values that correspond to the unique values of the indicator column since the descriptions of each category are quite long.\nDropped interval columns since the data was mentioned in other columns\nConvert start and end data to datetime\nRemoved “By” in the group values\nRemove all rows where the value column was NA, thus eliminating all NA values in the dataset. 12.5% of the original dataset was filtered out.\n\n\n\nCode\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n\n#We also need to check for any na values \n\n#cdc %&gt;% summarise_all(~ sum(is.na(.)))\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\n\n# cdc %&gt;% summarise_all(~ sum(is.na(.)))\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset. \n\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")\n\n\n\n\n\nCDC Keys\n\n\nAs described in step two, I have created a key for the descriptive categories within the dataset. The chart above shows the keys and their respective description.\nCDC Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#long-covid-symptoms---uk",
    "href": "data-cleaning.html#long-covid-symptoms---uk",
    "title": "Data Cleaning",
    "section": "Long Covid Symptoms - UK:",
    "text": "Long Covid Symptoms - UK:\nAmongst the data obtained through the UK survey, I will be focusing on survey data regarding symptoms related to prior health as well as employement status. The raw data would be classified as very messy, with most of the column names unreadable and data types unclear. Thus, I did the following for both datasets in order to clean the data.\n\nSince the formatting of the excel sheet is not in a traditional record format, we’ll need to remove some empty columns. Thus, I removed the first four rows, since they were empty.\nNext, we’ll make the top row of the dataframe and make those values the column names.\nUsing the janitor package in R, we’ll also make the column names “tidy”. This means the names will be lower case without any spaces.\nThe columns estimate, lower confidence bound, and upper confidence bound need to be datatype double, so we will cats those columns.\nChecking for NA values; if there are NA values in the estimate column, this means that that group’s data was not collected in the survey. Thus, we’ll drop those rows and report the loss of data (see code).\nLastly, we’ll save the cleaned datasets into csv files.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\nlong_covid_uk_health &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 3')\nlong_covid_uk_job &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 4')\n\n#For Long Covid with regards to Health \n\n#First, we'll remove the first three rows since they're all empty. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\n#Checking for NA values:\nlong_covid_uk_health %&gt;%\n  summarise_all(~ sum(is.na(.)))\n#No NA values! \n\n#Let's see the cleaned data: \nhead(long_covid_uk_health)\nwrite.csv(long_covid_uk_health, \"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\n\n#For Long Covid with regards to Employement Status:\n\n#Now, we'll do the same cleaning for employement status data since it's the same format:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\n#Checking for NA values:\nlong_covid_uk_job %&gt;%\n  summarise_all(~ sum(is.na(.)))\n#There are a significant number of NA values for estimate and the confidence intervals. \n#Since these are for specific groups, the NA's mean that data was not collected for these groups. \n#Since we can't use \"other\" groups to estimate this data, we will drop these rows. \n#This results is a loss of 13% of the original dataset. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!is.na(estimate))\n\nhead(long_covid_uk_job)\nwrite.csv(long_covid_uk_job, \"../../data/01-modified-data/long_covid_uk_job_clean.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLong Covid UK Survey Data based on Health History - Cleaned\n\n\nUK Survey Health Cleaned Data\n\n\n\nLong Covid UK Survey Data based on Employement Status - Cleaned\n\n\nUK Survey Employement Status Cleaned Data"
  },
  {
    "objectID": "data-cleaning.html#long-covid-news",
    "href": "data-cleaning.html#long-covid-news",
    "title": "Data Cleaning",
    "section": "Long Covid News:",
    "text": "Long Covid News:\nI used the NewsAPI to collect current news information surrounding long covid. Since the data came from an API, there isn’t too much cleaning needed to be done. For now, we’ll visualize word frequency amongst titles of news articles.\n\n\nCode\nnews = pd.read_csv('../../data/00-raw-data/long_covid_news_raw.csv')\ntext = news['title'].tolist()\ntext.append(news['content'].tolist())\ntext = ' '.join([str(elem) for elem in text])\n\n# #FILTER OUT UNWANTED CHARACTERS\nnew_text=\"\"\nfor character in text:\n    if character in string.printable:\n        new_text+=character\ntext=new_text\n\n# #FILTER OUT UNWANTED WORDS\nnew_text=\"\"\nfor word in nltk.tokenize.word_tokenize(text):\n    if word not in nltk.corpus.stopwords.words('english'):\n        if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n            #remove the last space\n            new_text=new_text[0:-1]+word+\" \"\n        else: #add a space\n            new_text+=word.lower()+\" \"\ntext=new_text\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ngenerate_word_cloud(text)\n\n\n\n\n\nAfter cleaning the data containing the title and content for each news report, I created a wordcloud displaying the cleaned text frequencies. As we can see, words relating to boosters, symptoms, and overall health are the most frequent after long covid. Further analysis will be needed in order to showcase sentiment within the news surrounding long covid as well as topics for symptoms, if they are reported.\nNews Cleaned Data"
  },
  {
    "objectID": "eda-code.html",
    "href": "eda-code.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "cleaning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\ncdc &lt;- read.csv(\"../../data/00-raw-data/cdc_raw.csv\")\nstr(cdc)\n\n'data.frame':   11376 obs. of  16 variables:\n $ indicator             : chr  \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" \"Ever experienced long COVID, as a percentage of all adults\" ...\n $ group                 : chr  \"National Estimate\" \"By Age\" \"By Age\" \"By Age\" ...\n $ state                 : chr  \"United States\" \"United States\" \"United States\" \"United States\" ...\n $ subgroup              : chr  \"United States\" \"18 - 29 years\" \"30 - 39 years\" \"40 - 49 years\" ...\n $ phase                 : num  3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 3.5 ...\n $ time_period           : int  46 46 46 46 46 46 46 46 46 46 ...\n $ time_period_label     : chr  \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" \"Jun 1 - Jun 13, 2022\" ...\n $ time_period_start_date: chr  \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" \"2022-06-01\" ...\n $ time_period_end_date  : chr  \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" \"2022-06-13\" ...\n $ value                 : num  14 17.8 15.2 16.9 15.3 10.9 7.1 4.2 10.5 17.3 ...\n $ lowci                 : num  13.5 15.9 14.1 15.7 14.1 9.8 5.9 3.4 9.8 16.5 ...\n $ highci                : num  14.5 19.8 16.2 18.3 16.7 12 8.5 5.3 11.2 18.1 ...\n $ confidence_interval   : chr  \"13.5 - 14.5\" \"15.9 - 19.8\" \"14.1 - 16.2\" \"15.7 - 18.3\" ...\n $ quartile_range        : chr  \"\" \"\" \"\" \"\" ...\n $ quartile_number       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ suppression_flag      : int  NA NA NA NA NA NA NA NA NA NA ...\n#Let's drop the last three columns related to quartiles since it is unnessesary. \ncdc &lt;- cdc %&gt;%\n  select(-quartile_range, -quartile_number, -suppression_flag)\n#Now, let's check the unique values of indicator to condense the values since they're a little too long\ncdc_keys &lt;- cdc %&gt;%\n  select(indicator) %&gt;%\n  distinct()\n\ncdc_keys &lt;- cdc_keys %&gt;%\n  mutate(key = 1:nrow(cdc_keys))\n\n#Since there are 10 unique categories, we'll create a new column with a key, such that the number matches up with a value. \ncdc &lt;- cdc %&gt;%\n  right_join(cdc_keys, by = \"indicator\")\n#We can also eliminate time_period_label and confidence_interval as both values are repeated in other columns \ncdc &lt;- cdc %&gt;%\n  select(-time_period_label, -confidence_interval)\n#Now, let's make the time start and time end datetime variables\ncdc &lt;- cdc %&gt;%\n  mutate(time_period_start_date = as.Date(time_period_start_date)) %&gt;%\n  mutate(time_period_end_date = as.Date(time_period_end_date))\n#Let's look at the unique values for the group column and adjust as needed:\n\ncdc &lt;- cdc %&gt;%\n  mutate(group = str_replace(group, \"^By\\\\s\", \"\"))\n#We also need to check for any na values \ncdc %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n  indicator group state subgroup phase time_period time_period_start_date\n1         0     0     0        0     0           0                      0\n  time_period_end_date value lowci highci key\n1                    0  1399  1399   1399   0\n\n#From this we can say that the columns that have na's are all value and the corresponding lowci and highci. Thus, since the main variable we are tracking is value, we will go ahead and drop all rows where value = NA. Viewing the data, there also appears to be a connection between phace = -1 and the NA values. \n\ncdc &lt;- cdc %&gt;%\n  filter(!is.na(value))\ncdc %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n  indicator group state subgroup phase time_period time_period_start_date\n1         0     0     0        0     0           0                      0\n  time_period_end_date value lowci highci key\n1                    0     0     0      0   0\n\n#The data no longer has NA's! We can also see that this decision filtered out 12.5% of the original dataset.\n#Lastly, we'll get rid of the indicator column and show the cleaned data:\ncdc &lt;- cdc %&gt;%\n  select(-indicator)\n\nhead(cdc)\n\n              group         state      subgroup phase time_period\n1 National Estimate United States United States   3.5          46\n2               Age United States 18 - 29 years   3.5          46\n3               Age United States 30 - 39 years   3.5          46\n4               Age United States 40 - 49 years   3.5          46\n5               Age United States 50 - 59 years   3.5          46\n6               Age United States 60 - 69 years   3.5          46\n  time_period_start_date time_period_end_date value lowci highci key\n1             2022-06-01           2022-06-13  14.0  13.5   14.5   1\n2             2022-06-01           2022-06-13  17.8  15.9   19.8   1\n3             2022-06-01           2022-06-13  15.2  14.1   16.2   1\n4             2022-06-01           2022-06-13  16.9  15.7   18.3   1\n5             2022-06-01           2022-06-13  15.3  14.1   16.7   1\n6             2022-06-01           2022-06-13  10.9   9.8   12.0   1\n\ncdc_keys\n\n                                                                                                  indicator\n1                                                Ever experienced long COVID, as a percentage of all adults\n2                                 Ever experienced long COVID, as a percentage of adults who ever had COVID\n3                                          Currently experiencing long COVID, as a percentage of all adults\n4                           Currently experiencing long COVID, as a percentage of adults who ever had COVID\n5                                                                                            Ever had COVID\n6         Any activity limitations from long COVID, as a percentage of adults who currently have long COVID\n7                                   Any activity limitations from long COVID, as a percentage of all adults\n8 Significant activity limitations from long COVID, as a percentage of adults who currently have long COVID\n9                           Significant activity limitations from long COVID, as a percentage of all adults\n  key\n1   1\n2   2\n3   3\n4   4\n5   5\n6   6\n7   7\n8   8\n9   9\nwrite.csv(cdc, \"../../data/01-modified-data/cdc_clean.csv\")"
  },
  {
    "objectID": "cleaning.html#symptoms",
    "href": "cleaning.html#symptoms",
    "title": "cleaning",
    "section": "Symptoms:",
    "text": "Symptoms:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlong_covid_uk_health &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 3')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n\nlong_covid_uk_job &lt;- read_excel(\"../../data/00-raw-data/longcovid_uk.xlsx\", sheet = 'Table 4')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n\n\n\n#For Long Covid with regards to Health \n\n#First, we'll remove the first three rows since they're all empty. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n\n#Next, we'll make top row the column names:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  clean_names()\n\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_health &lt;- long_covid_uk_health %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `estimate = as.double(estimate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `lower_95_percent_confidence_limit =\n  as.double(lower_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `upper_95_percent_confidence_limit =\n  as.double(upper_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n\n#Checking for NA values:\nlong_covid_uk_health %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n# A tibble: 1 × 6\n  symptom domain group estimate lower_95_percent_confid…¹ upper_95_percent_con…²\n    &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;                     &lt;int&gt;                  &lt;int&gt;\n1       0      0     0        8                         8                      8\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\n#No NA values! \n\n#Let's see the cleaned data: \nhead(long_covid_uk_health)\n\n# A tibble: 6 × 6\n  symptom    domain group estimate lower_95_percent_con…¹ upper_95_percent_con…²\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Abdominal… All p… All …     9.56                   8.82                  10.3 \n2 Abdominal… Healt… No h…     6.96                   6.04                   7.87\n3 Abdominal… Healt… Acti…     6.06                   4.26                   7.85\n4 Abdominal… Healt… Acti…    11.3                    9.7                   12.9 \n5 Abdominal… Healt… Acti…    17.2                   14.8                   19.6 \n6 Abdominal… Exten… Not …     2.29                   1.51                   3.07\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\nwrite.csv(long_covid_uk_health, \"../../data/01-modified-data/long_covid_uk_health_clean.csv\")\n\n\n#Now, we'll do the same cleaning for employement status data since it's the same format:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!row_number() %in% c(1, 2, 3, 4))\n\n#Next, we'll make top row the column names:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  purrr::set_names(as.character(slice(., 1))) %&gt;%\n  slice(-1)\n\n#Now, we need to name the column names tidy:\nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  clean_names()\n\n#Next, we'll need to change the datatypes of certain columns. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  mutate(estimate = as.double(estimate)) %&gt;%\n  mutate(lower_95_percent_confidence_limit = as.double(lower_95_percent_confidence_limit)) %&gt;%\n  mutate(upper_95_percent_confidence_limit = as.double(upper_95_percent_confidence_limit))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `estimate = as.double(estimate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `lower_95_percent_confidence_limit =\n  as.double(lower_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `upper_95_percent_confidence_limit =\n  as.double(upper_95_percent_confidence_limit)`.\nCaused by warning:\n! NAs introduced by coercion\n\n#Checking for NA values:\nlong_covid_uk_job %&gt;%\n  summarise_all(~ sum(is.na(.)))\n\n# A tibble: 1 × 6\n  symptom domain group estimate lower_95_percent_confid…¹ upper_95_percent_con…²\n    &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;                     &lt;int&gt;                  &lt;int&gt;\n1       0      0     0      111                       111                    111\n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\n#There are a significant number of NA values for estimate and the confidence intervals. \n#Since these are for specific groups, the NA's mean that data was not collected for these groups. \n#Since we can't use \"other\" groups to estimate this data, we will drop these rows. \n#This results is a loss of 13% of the original dataset. \nlong_covid_uk_job &lt;- long_covid_uk_job %&gt;%\n  filter(!is.na(estimate))\n\nhead(long_covid_uk_job)\n\n# A tibble: 6 × 6\n  symptom    domain group estimate lower_95_percent_con…¹ upper_95_percent_con…²\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Abdominal… All p… All …     9.56                   8.82                  10.3 \n2 Abdominal… Emplo… Empl…     8.9                    7.92                   9.89\n3 Abdominal… Emplo… Inac…    18.6                   15.8                   21.5 \n4 Abdominal… Emplo… Inac…     7.5                    6.34                   8.66\n5 Abdominal… Self-… Empl…     9.04                   7.97                  10.1 \n6 Abdominal… Self-… Self…     8.05                   5.56                  10.6 \n# ℹ abbreviated names: ¹​lower_95_percent_confidence_limit,\n#   ²​upper_95_percent_confidence_limit\n\nwrite.csv(long_covid_uk_job, \"../../data/01-modified-data/long_covid_uk_job_clean.csv\")"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Data Science and Analytics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "naive-bayes.html#preparing-the-data",
    "href": "naive-bayes.html#preparing-the-data",
    "title": "Naive Bayes",
    "section": "Preparing the Data",
    "text": "Preparing the Data\n\nLabeled Text Data"
  }
]