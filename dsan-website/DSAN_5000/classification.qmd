---
title: "Decision Trees: Classification"
---


```{python}
#| warning: false
#| output: false
#| echo: false


import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt
from sklearn import tree
from IPython.display import Image
from collections import Counter
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import recall_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
```


## Introduction

Decision trees represent non-parametric supervised learning models capable of handling both classification and regression tasks. In the context of classification, a decision tree works by predicting the value of a target variable through the acquisition of simple decision rules derived from the features present in the dataset. For the specific case of Long Covid analysis, we'll first apply a decision tree to the Harvard Long Covid Survey data.

In contrast to the clustering model where the focus was on the hospitalization variable, the classification model shifts attention to a different aspect of the data. The earlier findings from clustering suggested that the symptoms themselves might not directly correlate with hospitalization but could be indicative of another relevant label. In light of this, the decision tree is employed to predict the patient's sex based on a combination of symptoms and demographic information.

The objective is to unravel patterns within the dataset that link particular symptoms and demographic characteristics to the patient's sex. By constructing a decision tree, the model leverages a tree-like structure where each node represents a decision based on a feature, leading to subsequent nodes or leaves with predicted outcomes. This approach allows for the interpretation of which symptoms and demographics are most influential in determining a patient's sex. 


## Methods

::: {.panel-tabset}

## Correlarion:
```{python}
#| warning: false
#| code-fold: true
#| code-summary: "Date Cleaning"

harvard_covid = pd.read_csv("../../data/01-modified-data/harvard_long_covid_cleaned.csv")
X = harvard_covid.loc[:, 'vaccination':'headache_covid'].astype('category')
y = harvard_covid['sex'].map({'male': 0, 'female': 1})
print(X.columns)

X = X.to_numpy()
y = y.to_numpy()

df=pd.DataFrame(np.hstack((X,y.reshape(y.shape[0],1))))
corr = df.corr()

#Display a correlation map: 
sns.set_theme(style="white")
f, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure
cmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap
# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,
        square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
```

## Hypertuning:
```{python}
#| warning: false
#| code-fold: true
#| code-summary: "Hyptertuning the Model"


test_ratio=0.2
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0)
y_train=y_train.flatten()
y_test=y_test.flatten()

test_results=[]
train_results=[]

for num_layer in range(1,20):
    model = tree.DecisionTreeClassifier(max_depth=num_layer)
    model = model.fit(x_train,y_train)

    yp_train=model.predict(x_train)
    yp_test=model.predict(x_test)

    # print(y_pred.shape)
    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])
    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train, pos_label=1)])

test_df = pd.DataFrame(test_results, columns=['num_layer', 'accuracy', 'recall_0', 'recall_1'])
train_df = pd.DataFrame(train_results, columns=['num_layer', 'accuracy', 'recall_0', 'recall_1'])

plt.figure(figsize=(10, 6))
p1 = sns.lineplot(x='num_layer', y='accuracy', data=train_df, color='blue', marker='o')
sns.lineplot(x='num_layer', y='accuracy', data=test_df, color='red',  marker='o', ax=p1)
p1.set_xlabel('Number of layers in decision tree (max_depth)')
p1.set_ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')
plt.show()

plt.figure(figsize=(10, 6))
p2 = sns.lineplot(x='num_layer', y='recall_0', data=train_df, color='blue', marker='o')
sns.lineplot(x='num_layer', y='recall_0', data=test_df, color='red', marker='o', ax=p2)
p2.set_xlabel('Number of layers in decision tree (max_depth)')
p2.set_ylabel('RECALL (Y=0): Training (blue) and Test (red)')
plt.show()

plt.figure(figsize=(10, 6))
p3 = sns.lineplot(x='num_layer', y='recall_1', data=train_df, color='blue', marker='o')
sns.lineplot(x='num_layer', y='recall_1', data=test_df, color='red', marker='o', ax=p3)
p3.set_xlabel('Number of layers in decision tree (max_depth)')
p3.set_ylabel('RECALL (Y=1): Training (blue) and Test (red)')
plt.show()
```

## Visualizations:
```{python}
#| warning: false
#| code-fold: true
#| code-summary: "Confusion Plot"

model = tree.DecisionTreeClassifier(max_depth= 5)
model = model.fit(X,y)

def confusion_plot(y_data, y_pred):

    cm = confusion_matrix(y_data, y_pred, labels= model.classes_)

    TP = cm[0,0]
    FN = cm[0,1]
    FP = cm[1,0]
    TN = cm[1,1]
    
    print('ACCURACY:',accuracy_score(y_data, y_pred))
    print('NEGATIVE RECALL (Y=0):', TN/(TN+FP))
    print('NEGATIVE PRECISION (Y=0):', TN/(TN+FN))
    print('POSITIVE RECALL (Y=1):', TP/(TP+FN))
    print('POSITIVE PRECISION (Y=1):', TP/(TP+FP))
    print(cm)

    #Plotting confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot()
    plt.show()

print("------TRAINING------")
confusion_plot(y_train,yp_train)
print("------TEST------")
confusion_plot(y_test,yp_test)
```

## Decision Tree

```{python}
#| warning: false
#| code-fold: true
#| code-summary: "Decision Tree Plot"

def plot_tree(model, X, Y):
    model = model.fit(x_train, y_train)
    
    fig = plt.figure(figsize=(11,8))
    _ = tree.plot_tree(model, 
                   feature_names=X.columns,  
                   class_names=Y,
                   filled=True)

    plt.show()

X_df = harvard_covid.loc[:, 'vaccination':'headache_covid']
Y_df = list(harvard_covid['sex'].unique())

plot_tree(model, X_df, Y_df)

```

:::

In the heat map of all the variables we will be training on, we can notice a few interesting this. Some things to note is that there are a few variables that are correlated (most due to vaccination or similar symptoms in the body), but not enough such that the analysis of the decision tree would be impacted. 

From the accuracy, recall 1, and recall 0, we can see that the optimal depth is 5. From here we can take a look at the confusion matrices of the train and test data as well as create our decision tree. 

## Conclusion 

While our testing confusion matrix has a larger number of false positives, we can still make a few conclusions for the decision tree. Firstly, we can say the hospitalization due to long covid, from these survey results, seems to affect women as opposed to men. Additionally, those who experienced tingling seemed to be men over females. Additionally, as previously noted in EDA, those who experienced hair loss were entirely female. Hair loss is common in women due to stress and hormonal changes. This could mean that long covid may be affecting women hormonally or causing abnormal levels of stress, however more data and research would need to be collected to investigate that emerging thought. 

Thus, decision trees can provide an easy way to classify a number of binary classes. If data was to be publicly released regarding individuals who experienced either Long Covid or Covid and their symptoms, we would be able to create a classifier that determines what symptoms are more highly for those with Long Covid.



