---
title: "ARMA, ARIMA, and SARIMA Models"
---

ARMA and ARIMA Models are used in time-series in order to forecast the time series object at had. Thus, we will take a look at both the globalization index, the stock fluctuation of HYBE Co., Korean tourism, and popularity between KPOP and Western artists in order to forecast values and make future predictions. 

## Globalization Index: 
From our [Exploratory Data Analysis](https://shriya-chinthak.georgetown.domains/DSAN_5600/eda.html), we noticed the following information: 

* Prior to differencing, the ACF plot show several lags above the significance bands, indicating a **non-stationary** relationship. 
* The Augmented Dickey-Fuller Test confirmed that the data itself was **NOT stationary**.
* **First Differencing** will be used since the data becomes stationary. 

Thus, using this information, we will move on to creating the model: 
```{r}
#| echo: false
#| warning: false
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(imputeTS)
library(gridExtra)
library(reticulate)
library(readxl)
use_python("/usr/local/bin/python3", require = T)
knitr::knit_engines$set(python = reticulate::eng_python)
```


::: {.panel-tabset}

## ACF and PACF

* Since the ACF plot doesn't have any significant peaks, **q = 0**. 

* Since the PACF plot doesn't have any significant peaks, **p = 0**. 

* Since we differenced once, **d = 1**.

```{r}
#| echo: false
#| warning: false
global <- read_csv('globalization.csv')

# Filter information
global <- global %>%
  filter(country == 'United States') %>%
  select(year, KOFGI) %>%
  mutate(year = as.Date(year))

# Create time series
global_ts <-ts(global$KOFGI, star=decimal_date(as.Date("1970-01-01", format = "%Y-%m-%d")), frequency = 1)

plot3 <- ggAcf(diff(global_ts), 50, main="First Differenced Data")
plot4 <- ggPacf(diff(global_ts), 50, main="First Differenced Data")
grid.arrange(plot3, plot4, ncol=2)
```

## Manual ARIMA
Thus, we can easily view that the best model has values **p=0, d=1, and q=0**. 
```{r}
#| code-fold: true 
#| code-summary: 'ARIMA Model'
#| warning: false

set.seed(123)

d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)


for (p in 0:4)# p=0,1,2,3,4 : 5
{
  for(q in 0:4)# q=0,1,2,3,4 :5
  {
    model<- Arima(global_ts,order=c(p,d,q),include.drift=TRUE) 
    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)

```

```{r}
#| echo: false
print("AIC:")
temp[which.min(temp$AIC),]
print("BIC:")
temp[which.min(temp$BIC),]
print("AICc:")
temp[which.min(temp$AICc),]
```

## Equation 

From the results of the `sarima()` function, we call say that the equation is as follows:

\begin{align}
x_{t} = w_{t} -1w_{t-1} + 0.4805
\end{align}

From the model diagonotics presented, we can also say that the ACF plot of residuals shows no significance, meaning the residuals are not correlated. Additionally, the p-values of the Ljung-Box statistic is much higher than the significance band, meaning that we fail to reject the null hypothesis and can say that the model is not autocorrelated. 

```{r}
#| code-fold: true
sarima(global_ts, 0, 1, 0)
```

## auto.arima()

`auto.arima()` concluded that ARIMA(0,2,1) is the best model. However, due to it's greater AIC and BIC values, we have decided to stick if ARIMA(0,1,0). Thus, we will try forecasting both that model and ARIMA(0,1,0)

```{r}
#| code-fold: true
#| warning: false
auto.arima(global_ts)

sarima(global_ts, 0, 2, 1)
```

## Forecasting
```{r}
#| code-fold: true 
#| code-summary: 'Forecasting'
#| warning: false
fit <- Arima(global_ts, order=c(0, 1, 0))
autoplot(forecast(fit))
```

## Benchmark Comparison
```{r}
#| code-fold: true 
#| code-summary: 'Forecast comparison'
#| warning: false


autoplot(global_ts) +
  autolayer(meanf(global_ts, h=11),
            series="Mean", PI=FALSE) +
  autolayer(naive(global_ts, h=11),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(global_ts, h=11),
            series="Seasonal naïve", PI=FALSE) +
  autolayer(forecast(fit, h=11),
            series="Fit", PI=FALSE) +
  ggtitle("Forecasts for yearly globalization metric") +
  xlab("Year") + ylab("KOF Index") +
  guides(colour=guide_legend(title="Forecast"))
```

:::

Thus, from the ARIMA procedure, we found that our manual ARIMA approach was more effective in finding the model with the best AIC and BIC values than auto.arima(). From the forecast, we can clearly see that while globalization has been on an upward trend, it is predicted to level out in the next year. While this prediction is not promising, it does perform better than out benchmark fits.  

---

# Korean Tourism

In order to understand globalization within the USA through the lens on South Korea, we will need to also look at the tourism coming into SK from abroad. An example of this is using monthly air travel passeneger data into Incheon Airport, South Korea's largest international airport. Specifically, we'll be focusing on the number of passengers from international flights coming into South Korea for tourism as they best represent foreign interest of the nation. 

From our [Exploratory Data Analysis](https://shriya-chinthak.georgetown.domains/DSAN_5600/eda.html), we noticed the following information: 

* Prior to differencing, the ACF plot show several lags above the significance bands, indicating a **non-stationary** relationship. 
* **Season + First Differencing** resulted in the data being stationary. 

::: {.panel-tabset}

## ACF and PACF

From these plots, we can say that q = 2, p = 2, 4, d = 1, Q = 1, and P = 1. 

```{r}
#| warning: false
#| code-fold: true


sk_passengers <- read_xlsx('raw_data/sk_passenger_arrivals.xlsx')

sk_passengers <- sk_passengers %>%
  unite(date, year, month, sep = '-') %>%
  mutate(date = as.Date(paste(date, '01', sep = '-'))) %>%
  filter(year(date) < 2020) #In order to avoid the anomaly of the 2020 pandemic

air_travel_ts <- ts(sk_passengers$Passengers, start = c(2010, 10), 
                    frequency = 12)

ggAcf(air_travel_ts %>% diff() %>% diff(12), lag.max = 30, main="Seasonal Differenced + First Differenced ACF")
ggPacf(air_travel_ts %>% diff() %>% diff(12), lag.max = 30, main="Seasonal Differenced + First Differenced PACF")

```

## Manual SARIMA
Moving on with model **ARIMA(0,1,2)x(0,1,1)[12]** due to the lowest AIC, AICc, and BIC values. 

```{r}
#| code-fold: true

#write a function
SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  
  temp=c()
  d=1
  D=1
  s=12
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*35),nrow=35)
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  temp
}

output=SARIMA.c(p1=1,p2=5,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=air_travel_ts)
knitr::kable(output)

output[which.min(output$AIC),]
output[which.min(output$BIC),]
output[which.min(output$AICc),]
```

## auto.arima()
```{r}
#| code-fold: true
auto.arima(air_travel_ts)
```

## Equation
This model seems to be a goof fit for a number of reasons. Primarily, the ACF plot shows almost no correlation, indicating that the model has harnessed everything that left is white noise. This indicates a good model fit. Additionally, the Ljung-Box statistic shows almost no autocorrelation within the model. Lastly, all coefficients within the table are significant. 

\begin{align}
(1 - \beta)(1 - \beta^{12}) (Y_t - \mu) = (1 + 0.2648\beta + 0.4423\beta^2)(1 - 0.6406\beta^{12}) \epsilon_t
\end{align}


```{r}
#| code-fold: true

set.seed(123)
model_output <- capture.output(sarima(air_travel_ts, 0,1,2,0,1,1,12))
cat(model_output[28:60], model_output[length(model_output)], sep = "\n") 
```

## Fitting
```{r}
#| code-fold: true
fit <- Arima(air_travel_ts, order=c(0,1,2), seasonal=c(0,1,1))
summary(fit)
```

## Forecasting
```{r}
#| code-fold: true
sarima.for(air_travel_ts, 60, 0,1,2,0,1,1,12)
```

:::

Similar to out globalization index, we can see that the manual approach to find the best model was better than auto.arima() since the resulting model had a better AIC, BIC, AICc values. Our forecast shows a positive upwards trend in international passenger arrivals into Incheon Airport, South Korea. Please note, this forecast shows five years into the future if the COVID-19 pandemic didn't cause any anomalies within air travel. However, even knowing that the actual data isn't the same, we can approximate that the travel industry would recover in a similar pattern, with a positive trend of people coming into Korea. Thus, this furthers the narrative of globalization, and specifically, Korean culture reaching outside its country's borders into the West. 

---

# Forecasting HYBE

As we've seen thus far, BTS, a group under HYBE, seems to have far better success than other KPOP groups in the West. However, with their recent announcement of mandatory military enlightment, we want to see how HYBE stock will forecast in the coming months.

From our [Exploratory Data Analysis](https://shriya-chinthak.georgetown.domains/DSAN_5600/eda.html), we noticed the following information: 

* Prior to differencing, the ACF plot show several lags above the significance bands, indicating a **non-stationary** relationship. 
* **Second Differencing** resulted in the data being stationary.

::: {.panel-tabset}

## ACF & PACF
```{r}
#| warning: false
#| code-fold: true
options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)


name <- getSymbols('352820.KS', from = "2019-01-01", to = "2023-09-01")
HYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)
HYBE <- HYBE %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(Price = X352820.KS.Adjusted) %>%
  mutate(Price = Price/1352.60)

start_date <- as.Date("2020-10-15")
end_date <- as.Date("2023-09-01")

all_dates <- data.frame(Date = seq(start_date, end_date, by = "days"))

merged_data <- all_dates %>%
  left_join(HYBE, by = "Date")

imputed_time_series <- na_ma(merged_data, k = 4, weighting = "exponential")
df_HYBE <-data.frame(imputed_time_series)
df_HYBE$Date <-as.Date(df_HYBE$Date,format = "%Y-%m-%d")

write.csv(df_HYBE, 'cleaned_data/HYBE_cleaned_data.csv')

# Create time series
HYBE_ts <-ts(df_HYBE$Price, star=decimal_date(as.Date("2020-10-15", format = "%Y-%m-%d")), frequency = 365.25)

ggAcf(diff(diff(HYBE_ts)), lag.max = 30, main="Second Differenced Data")
ggPacf(diff(diff(HYBE_ts)), lag.max = 30, main="Second Differenced Data")
```

## Manual ARIMA
Thus, we can easily view that the best model has values **p=1,2,3,4, d=2, and q=1**. 
```{r}
#| code-fold: true 
#| code-summary: 'ARIMA Model'
#| warning: false

set.seed(123)

d=2
i=1
temp= data.frame()
ls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)


for (p in 0:4)# p=0,1,2,3,4 : 5
{
  for(q in 0:4)# q=0,1,2,3,4 :5
  {
    model<- Arima(HYBE_ts,order=c(p,d,q),include.drift=TRUE) 
    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)
```

```{r}
#| echo: false
print("AIC:")
temp[which.min(temp$AIC),]
print("BIC:")
temp[which.min(temp$BIC),]
print("AICc:")
temp[which.min(temp$AICc),]
```


## Equation 

From the results of the `sarima()` function, we call say that the equation is as follows:

\begin{align}
(1 - 0.9590\beta - 0.4301\beta^2)(1 - \beta)(1 - \beta)^2 (Y_t - \mu) = (1 - 2.1072\beta - 1.7438\beta^2 + 0.6367\beta^3) \epsilon_t
\end{align}

```{r}
#| code-fold: true
sarima(HYBE_ts, 2, 2, 3)
```


## auto.arima()

`auto.arima()` concluded that ARIMA(4,1,2) is the best model. Comparing this to ARIMA(2,2,3), we can see that the AIC, BIC, and AICc values are much better. Additionally, the Ljung-Box statistic shows almost no autocorrelation within the model. 

```{r}
#| code-fold: true
#| warning: false
auto.arima(HYBE_ts)

sarima(HYBE_ts, 4, 1, 2)
```

## Forecasting
```{r}
#| code-fold: true 
#| code-summary: 'Forecasting'
#| warning: false
fit <- Arima(HYBE_ts, order=c(4, 1, 2))
autoplot(forecast(fit, h = 365))
```

## Benchmark Comparison
```{r}
#| code-fold: true 
#| code-summary: 'Forecast comparison'
#| warning: false


autoplot(HYBE_ts) +
  autolayer(meanf(HYBE_ts, h=365),
            series="Mean", PI=FALSE) +
  autolayer(naive(HYBE_ts, h=365),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(HYBE_ts, h=365),
            series="Seasonal naïve", PI=FALSE) +
  autolayer(forecast(fit, h=365),
            series="Fit", PI=FALSE) +
  ggtitle("Forecasts for HYBE Stock Price") +
  xlab("Year") + ylab("Price (USD)") +
  guides(colour=guide_legend(title="Forecast"))
```

:::

From the manual approach, we can also say that the ACF plot of residuals shows no significance, meaning the residuals are not correlated. Additionally, the p-values of the Ljung-Box statistic is on the lower side, however, we will continue and see whether auto.arima() creates a better model. Thus, after forecasting ARIMA(4,1,2), we can see that the forecast predicts a consistent stock price for the next year. Obviously, this doesn't seem feasible, as stock prices are constantly changing. However, what this does tell us is that there isn't an immediate positive trend in HYBE stock to say that the prediction would be upward. Therefore, we'll move forward and see how other KPOP labels and Western record labels may effect volitilaty and unpredictability of HYBE's future. 

--- 

# Popularity: KPOP

In order to get a better understanding of the popularity of KPOP as the years progress, we will create a ARMA model to forecast.

From our [Exploratory Data Analysis](https://shriya-chinthak.georgetown.domains/DSAN_5600/eda.html), we noticed the following information: 

* Our original data, when plotted in an ACF and PACF plot, returned lags well within the significance bounds, meaning that the data was **stationary without differencing**. 

::: {.panel-tabset}

## ACF & PACF 
```{r}
#| code-fold: true

kpop_data <- read.csv("cleaned_data/kpop_popularity.csv")
kpop_ts <- ts(kpop_data$popularity, frequency = 1)

ggAcf(kpop_ts)+ggtitle("ACF Plot for Popularity Score of KPOP Artists")
ggPacf(kpop_ts)+ggtitle("PACF Plot for Popularity Score of KPOP Artists")
```

## Manual ARMA
```{r}
#| code-fold: true
#| warning: true

set.seed(123)

d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)


for (p in 0:4)# p=0,1,2,3,4 : 5
{
  for(q in 0:4)# q=0,1,2,3,4 :5
  {
    model<- Arima(kpop_ts,order=c(p,d,q),include.drift=TRUE) 
    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)

```

```{r}
#| echo: false
print("AIC:")
temp[which.min(temp$AIC),]
print("BIC:")
temp[which.min(temp$BIC),]
print("AICc:")
temp[which.min(temp$AICc),]
```


## Equation
The resulting model is a constant function as a result of model selection. Unfortunelty, due to the lack of data points, we are unable to output the model diagnostics of the ARMA(0,0) model. Using the auto.arima() function, the equation for the model is written as follows. 

\begin{align}
x_{t} = w_{t} + 77.3352
\end{align}

## auto.arima()
The auto.arima() function confirms our ACF and PACF plots with the model ARIMA(0,0,0)

```{r}
#| code-fold: true
auto.arima(kpop_ts)
```

## Forecast

```{r}
#| code-fold: true
fit <- Arima(kpop_ts, order=c(0, 0, 0))
autoplot(forecast(fit))
```

## Benchmark Comparison

```{r}
#| code-fold: true
autoplot(kpop_ts) +
  autolayer(meanf(kpop_ts, h=11),
            series="Mean", PI=FALSE) +
  autolayer(naive(kpop_ts, h=11),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(kpop_ts, h=11),
            series="Seasonal naïve", PI=FALSE) +
  autolayer(forecast(fit, h=11),
            series="Fit", PI=FALSE) +
  ggtitle("Forecasts for Yearly Average Popularity Score of KPOP Artists") +
  xlab("Year") + ylab("Popularity Score") +
  guides(colour=guide_legend(title="Forecast"))
```

:::

Since we cannot see any siginificant lags in the ACF and PACF, we'll need to use a ARMA(0,0,0) model. However, running a manual and auto.arima() can help to decide the best approach. Through the manual approach, we found ARIMA(0,1,0) the best purely based on AIC and BIC. However, because this data cannot be differenced, ARIMA(0,0,0) is the best option, with auto.arima() solidfying this notion. As a result, the forecast is simply the mean value of the popularity, which is not a good indicator of the future forecast. The lack of data points unfortunelty prevented an in depth prediction, however, we can note that as as KPOP continues to be streamed, we will continue to see rising popularity. 

---

# Popularity: Western

In order to get a better understanding of the popularity of Western artists as the years progress, we will create an ARMA model to forecast. As a reminder, due to the new Spotify API regulations, popularity scores are not public for all songs. Therefore, to use the metric we've extrapolated and averaged the scores. For more details, please [Data Visualizations](https://shriya-chinthak.georgetown.domains/DSAN_5600/data-visualization.html). 

From our [Exploratory Data Analysis](https://shriya-chinthak.georgetown.domains/DSAN_5600/eda.html), we noticed the following information: 

* Our original data, when plotted in an ACF and PACF plot, returned lags well within the significance bounds, meaning that the data was **stationary without differencing**. 

::: {.panel-tabset}

## ACF & PACF

```{r}
#| code-fold: true

western_data <- read.csv("cleaned_data/western_popularity.csv")
western_ts <- ts(western_data$popularity, frequency = 1)

ggAcf(western_ts)+ggtitle("ACF Plot for Popularity Score of Western Artists")
ggPacf(western_ts)+ggtitle("PACF Plot for Popularity Score of Western Artists")

```

## Manual ARIMA

```{r}
#| code-fold: true
#| warning: true

set.seed(123)

d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)


for (p in 0:4)# p=0,1,2,3,4 : 5
{
  for(q in 0:4)# q=0,1,2,3,4 :5
  {
    model<- Arima(western_ts,order=c(p,d,q),include.drift=TRUE) 
    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(temp)

```

```{r}
#| echo: false
print("AIC:")
temp[which.min(temp$AIC),]
print("BIC:")
temp[which.min(temp$BIC),]
print("AICc:")
temp[which.min(temp$AICc),]
```

## Model Diagnotics

The manual approach suggested the model ARIMA(0,1,2) based on the lowest AIC and BIC values. After running the model diagnostics, we have the following. 

```{r}
#| code-fold: true
sarima(western_ts, 0, 1, 2)
```

## auto.arima()
Cross referencing with auto.arima() resulted in the model ARIMA(0,1,1). Through a model diagnostics analysis, we can see that while the AIC and BIC values are slightly higher, the Ljung-Box statistic p-values are much higher than the significance band, meaning that we fail to reject the null hypothesis and can say that the model is not autocorrelated. Therefore, we'll move forward with this model. 

\begin{align}
(1 - B)(Y_t - \mu) = -0.7565 \epsilon_t
\end{align}

```{r}
#| code-fold: true
auto.arima(western_ts) #produced 0,1,1
sarima(western_ts, 0, 1, 1)
```

## Forecast

```{r}
#| code-fold: true
fit <- Arima(western_ts, order=c(0, 1, 1))
autoplot(forecast(fit))
```

## Benchmark Comparison

```{r}
#| code-fold: true


autoplot(western_ts) +
  autolayer(meanf(western_ts, h=11),
            series="Mean", PI=FALSE) +
  autolayer(naive(western_ts, h=11),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(western_ts, h=11),
            series="Seasonal naïve", PI=FALSE) +
  autolayer(forecast(fit, h=11),
            series="Fit", PI=FALSE) +
  ggtitle("Forecasts for Yearly Average Popularity Score of Western Artists") +
  xlab("Year") + ylab("Popularity Score") +
  guides(colour=guide_legend(title="Forecast"))
```

:::

For Western artists we saw that auto.arima() found a better model than our manual approach. The forecast shows a constant metric for the popularity score. Unfortunelty, this model, like the popularity score for KPOP, was not able to meet benchmark standards, simply due to the lack of data points. 

For future analysis of popularity, we must investigate another outlet as opposed to the Spotify metric due to the new API regulations. A different metric with a string database would allow us to properly predict popularity based on genre and artist. However, what we can say is that popularity for both Western and KPOP artists fluctuate. To further estimate popularity of an artists, we must try to look at their musical characteristics and how this can predict popularity in time. 

