{"title":"Deep Learning for Time Series","markdown":{"yaml":{"title":"Deep Learning for Time Series"},"headingText":"Obtaining the Models","containsRefs":false,"markdown":"\n```{r}\n#| echo: false\n#| output: false\n#| warning: false\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\npy_install(\"tensorflow\")\n```\n\n```{python}\n#| warning: false\n#| echo: false\n#| output: false\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n#Keras packages: \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM ,GRU\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n```\n\nIn order to compare the tradition time series models, we'll next take a look at deep learning models for comparison. The data we'll take a look at is the Air passangers into South Korea which we modeled through ARIMA. In order to compare the predictions of that model with deep learning, we will create 3 different neural network models (RNNs, GRU, LSTM) using the tensorflow package found in python. After we gather the predictions for these models, we'll compare those predictions with ARIMA's forecast via RMSE as well as test the forecasting reach of the models. \n\n\nFirst, we'll begin by gathering the data and transforming it such that it is suitable for deep learning. Below is confirmation that the data was transformed and no NA values were found. \n\n```{python}\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Data Transformation\"\n\n\nsk_passengers = pd.read_excel('raw_data/sk_passenger_arrivals.xlsx')\n\n# Manipulate data\nsk_passengers['date'] = sk_passengers['year'].astype(str) + '-' + sk_passengers['month'].astype(str)\nsk_passengers['date'] = pd.to_datetime(sk_passengers['date'] + '-01')\nsk_passengers = sk_passengers[sk_passengers['date'].dt.year < 2020]\n\ndf = sk_passengers\n\ndf = df.rename(columns={\"date\": \"t\", \"Passengers\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n```\n\nNext, we'll go ahead and define mutliple functions used for each deep learning algorithm. The functions form_arrays, regression_report, and history_plots are used for the pre-prosessing and post anylsis of the model's results. \n\n```{python}\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Function Definitions\"\n\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample: \n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" RMSE:\",(mean_squared_error(yt,ytp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" RMSE:\",(mean_squared_error(yv,yvp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n```\n\nNow that we have our functions defined, let's visualize the raw, normalized, and train-validation split. \n\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Visualization of Raw Data\"\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Raw Data\")\nplt.show()\n\nprint(np.mean(x,axis=0).shape,np.std(x,axis=0).shape)\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\nprint(x.shape)\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Normalized Data\")\nplt.show()\n\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; xt=x[0:cut]\ntv=t[cut:]; xv=x[cut:]\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.title(\"Train/Validation Split\")\nplt.show()\n```\n\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Training/Validation Shape\"\n\n\n# training\nL=5; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n```\n\nFinally, we can begin running our machine learning models. The models and their results are as follows: RNN, GRU, and LSTM.\n\n### RNN\n\n::: {.panel-tabset}\n\n## Results \n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"RNN\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n## Error\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n---\n\n### GRU \n\n::: {.panel-tabset}\n\n## Results\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"GRU\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n\n```\n\n\n## Error\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n---\n\n### LSTM \n::: {.panel-tabset}\n\n## Results\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"LSTM\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n## Error \n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n## Comparing the Models\n\nFrom the models given, we can see the following RMSE values: \n\nRNN\n: Training: 0.11260836081499234, Validation: 0.48808609621144566\n\nGRU\n: Training: 0.1748385330640682; Validation: 0.27231605760159167\n\nLSTM\n: Training: 0.24832560941644433; Validation: 0.6533320413106263\n\nThus, we can see that based on the loss metric (RMSE), the GRU model seems to be be the best for predicting air passengers from international flights into South Korea. \n\n## The Problem with Forecasting:\nThe 3 deep learning models, while highly effective, cannot forecast with \"new\" x-values. Unfortunelty, due to that way we'll structured these models, those \"x-values\", rather than time, is the value being measured, in our case, air passengers. Thus, we shall focus on the forecasting ability of the model through validation results. Taking a look at GRU, when checking the RMSE value after multiple lengths of train/validation, we can see that as the validation size increases, the RMSE actually did decrease. However, the other models failed to reduce the loss function as the validation size increased. However, we can note that all three models benefited from regularizing around 0 since the predictions for regularized data above 0 had a great loss return. \n\n## Comparing to ARIMA/SARIMA \nPreviously, the air passenger dataset was used to model a SARIMA model, which produced a positivly trended forecast. That specific fit produced an RMSE of 86406.29, which is much much higher that any of the deep learning models. However, the visualization of the forecast from the SARIMA model fit much better than that of any of the deep learning models. \n\nThus, as of now, we can say that while the deep learning models effectively reduced the RMSE and other loss functions, these models lack the ability to forecast far in the future or accurately account for seasonality and volitility in the data. Therefore, we can say that more research and deep learning implementations would need to be performed in order to compare to the forecasting abilities of SARIMA and other time series models. ","srcMarkdownNoYaml":"\n```{r}\n#| echo: false\n#| output: false\n#| warning: false\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\npy_install(\"tensorflow\")\n```\n\n```{python}\n#| warning: false\n#| echo: false\n#| output: false\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n#Keras packages: \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM ,GRU\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n```\n\nIn order to compare the tradition time series models, we'll next take a look at deep learning models for comparison. The data we'll take a look at is the Air passangers into South Korea which we modeled through ARIMA. In order to compare the predictions of that model with deep learning, we will create 3 different neural network models (RNNs, GRU, LSTM) using the tensorflow package found in python. After we gather the predictions for these models, we'll compare those predictions with ARIMA's forecast via RMSE as well as test the forecasting reach of the models. \n\n## Obtaining the Models\n\nFirst, we'll begin by gathering the data and transforming it such that it is suitable for deep learning. Below is confirmation that the data was transformed and no NA values were found. \n\n```{python}\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Data Transformation\"\n\n\nsk_passengers = pd.read_excel('raw_data/sk_passenger_arrivals.xlsx')\n\n# Manipulate data\nsk_passengers['date'] = sk_passengers['year'].astype(str) + '-' + sk_passengers['month'].astype(str)\nsk_passengers['date'] = pd.to_datetime(sk_passengers['date'] + '-01')\nsk_passengers = sk_passengers[sk_passengers['date'].dt.year < 2020]\n\ndf = sk_passengers\n\ndf = df.rename(columns={\"date\": \"t\", \"Passengers\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n```\n\nNext, we'll go ahead and define mutliple functions used for each deep learning algorithm. The functions form_arrays, regression_report, and history_plots are used for the pre-prosessing and post anylsis of the model's results. \n\n```{python}\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Function Definitions\"\n\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample: \n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" RMSE:\",(mean_squared_error(yt,ytp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" RMSE:\",(mean_squared_error(yv,yvp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n```\n\nNow that we have our functions defined, let's visualize the raw, normalized, and train-validation split. \n\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Visualization of Raw Data\"\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Raw Data\")\nplt.show()\n\nprint(np.mean(x,axis=0).shape,np.std(x,axis=0).shape)\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\nprint(x.shape)\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Normalized Data\")\nplt.show()\n\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; xt=x[0:cut]\ntv=t[cut:]; xv=x[cut:]\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.title(\"Train/Validation Split\")\nplt.show()\n```\n\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"Training/Validation Shape\"\n\n\n# training\nL=5; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n```\n\nFinally, we can begin running our machine learning models. The models and their results are as follows: RNN, GRU, and LSTM.\n\n### RNN\n\n::: {.panel-tabset}\n\n## Results \n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"RNN\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n## Error\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n---\n\n### GRU \n\n::: {.panel-tabset}\n\n## Results\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"GRU\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n\n```\n\n\n## Error\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n---\n\n### LSTM \n::: {.panel-tabset}\n\n## Results\n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"LSTM\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n## Error \n```{python}\n#| message: false\n#| warning: false\n#| code-fold: true\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n```\n\n:::\n\n## Comparing the Models\n\nFrom the models given, we can see the following RMSE values: \n\nRNN\n: Training: 0.11260836081499234, Validation: 0.48808609621144566\n\nGRU\n: Training: 0.1748385330640682; Validation: 0.27231605760159167\n\nLSTM\n: Training: 0.24832560941644433; Validation: 0.6533320413106263\n\nThus, we can see that based on the loss metric (RMSE), the GRU model seems to be be the best for predicting air passengers from international flights into South Korea. \n\n## The Problem with Forecasting:\nThe 3 deep learning models, while highly effective, cannot forecast with \"new\" x-values. Unfortunelty, due to that way we'll structured these models, those \"x-values\", rather than time, is the value being measured, in our case, air passengers. Thus, we shall focus on the forecasting ability of the model through validation results. Taking a look at GRU, when checking the RMSE value after multiple lengths of train/validation, we can see that as the validation size increases, the RMSE actually did decrease. However, the other models failed to reduce the loss function as the validation size increased. However, we can note that all three models benefited from regularizing around 0 since the predictions for regularized data above 0 had a great loss return. \n\n## Comparing to ARIMA/SARIMA \nPreviously, the air passenger dataset was used to model a SARIMA model, which produced a positivly trended forecast. That specific fit produced an RMSE of 86406.29, which is much much higher that any of the deep learning models. However, the visualization of the forecast from the SARIMA model fit much better than that of any of the deep learning models. \n\nThus, as of now, we can say that while the deep learning models effectively reduced the RMSE and other loss functions, these models lack the ability to forecast far in the future or accurately account for seasonality and volitility in the data. Therefore, we can say that more research and deep learning implementations would need to be performed in order to compare to the forecasting abilities of SARIMA and other time series models. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"deep-learning.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"minty","title":"Deep Learning for Time Series"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}