{"title":"ARIMAX, SARIMAX, and VAR","markdown":{"yaml":{"title":"ARIMAX, SARIMAX, and VAR","bibliography":"intro_reference.bib"},"headingText":"Key Questions:","containsRefs":false,"markdown":"\n\nIn order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we'll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations @grammy. Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\n\nThe next relationship we'll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we'll see Universal Music Group and Warner's relationship on HYBE and whether it's significant.\n\nLastly, we'll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry.\n\n\n1. What is the relationship between KPOP groups?\n2. What is the relationship between HYBE and the Western industry?\n3. What is the relationship between cultural globalization and Korean tourism?\n4. Can musical characteristics be used to predict the popularity of KPOP?\n5. Can musical characteristics be used to predict the popularity of western music?\n\n## (1) The KPOP Record Labels - VAR:\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\n#install.packages(\"grDevices\")\n#library(grDevices)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\nlibrary(readxl)\nlibrary(dplyr)\n```\n\nFirstly, let's gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed. \n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Collection\"\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n```\n\n::: {.panel-tabset}\n\n## Visualization\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let's continue with the VAR model to see what the multivariate relationship is. \n\n```{r}\n#| code-fold: true\nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\nautoplot(df_ts)\n```\n\n## VARselect\nWe can see that the p-values detected from VARselect() are 5 and 1. \n```{r}\n#| code-fold: true\nVARselect(df_ts, lag.max=10, type=\"both\")\n```\n\n## Initial selection \nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don't see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM. \n\nThus, before we continue with the model, we will also verify through a CV test. \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df_ts, p=1, type='both'))\nsummary(vars::VAR(df_ts, p=5, type='both'))\n```\n\n## Cross Validation \nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. \n\n```{r}\n#| code-fold: true\n#| warning: true\n\n\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df_ts[-(start:end), ]\n  test_model <- df_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\n## Model Creation\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies. \n\n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n#--\n\nvar_model_2 <- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n```\n\n## Forecasting \n```{r}\n#| code-fold: true\n\n\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\n:::\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. This could mean that, if HYBE were to proceed with business decisions based on KPOP record labels, they would face a downward trend in their stock prices. \n\n---\n\n## (2) KPOP and the Western industry - VAR:\n\nSimilarly, we'll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE's artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three. \n\nWe'll follow the same steps as before the get some initial p values from VARselect(). \n\n::: {.panel-tabset}\n\n## Visualization\nFrom an initial visualization, it doesn't appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we'll continue with the VAR model to work on forecasting.\n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\n#Creating a subset of only Korean Record label stock data\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\n## VARselect\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let's continue by analyzing the residuals squared errors. \n\n```{r}\n#| code-fold: true\nVARselect(df2_ts, lag.max=10, type=\"both\")\n```\n\n\n## Initial selection\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we'll continue model selection through cross validation. \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df2_ts, p=1, type='both'))\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n```\n\n\n## Cross Validation \nCV seems to have chosen a different model where p=8. Thus, we'll create models for p=1,5,8. \n\n```{r}\n#| code-fold: true\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df2_ts[-(start:end), ]\n  test_model <- df2_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\n\n## Model Creation\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 < 0.05. \n\n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\") \n\n#--\n\nvar_model_2 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n#--\n\nvar_model_3 <- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n```\n\n## Forecasting\n\n```{r}\n#| code-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\n:::\n\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP's stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea. \n\nKnowing this downward trend in the stock prices of the biggest performing record music agency, we may start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea. \n\n---\n\n## (3) - Foreign tourism in Korea on Cultural Globalization in the USA \n\nLet's see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting. \n\nWe'll combine the globalization index data from KOF with the South Korean tourism data from Statistica. \n```{r}\n#| code-fold: true\n#| warning: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal <- read_csv('globalization.csv')\nglobal <- global %>%\n  filter(code == \"USA\") %>%\n  dplyr::select(year, KOFCuGIdf)\n\ntourism <- read_xlsx('raw_data/tourism.xlsx', sheet = 'Data')\n\n\nby <- join_by(Year == year)\ndf3 <- tourism %>%\n  left_join(global, by = by) %>%\n  rename(tourists = `Number of visitor arrivals in South Korea`) %>%\n  mutate(tourists = 1000000*tourists) %>%\n  drop_na()\n\nhead(df3)\n```\n\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP's influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture. \n\n::: {.panel-tabset}\n\n## Visualization\n\nFrom the graph above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020. \n\n```{r}\n#| code-fold: true\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n```\n\n```{r}\n#| echo: false\ndf3 <- df3 %>% slice(-n())\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n```\n\n## Using Auto.Arima()\nNow, let's move on with the ARIMAX/ARMAX model. First, we'll create a model using auto.arima(). \n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant. \n\n\n```{r}\n#| code-fold: true\nfit <- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n\n## Manual Model \n\nWe'll move now to find the ARMAX model manually. Let's start by taking creating a regression model of tourism on cultural globalization. Using that model, we'll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we'll validate the model through cross validation. \n\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data. \n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0).\n\n```{r}\n#| code-fold: true\n#| warning: false\n\ndf3$tourists <-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf <-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg <- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n## Model Fits\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we'll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting. \n\n```{r}\n#| code-fold: true\n#| warning: false\ncapture.output(sarima(res.fit, 1,0,0)) \ncapture.output(sarima(res.fit, 2,0,2)) \n```\n\n\n## CV\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we'll choose to forecast Korean tourism on cultural globalization in the US via model 1. \n\n```{r}\n#| code-fold: true\n#| warning: false\nn <- length(res.fit)\nk <- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 15)\nrmse2 <- matrix(NA, 15)\nrmse3 <- matrix(NA, 15)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  fit3 <- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 <- forecast(fit3, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] <- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n```\n\n## Final Model Fit\n```{r}\n#| code-fold: true\n#| warning: false\nfit <- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n```\n\n## Forecasting\n```{r}\n#| code-fold: true\n#| warning: false\n\ntourists_fit <- auto.arima(global_ts[, \"tourists\"]) \nft <- forecast(tourists_fit)\n\nfcast <- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n\nsummary(tourists_fit)\n```\n\n:::\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea's tourism of foreigners will see a slight decrease. As we've observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done. \n\n## (4) KPOP and Musical Characteristsics \n\nAs we saw in [Data Visualization](http://shriya-chinthak.georgetown.domains/DSAN_5600/data-visualization.html), KPOP as a genre seems to be heavily correlated with loudness, energy, and valence. This energetic sound is something that is very characteristic of KPOP, and thus, it will be insightful to note is these factors someone change the prediction of the popularity metric. \n\n*Please note: As a reminder, due to the recent changes in the Spotify API, popularity score is no longer available for all songs. Thus, in order to represent all the songs of an artist, we extrapolated with linear regression. Additionally, since songs releases are no consistent, we do not have as much data to work with for the four artsist we're analyzing.*\n\nTo analyze KPOP as a whole, we'll be taking the average of all metrics per year as well as the average popularity score. \n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Collection\"\n\n\nkpop_artists <- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists <- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\nspotify <- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\nkpop_arimax <- spotify %>%\n  filter(artist_name %in% kpop_artists) %>%\n  group_by(album_release_year) %>%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n```\n\n::: {.panel-tabset}\n\n## Visualization\nAll musical characteristics do not seem to have a significant trend or patterns in the data. \n\n```{r}\nkpop_ts <-ts(kpop_arimax, start = 2013, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(kpop_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for KPOP Artists\") \n```\n\n## auto.arima()\nThe auto.arima() model, similar to our ARIMA model, produced ARIMA(0,0,0). Thus, we'll continue to explore a linear regression with the model ARIMA(0,0,0). \n\n```{r}\nkpop_arimax$instrumentalness <-ts(kpop_arimax$instrumentalness, start= 2013, frequency = 1)\nkpop_arimax$valence <-ts(kpop_arimax$valence, start= 2013, frequency = 1)\nkpop_arimax$danceability <-ts(kpop_arimax$danceability, start= 2013, frequency = 1)\nkpop_arimax$energy <-ts(kpop_arimax$energy, start= 2013, frequency = 1)\nkpop_arimax$loudness <-ts(kpop_arimax$loudness, start= 2013, frequency = 1)\nkpop_arimax$speechiness <-ts(kpop_arimax$speechiness, start= 2013, frequency = 1)\nkpop_arimax$acousticness <-ts(kpop_arimax$acousticness, start= 2013, frequency = 1)\nkpop_arimax$liveness <-ts(kpop_arimax$liveness, start= 2013, frequency = 1)\nkpop_arimax$tempo <-ts(kpop_arimax$tempo, start= 2013, frequency = 1)\nkpop_arimax$popularity <-ts(kpop_arimax$popularity, start= 2013, frequency = 1)\n\n\nfit <- auto.arima(kpop_ts[, \"popularity\"], xreg = kpop_ts[, c(2:10)])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n## Manual Model\nUnfortunetly, while the residuals are not autocorrelated, none of the predictors were deemed significant to the popularity score. Therefore, for the purposes of modeling, we'll continue was the smallest p-valed predictors intrumentalness and tempo. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n############# First fit the linear model##########\nfit.reg <- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = kpop_arimax)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| echo: false\nfit.reg <- lm(popularity ~ instrumentalness + tempo, data = kpop_arimax)\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit) +ggtitle(\"ACF Plot for Residuals - Second Function\")\nggPacf(res.fit)+ggtitle(\"PACF Plot for Residuals - Second Function\")\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n## Model Fits\nThe only model selected was ARIMA(0,0,0). However, due to a lack of data, we were unable to capture model diagnostic output for the model ARIMA(0,0,0) on the residuals. Thus, we'll continue this linear analysis. \n```{r}\n#| eval: false\ncapture.output(sarima(res.fit, 0,0,0))\n```\n\n## CV\nSince we're only looking at one model, there is no need for cross validation. We'll continue to forecasting this model. \n\n## Forecasting\n```{r}\nfit <- Arima(kpop_ts[, \"popularity\"], order=c(0,0,0), xreg = kpop_ts[, c('instrumentalness', 'tempo')])\n\nmusic_fit <- auto.arima(kpop_ts[, \"instrumentalness\"]) \nft <- forecast(music_fit)\n\nmusic_fit <- auto.arima(kpop_ts[, \"tempo\"]) \nft2 <- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            TEMPO = ft2$mean)\n\nfcast <- forecast(fit, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n```\n\n:::\n\nUnfortunetly, due to the limitations of the dataset, we were unable to accurately predict popularity based on musical characteristics. However, some insights were that while tempo and instrumentalness were siginificant on their own at 90% confidence to the popularity, as a whole, the musical characteristics weren't significant to the popularity. \n\n\n## (5) Western Artists' Discography and Musical Characteristsics \nAs western artists have dominated the global sphere for decades, it would be interesting to understand if there are specific musical qualities that attribute to this popularity and fame. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Data Collection\"\n#| warning: false\n\n\nwestern_arimax <- spotify %>%\n  filter(artist_name %in% western_artists) %>%\n  group_by(album_release_year) %>%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n```\n\n::: {.panel-tabset}\n\n## Visualization\nSimilarly to KPOP, we cannot see ask specific trends of patterns in this data. \n```{r}\n#| code-fold: true\nwestern_ts <-ts(western_arimax, start = 2003, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(western_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for Western Artists\") \n```\n\n## auto.arima()\nBased on our data, auto.arima() suggests the model ARIMA(0,0,0). This may be due to the fact that musical characteristics do not effect popularity. We'll look to out manual approach for other conclusions. \n\n```{r}\n#| code-fold: true\nwestern_arimax$instrumentalness <-ts(western_arimax$instrumentalness, start= 2003, frequency = 1)\nwestern_arimax$valence <-ts(western_arimax$valence, start= 2003, frequency = 1)\nwestern_arimax$danceability <-ts(western_arimax$danceability, start= 2003, frequency = 1)\nwestern_arimax$energy <-ts(western_arimax$energy, start= 2003, frequency = 1)\nwestern_arimax$loudness <-ts(western_arimax$loudness, start= 2003, frequency = 1)\nwestern_arimax$speechiness <-ts(western_arimax$speechiness, start= 2003, frequency = 1)\nwestern_arimax$acousticness <-ts(western_arimax$acousticness, start= 2003, frequency = 1)\nwestern_arimax$liveness <-ts(western_arimax$liveness, start= 2003, frequency = 1)\nwestern_arimax$tempo <-ts(western_arimax$tempo, start= 2003, frequency = 1)\nwestern_arimax$popularity <-ts(western_arimax$popularity, start= 2003, frequency = 1)\n\n\nfit <- auto.arima(western_ts[, \"popularity\"], xreg = western_ts[, c(2:10)])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n## Manual Model \nAfter regressing on the musical characteristics, it was found that instrumentalness, danceability, energy, loudness, and speechiness were significant to the popularity score. The residuals of the narrowed done model were approximately siginificant. Thus, using those residuals, the model we found manually was ARIMA(0,0,1), which produces a much smaller AIC, BIC, and high p-values in the Ljung-Box statistic test. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n############# First fit the linear model##########\nfit.reg <- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = western_ts)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2003, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| echo: false\nfit.reg <- lm(popularity ~ instrumentalness + danceability + energy + loudness + speechiness, data = western_arimax)\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit) +ggtitle(\"ACF Plot for Residuals - Second Function\")\nggPacf(res.fit)+ggtitle(\"PACF Plot for Residuals - Second Function\")\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n\n## Model Fits\nThe model ARIMA(0,0,1) works siginificantly better than ARIMA(0,0,0) due to the lower AIC, BIC, and AICc numbers. \n```{r}\n#| warning: false\n#| code-fold: true\ncapture.output(sarima(res.fit, 0,0,1)) \ncapture.output(sarima(res.fit, 0,0,0)) \n```\n\n## CV\nUsing corss validation, we can confirm that ARIMA(0,0,1), or fit2, is the better model since is stays at a lower RMSE for the majority of the time in the plot below. \n```{r}\n#| warning: false\n#| code-fold: true\nn <- length(res.fit)\nk <- 3  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 8)\nrmse2 <- matrix(NA, 8)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:8) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(0, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(0, 0, 1), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n}\n\nplot(1:8, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:8, rmse1, type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\"), col = 2:3, lty = 1)\n\n```\n\n## Final Model Fit \nThe final fit is ARIMA(0,0,1) of the residuals of regression on popularity with predictors instrumentalness, danceability, energy, loudness, and speechiness. \n```{r}\n#| warning: false\n#| code-fold: true\nfit <- Arima(western_ts[, \"popularity\"], order=c(0,0,1), xreg = western_ts[, c('instrumentalness', 'danceability','energy', 'loudness', 'speechiness')])\n\nsummary(fit)\n```\n\n## Forecasting\n```{r}\n#| warning: false\n#| code-fold: true\nmusic_fit <- auto.arima(western_ts[, \"instrumentalness\"]) \nft <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"danceability\"]) \nft2 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"energy\"]) \nft3 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"loudness\"]) \nft4 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"speechiness\"]) \nft5 <- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            DANCE = ft2$mean,\n            ENERGY = ft3$mean,\n            LOUDNESS = ft4$mean,\n            SPEECH = ft5$mean)\n\nfcast <- forecast(fit, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n```\n\n:::\n\nThe forecasting for Western Artists with musical charactericals as predictors did enhance the overall predition, we a approximately constant popularity going forward. From this, we could say that popularity for Western artists seems to do more with musical characteristics than KPOP artists. This may be because KPOP artists rise to fame for a variety of reasons, viral music videos, dancing, viral moments, social media presence, and more. Therefore, for KPOP, it may not be a good metric to only use musical characteristics to define popularity. ","srcMarkdownNoYaml":"\n\nIn order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we'll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations @grammy. Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\n\nThe next relationship we'll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we'll see Universal Music Group and Warner's relationship on HYBE and whether it's significant.\n\nLastly, we'll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry.\n\n### Key Questions:\n\n1. What is the relationship between KPOP groups?\n2. What is the relationship between HYBE and the Western industry?\n3. What is the relationship between cultural globalization and Korean tourism?\n4. Can musical characteristics be used to predict the popularity of KPOP?\n5. Can musical characteristics be used to predict the popularity of western music?\n\n## (1) The KPOP Record Labels - VAR:\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\n#install.packages(\"grDevices\")\n#library(grDevices)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\nlibrary(readxl)\nlibrary(dplyr)\n```\n\nFirstly, let's gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed. \n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Collection\"\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n```\n\n::: {.panel-tabset}\n\n## Visualization\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let's continue with the VAR model to see what the multivariate relationship is. \n\n```{r}\n#| code-fold: true\nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\nautoplot(df_ts)\n```\n\n## VARselect\nWe can see that the p-values detected from VARselect() are 5 and 1. \n```{r}\n#| code-fold: true\nVARselect(df_ts, lag.max=10, type=\"both\")\n```\n\n## Initial selection \nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don't see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM. \n\nThus, before we continue with the model, we will also verify through a CV test. \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df_ts, p=1, type='both'))\nsummary(vars::VAR(df_ts, p=5, type='both'))\n```\n\n## Cross Validation \nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. \n\n```{r}\n#| code-fold: true\n#| warning: true\n\n\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df_ts[-(start:end), ]\n  test_model <- df_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\n## Model Creation\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies. \n\n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n#--\n\nvar_model_2 <- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n```\n\n## Forecasting \n```{r}\n#| code-fold: true\n\n\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\n:::\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. This could mean that, if HYBE were to proceed with business decisions based on KPOP record labels, they would face a downward trend in their stock prices. \n\n---\n\n## (2) KPOP and the Western industry - VAR:\n\nSimilarly, we'll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE's artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three. \n\nWe'll follow the same steps as before the get some initial p values from VARselect(). \n\n::: {.panel-tabset}\n\n## Visualization\nFrom an initial visualization, it doesn't appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we'll continue with the VAR model to work on forecasting.\n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\n#Creating a subset of only Korean Record label stock data\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\n## VARselect\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let's continue by analyzing the residuals squared errors. \n\n```{r}\n#| code-fold: true\nVARselect(df2_ts, lag.max=10, type=\"both\")\n```\n\n\n## Initial selection\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we'll continue model selection through cross validation. \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df2_ts, p=1, type='both'))\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n```\n\n\n## Cross Validation \nCV seems to have chosen a different model where p=8. Thus, we'll create models for p=1,5,8. \n\n```{r}\n#| code-fold: true\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df2_ts[-(start:end), ]\n  test_model <- df2_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\n\n## Model Creation\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 < 0.05. \n\n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\") \n\n#--\n\nvar_model_2 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n#--\n\nvar_model_3 <- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n```\n\n## Forecasting\n\n```{r}\n#| code-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\n:::\n\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP's stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea. \n\nKnowing this downward trend in the stock prices of the biggest performing record music agency, we may start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea. \n\n---\n\n## (3) - Foreign tourism in Korea on Cultural Globalization in the USA \n\nLet's see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting. \n\nWe'll combine the globalization index data from KOF with the South Korean tourism data from Statistica. \n```{r}\n#| code-fold: true\n#| warning: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal <- read_csv('globalization.csv')\nglobal <- global %>%\n  filter(code == \"USA\") %>%\n  dplyr::select(year, KOFCuGIdf)\n\ntourism <- read_xlsx('raw_data/tourism.xlsx', sheet = 'Data')\n\n\nby <- join_by(Year == year)\ndf3 <- tourism %>%\n  left_join(global, by = by) %>%\n  rename(tourists = `Number of visitor arrivals in South Korea`) %>%\n  mutate(tourists = 1000000*tourists) %>%\n  drop_na()\n\nhead(df3)\n```\n\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP's influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture. \n\n::: {.panel-tabset}\n\n## Visualization\n\nFrom the graph above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020. \n\n```{r}\n#| code-fold: true\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n```\n\n```{r}\n#| echo: false\ndf3 <- df3 %>% slice(-n())\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n```\n\n## Using Auto.Arima()\nNow, let's move on with the ARIMAX/ARMAX model. First, we'll create a model using auto.arima(). \n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant. \n\n\n```{r}\n#| code-fold: true\nfit <- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n\n## Manual Model \n\nWe'll move now to find the ARMAX model manually. Let's start by taking creating a regression model of tourism on cultural globalization. Using that model, we'll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we'll validate the model through cross validation. \n\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data. \n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0).\n\n```{r}\n#| code-fold: true\n#| warning: false\n\ndf3$tourists <-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf <-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg <- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n## Model Fits\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we'll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting. \n\n```{r}\n#| code-fold: true\n#| warning: false\ncapture.output(sarima(res.fit, 1,0,0)) \ncapture.output(sarima(res.fit, 2,0,2)) \n```\n\n\n## CV\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we'll choose to forecast Korean tourism on cultural globalization in the US via model 1. \n\n```{r}\n#| code-fold: true\n#| warning: false\nn <- length(res.fit)\nk <- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 15)\nrmse2 <- matrix(NA, 15)\nrmse3 <- matrix(NA, 15)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  fit3 <- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 <- forecast(fit3, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] <- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n```\n\n## Final Model Fit\n```{r}\n#| code-fold: true\n#| warning: false\nfit <- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n```\n\n## Forecasting\n```{r}\n#| code-fold: true\n#| warning: false\n\ntourists_fit <- auto.arima(global_ts[, \"tourists\"]) \nft <- forecast(tourists_fit)\n\nfcast <- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n\nsummary(tourists_fit)\n```\n\n:::\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea's tourism of foreigners will see a slight decrease. As we've observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done. \n\n## (4) KPOP and Musical Characteristsics \n\nAs we saw in [Data Visualization](http://shriya-chinthak.georgetown.domains/DSAN_5600/data-visualization.html), KPOP as a genre seems to be heavily correlated with loudness, energy, and valence. This energetic sound is something that is very characteristic of KPOP, and thus, it will be insightful to note is these factors someone change the prediction of the popularity metric. \n\n*Please note: As a reminder, due to the recent changes in the Spotify API, popularity score is no longer available for all songs. Thus, in order to represent all the songs of an artist, we extrapolated with linear regression. Additionally, since songs releases are no consistent, we do not have as much data to work with for the four artsist we're analyzing.*\n\nTo analyze KPOP as a whole, we'll be taking the average of all metrics per year as well as the average popularity score. \n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Collection\"\n\n\nkpop_artists <- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists <- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\nspotify <- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\nkpop_arimax <- spotify %>%\n  filter(artist_name %in% kpop_artists) %>%\n  group_by(album_release_year) %>%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n```\n\n::: {.panel-tabset}\n\n## Visualization\nAll musical characteristics do not seem to have a significant trend or patterns in the data. \n\n```{r}\nkpop_ts <-ts(kpop_arimax, start = 2013, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(kpop_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for KPOP Artists\") \n```\n\n## auto.arima()\nThe auto.arima() model, similar to our ARIMA model, produced ARIMA(0,0,0). Thus, we'll continue to explore a linear regression with the model ARIMA(0,0,0). \n\n```{r}\nkpop_arimax$instrumentalness <-ts(kpop_arimax$instrumentalness, start= 2013, frequency = 1)\nkpop_arimax$valence <-ts(kpop_arimax$valence, start= 2013, frequency = 1)\nkpop_arimax$danceability <-ts(kpop_arimax$danceability, start= 2013, frequency = 1)\nkpop_arimax$energy <-ts(kpop_arimax$energy, start= 2013, frequency = 1)\nkpop_arimax$loudness <-ts(kpop_arimax$loudness, start= 2013, frequency = 1)\nkpop_arimax$speechiness <-ts(kpop_arimax$speechiness, start= 2013, frequency = 1)\nkpop_arimax$acousticness <-ts(kpop_arimax$acousticness, start= 2013, frequency = 1)\nkpop_arimax$liveness <-ts(kpop_arimax$liveness, start= 2013, frequency = 1)\nkpop_arimax$tempo <-ts(kpop_arimax$tempo, start= 2013, frequency = 1)\nkpop_arimax$popularity <-ts(kpop_arimax$popularity, start= 2013, frequency = 1)\n\n\nfit <- auto.arima(kpop_ts[, \"popularity\"], xreg = kpop_ts[, c(2:10)])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n## Manual Model\nUnfortunetly, while the residuals are not autocorrelated, none of the predictors were deemed significant to the popularity score. Therefore, for the purposes of modeling, we'll continue was the smallest p-valed predictors intrumentalness and tempo. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n############# First fit the linear model##########\nfit.reg <- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = kpop_arimax)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| echo: false\nfit.reg <- lm(popularity ~ instrumentalness + tempo, data = kpop_arimax)\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit) +ggtitle(\"ACF Plot for Residuals - Second Function\")\nggPacf(res.fit)+ggtitle(\"PACF Plot for Residuals - Second Function\")\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n## Model Fits\nThe only model selected was ARIMA(0,0,0). However, due to a lack of data, we were unable to capture model diagnostic output for the model ARIMA(0,0,0) on the residuals. Thus, we'll continue this linear analysis. \n```{r}\n#| eval: false\ncapture.output(sarima(res.fit, 0,0,0))\n```\n\n## CV\nSince we're only looking at one model, there is no need for cross validation. We'll continue to forecasting this model. \n\n## Forecasting\n```{r}\nfit <- Arima(kpop_ts[, \"popularity\"], order=c(0,0,0), xreg = kpop_ts[, c('instrumentalness', 'tempo')])\n\nmusic_fit <- auto.arima(kpop_ts[, \"instrumentalness\"]) \nft <- forecast(music_fit)\n\nmusic_fit <- auto.arima(kpop_ts[, \"tempo\"]) \nft2 <- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            TEMPO = ft2$mean)\n\nfcast <- forecast(fit, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n```\n\n:::\n\nUnfortunetly, due to the limitations of the dataset, we were unable to accurately predict popularity based on musical characteristics. However, some insights were that while tempo and instrumentalness were siginificant on their own at 90% confidence to the popularity, as a whole, the musical characteristics weren't significant to the popularity. \n\n\n## (5) Western Artists' Discography and Musical Characteristsics \nAs western artists have dominated the global sphere for decades, it would be interesting to understand if there are specific musical qualities that attribute to this popularity and fame. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Data Collection\"\n#| warning: false\n\n\nwestern_arimax <- spotify %>%\n  filter(artist_name %in% western_artists) %>%\n  group_by(album_release_year) %>%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n```\n\n::: {.panel-tabset}\n\n## Visualization\nSimilarly to KPOP, we cannot see ask specific trends of patterns in this data. \n```{r}\n#| code-fold: true\nwestern_ts <-ts(western_arimax, start = 2003, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(western_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for Western Artists\") \n```\n\n## auto.arima()\nBased on our data, auto.arima() suggests the model ARIMA(0,0,0). This may be due to the fact that musical characteristics do not effect popularity. We'll look to out manual approach for other conclusions. \n\n```{r}\n#| code-fold: true\nwestern_arimax$instrumentalness <-ts(western_arimax$instrumentalness, start= 2003, frequency = 1)\nwestern_arimax$valence <-ts(western_arimax$valence, start= 2003, frequency = 1)\nwestern_arimax$danceability <-ts(western_arimax$danceability, start= 2003, frequency = 1)\nwestern_arimax$energy <-ts(western_arimax$energy, start= 2003, frequency = 1)\nwestern_arimax$loudness <-ts(western_arimax$loudness, start= 2003, frequency = 1)\nwestern_arimax$speechiness <-ts(western_arimax$speechiness, start= 2003, frequency = 1)\nwestern_arimax$acousticness <-ts(western_arimax$acousticness, start= 2003, frequency = 1)\nwestern_arimax$liveness <-ts(western_arimax$liveness, start= 2003, frequency = 1)\nwestern_arimax$tempo <-ts(western_arimax$tempo, start= 2003, frequency = 1)\nwestern_arimax$popularity <-ts(western_arimax$popularity, start= 2003, frequency = 1)\n\n\nfit <- auto.arima(western_ts[, \"popularity\"], xreg = western_ts[, c(2:10)])\nsummary(fit)\ncheckresiduals(fit)\n```\n\n## Manual Model \nAfter regressing on the musical characteristics, it was found that instrumentalness, danceability, energy, loudness, and speechiness were significant to the popularity score. The residuals of the narrowed done model were approximately siginificant. Thus, using those residuals, the model we found manually was ARIMA(0,0,1), which produces a much smaller AIC, BIC, and high p-values in the Ljung-Box statistic test. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n############# First fit the linear model##########\nfit.reg <- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = western_ts)\nsummary(fit.reg)\n```\n\n```{r}\n#| code-fold: true\nres.fit<-ts(residuals(fit.reg), start= 2003, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\n\n```{r}\n#| echo: false\nfit.reg <- lm(popularity ~ instrumentalness + danceability + energy + loudness + speechiness, data = western_arimax)\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit) +ggtitle(\"ACF Plot for Residuals - Second Function\")\nggPacf(res.fit)+ggtitle(\"PACF Plot for Residuals - Second Function\")\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\n\n## Model Fits\nThe model ARIMA(0,0,1) works siginificantly better than ARIMA(0,0,0) due to the lower AIC, BIC, and AICc numbers. \n```{r}\n#| warning: false\n#| code-fold: true\ncapture.output(sarima(res.fit, 0,0,1)) \ncapture.output(sarima(res.fit, 0,0,0)) \n```\n\n## CV\nUsing corss validation, we can confirm that ARIMA(0,0,1), or fit2, is the better model since is stays at a lower RMSE for the majority of the time in the plot below. \n```{r}\n#| warning: false\n#| code-fold: true\nn <- length(res.fit)\nk <- 3  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 8)\nrmse2 <- matrix(NA, 8)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:8) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(0, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(0, 0, 1), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n}\n\nplot(1:8, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:8, rmse1, type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\"), col = 2:3, lty = 1)\n\n```\n\n## Final Model Fit \nThe final fit is ARIMA(0,0,1) of the residuals of regression on popularity with predictors instrumentalness, danceability, energy, loudness, and speechiness. \n```{r}\n#| warning: false\n#| code-fold: true\nfit <- Arima(western_ts[, \"popularity\"], order=c(0,0,1), xreg = western_ts[, c('instrumentalness', 'danceability','energy', 'loudness', 'speechiness')])\n\nsummary(fit)\n```\n\n## Forecasting\n```{r}\n#| warning: false\n#| code-fold: true\nmusic_fit <- auto.arima(western_ts[, \"instrumentalness\"]) \nft <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"danceability\"]) \nft2 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"energy\"]) \nft3 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"loudness\"]) \nft4 <- forecast(music_fit)\n\nmusic_fit <- auto.arima(western_ts[, \"speechiness\"]) \nft5 <- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            DANCE = ft2$mean,\n            ENERGY = ft3$mean,\n            LOUDNESS = ft4$mean,\n            SPEECH = ft5$mean)\n\nfcast <- forecast(fit, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n```\n\n:::\n\nThe forecasting for Western Artists with musical charactericals as predictors did enhance the overall predition, we a approximately constant popularity going forward. From this, we could say that popularity for Western artists seems to do more with musical characteristics than KPOP artists. This may be because KPOP artists rise to fame for a variety of reasons, viral music videos, dancing, viral moments, social media presence, and more. Therefore, for KPOP, it may not be a good metric to only use musical characteristics to define popularity. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"arimax.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"minty","title":"ARIMAX, SARIMAX, and VAR","bibliography":["intro_reference.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}