{"title":"ARIMAX, SARIMAX, and VAR","markdown":{"yaml":{"title":"ARIMAX, SARIMAX, and VAR","bibliography":"intro_reference.bib"},"headingText":"Key Questions:","containsRefs":false,"markdown":"\n\nIn order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we'll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations (@grammy). Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\n\nThe next relationship we'll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we'll see Universal Music Group and Warner's relationship on HYBE and whether it's significant.\n\nLastly, we'll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry.\n\n\n1.  What is the relationship between KPOP groups?\n2.  What is the relationship between HYBE and the Western industry?\n3.  What is the relationship between cultural globalization and Korean tourism?\n\n## (1) The KPOP Record Labels - VAR:\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\n#install.packages(\"grDevices\")\n#library(grDevices)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\nlibrary(readxl)\nlibrary(dplyr)\n```\n\nFirstly, let's gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n```\n\n### Converting to Time Series\n\nNext, we'll take all the KPOP entertainment companies' stock prices and convert them into time series objects. \n```{r}\n#| code-fold: true\nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n```\n\n### Visualizing the data:\n```{r}\n#| code-fold: true\nautoplot(df_ts)\n```\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let's continue with the VAR model to see what the multivariate relationship is. \n\n### VARselect\n```{r}\n#| code-fold: true\nVARselect(df_ts, lag.max=10, type=\"both\")\n```\n\nWe can see that the p-values detected from VARselect() are 5 and 1. \n\n### Initial selection: \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df_ts, p=1, type='both'))\nsummary(vars::VAR(df_ts, p=5, type='both'))\n```\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don't see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM. \n\nThus, before we continue with the model, we will also verify through a CV test. \n\n### Cross Validation: \n```{r}\n#| code-fold: true\n#| warning: true\n\n\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df_ts[-(start:end), ]\n  test_model <- df_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. \n\n### Model Creation: \n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n#--\n\nvar_model_2 <- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n```\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies. \n\n### Forecasting: \n```{r}\n#| cold-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. \n\n\n## (2) KPOP and the Western industry - VAR:\n\nSimilarly, we'll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE's artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three. \n\nWe'll follow the same steps as before the get some initial p values from VARselect(). \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\n#Creating a subset of only Korean Record label stock data\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\nFrom an initial visualization, it doesn't appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we'll continue with the VAR model to work on forecasting.\n\n### VARselect\n```{r}\n#| code-fold: true\nVARselect(df2_ts, lag.max=10, type=\"both\")\n```\n\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let's continue by analyzing the residuals squared errors. \n\n### Initial selection: \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df2_ts, p=1, type='both'))\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n```\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we'll continue model selection through cross validation. \n\n### Cross Validation: \n```{r}\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df2_ts[-(start:end), ]\n  test_model <- df2_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\nCV seems to have chosen a different model where p=8. Thus, we'll create models for p=1,5,8. \n\n### Model Creation: \n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\") \n\n#--\n\nvar_model_2 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n#--\n\nvar_model_3 <- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n```\n\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 < 0.05. \n\n### Forecasting:\n```{r}\n#| cold-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP's stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea. \n\nKnowing this downward trend in the stock prices of the biggest performing KPOP music agency, we can start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea. \n\n\n## (3) - Foreign tourism in Korea on Cultural Globalization in the USA \n\nLet's see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting. \n\n### Gathering the Data\n\nWe'll combine the globalization index data from KOF with the South Korean tourism data from Statistica. \n```{r}\n#| code-fold: true\n#| warning: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal <- read_csv('globalization.csv')\nglobal <- global %>%\n  filter(code == \"USA\") %>%\n  dplyr::select(year, KOFCuGIdf)\n\ntourism <- read_xlsx('tourism.xlsx', sheet = 'Data')\n\n\nby <- join_by(Year == year)\ndf3 <- tourism %>%\n  left_join(global, by = by) %>%\n  rename(tourists = `Number of visitor arrivals in South Korea`) %>%\n  mutate(tourists = 1000000*tourists) %>%\n  drop_na()\n```\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP's influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture. \n\n```{r}\n#| code-fold: true\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n```\n\nFrom the graphs above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020. \n\n```{r}\n#| echo: false\ndf3 <- df3 %>% slice(-n())\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n```\n\n### Using Auto.Arima()\n\nNow, let's move on with the ARIMAX/ARMAX model. First, we'll create a model using auto.arima(). \n\n```{r}\n#| code-fold: true\nfit <- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\ncheckresiduals(fit)\n```\n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant. \n\n\n### Manually Finding the Model: \n\nWe'll move now to find the ARMAX model manually. Let's start by taking creating a regression model of tourism on cultural globalization. Using that model, we'll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we'll validate the model through cross validation. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\ndf3$tourists <-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf <-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg <- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n```\n\n```{r}\nres.fit<-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data. \n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0). Thus, we'll take a look at the residuals of the following models:\n\n```{r}\n#| code-fold: true\n#| warning: false\ncapture.output(sarima(res.fit, 1,0,0)) \ncapture.output(sarima(res.fit, 2,0,2)) \n```\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we'll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting. \n\n\n### CV\n\n```{r}\n#| code-fold: true\n#| warning: false\nn <- length(res.fit)\nk <- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 15)\nrmse2 <- matrix(NA, 15)\nrmse3 <- matrix(NA, 15)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  fit3 <- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 <- forecast(fit3, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] <- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n```\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we'll choose to forecast Korean tourism on cultural globalization in the US via model 1. \n\n```{r}\n#| code-fold: true\n#| warning: false\nfit <- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n```\n\n### Forecasting: \n\n```{r}\n#| code-fold: true\n#| warning: false\ntourists_fit <-auto.arima(global_ts[, \"tourists\"]) \nsummary(tourists_fit)\n\nft<-forecast(tourists_fit)\n\nfcast <- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n```\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea's tourism of foreigners will see a slight decrease. As we've observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done. ","srcMarkdownNoYaml":"\n\nIn order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we'll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations (@grammy). Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\n\nThe next relationship we'll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we'll see Universal Music Group and Warner's relationship on HYBE and whether it's significant.\n\nLastly, we'll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry.\n\n### Key Questions:\n\n1.  What is the relationship between KPOP groups?\n2.  What is the relationship between HYBE and the Western industry?\n3.  What is the relationship between cultural globalization and Korean tourism?\n\n## (1) The KPOP Record Labels - VAR:\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\n#install.packages(\"grDevices\")\n#library(grDevices)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\nlibrary(readxl)\nlibrary(dplyr)\n```\n\nFirstly, let's gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n```\n\n### Converting to Time Series\n\nNext, we'll take all the KPOP entertainment companies' stock prices and convert them into time series objects. \n```{r}\n#| code-fold: true\nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n```\n\n### Visualizing the data:\n```{r}\n#| code-fold: true\nautoplot(df_ts)\n```\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let's continue with the VAR model to see what the multivariate relationship is. \n\n### VARselect\n```{r}\n#| code-fold: true\nVARselect(df_ts, lag.max=10, type=\"both\")\n```\n\nWe can see that the p-values detected from VARselect() are 5 and 1. \n\n### Initial selection: \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df_ts, p=1, type='both'))\nsummary(vars::VAR(df_ts, p=5, type='both'))\n```\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don't see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM. \n\nThus, before we continue with the model, we will also verify through a CV test. \n\n### Cross Validation: \n```{r}\n#| code-fold: true\n#| warning: true\n\n\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df_ts[-(start:end), ]\n  test_model <- df_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. \n\n### Model Creation: \n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n#--\n\nvar_model_2 <- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"sm\")\nplot(gu.serial, names = \"jyp\") \nplot(gu.serial, names = \"yg\")\n\n```\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies. \n\n### Forecasting: \n```{r}\n#| cold-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. \n\n\n## (2) KPOP and the Western industry - VAR:\n\nSimilarly, we'll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE's artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three. \n\nWe'll follow the same steps as before the get some initial p values from VARselect(). \n\n```{r}\n#| code-fold: true\n#| warning: false\n\n\n#Creating a subset of only Korean Record label stock data\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\nFrom an initial visualization, it doesn't appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we'll continue with the VAR model to work on forecasting.\n\n### VARselect\n```{r}\n#| code-fold: true\nVARselect(df2_ts, lag.max=10, type=\"both\")\n```\n\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let's continue by analyzing the residuals squared errors. \n\n### Initial selection: \n```{r}\n#| code-fold: true\n#| warning: false\nsummary(vars::VAR(df2_ts, p=1, type='both'))\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n```\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we'll continue model selection through cross validation. \n\n### Cross Validation: \n```{r}\nfolds = 5 \nbest_model <- NULL\nbest_performance <- Inf \n\nfold_s <- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start <- (fold-1)*fold_s+1\n  end <- fold*fold_s\n  \n  train_model <- df2_ts[-(start:end), ]\n  test_model <- df2_ts[start:end, ]\n  \n  sel <- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag <- sel$selection[1]\n  \n  fit <- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h <- nrow(test_model)\n  pred <- predict(fit, n.ahead = h)\n  \n  pred_hybe <- pred$fcst$hybe[,1]\n  mse <- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse < best_performance){\n    best_model <- fit\n    best_performance <- mse\n  }\n}\n\nprint(\"The best model is: \")\nprint(best_model)\n```\n\nCV seems to have chosen a different model where p=8. Thus, we'll create models for p=1,5,8. \n\n### Model Creation: \n```{r}\n#| code-fold: true\n\nvar_model_1 <- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\") \n\n#--\n\nvar_model_2 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n#--\n\nvar_model_3 <- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial <- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\nplot(gu.serial, names = \"hybe\") \nplot(gu.serial, names = \"umgp\")\nplot(gu.serial, names = \"sony\")\n\n```\n\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 < 0.05. \n\n### Forecasting:\n```{r}\n#| cold-fold: true\npar(mar=c(1,2,3,1))\nvar_model_1 <- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n```\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP's stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea. \n\nKnowing this downward trend in the stock prices of the biggest performing KPOP music agency, we can start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea. \n\n\n## (3) - Foreign tourism in Korea on Cultural Globalization in the USA \n\nLet's see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting. \n\n### Gathering the Data\n\nWe'll combine the globalization index data from KOF with the South Korean tourism data from Statistica. \n```{r}\n#| code-fold: true\n#| warning: false\n#| echo: false\n\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal <- read_csv('globalization.csv')\nglobal <- global %>%\n  filter(code == \"USA\") %>%\n  dplyr::select(year, KOFCuGIdf)\n\ntourism <- read_xlsx('tourism.xlsx', sheet = 'Data')\n\n\nby <- join_by(Year == year)\ndf3 <- tourism %>%\n  left_join(global, by = by) %>%\n  rename(tourists = `Number of visitor arrivals in South Korea`) %>%\n  mutate(tourists = 1000000*tourists) %>%\n  drop_na()\n```\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP's influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture. \n\n```{r}\n#| code-fold: true\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n```\n\nFrom the graphs above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020. \n\n```{r}\n#| echo: false\ndf3 <- df3 %>% slice(-n())\nglobal_ts <-ts(df3, start = 2000, frequency = 1)\n```\n\n### Using Auto.Arima()\n\nNow, let's move on with the ARIMAX/ARMAX model. First, we'll create a model using auto.arima(). \n\n```{r}\n#| code-fold: true\nfit <- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\ncheckresiduals(fit)\n```\n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant. \n\n\n### Manually Finding the Model: \n\nWe'll move now to find the ARMAX model manually. Let's start by taking creating a regression model of tourism on cultural globalization. Using that model, we'll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we'll validate the model through cross validation. \n\n```{r}\n#| code-fold: true\n#| warning: false\n\ndf3$tourists <-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf <-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg <- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n```\n\n```{r}\nres.fit<-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\nggPacf(res.fit)\n```\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data. \n\n```{r}\n#| code-fold: true\n#| warning: false\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n```\n\n```{r}\n#| code-fold: true\n#| warning: false\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),] \noutput[which.min(output$AICc),]\n```\n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0). Thus, we'll take a look at the residuals of the following models:\n\n```{r}\n#| code-fold: true\n#| warning: false\ncapture.output(sarima(res.fit, 1,0,0)) \ncapture.output(sarima(res.fit, 2,0,2)) \n```\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we'll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting. \n\n\n### CV\n\n```{r}\n#| code-fold: true\n#| warning: false\nn <- length(res.fit)\nk <- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 <- matrix(NA, 15)\nrmse2 <- matrix(NA, 15)\nrmse3 <- matrix(NA, 15)\n\nst <- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end <- st + i - 1\n  xtrain <- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start <- train_end + 1\n  test_end <- min(st + i, tsp(res.fit)[2])\n  xtest <- window(res.fit, start = test_start, end = test_end)\n\n  fit <- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast <- forecast(fit, h = 4)\n\n  fit2 <- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 <- forecast(fit2, h = 4)\n\n  fit3 <- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 <- forecast(fit3, h = 4)\n\n  rmse1[i] <- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] <- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n```\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we'll choose to forecast Korean tourism on cultural globalization in the US via model 1. \n\n```{r}\n#| code-fold: true\n#| warning: false\nfit <- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n```\n\n### Forecasting: \n\n```{r}\n#| code-fold: true\n#| warning: false\ntourists_fit <-auto.arima(global_ts[, \"tourists\"]) \nsummary(tourists_fit)\n\nft<-forecast(tourists_fit)\n\nfcast <- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n```\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea's tourism of foreigners will see a slight decrease. As we've observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"arimax.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"minty","title":"ARIMAX, SARIMAX, and VAR","bibliography":["intro_reference.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}