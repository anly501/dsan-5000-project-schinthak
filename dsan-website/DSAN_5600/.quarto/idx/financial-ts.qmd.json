{"title":"Financial Time Series Models (ARCH/GARCH)","markdown":{"yaml":{"title":"Financial Time Series Models (ARCH/GARCH)"},"headingText":"HYBE and the KPOP record labels:","containsRefs":false,"markdown":"\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\n```\n\nIn order to conduct a financial time series analysis, we'll continue to look at the stock prices between US majority music labels and KPOP public record labels. In particular, let's take a look at HYBE, a music label that is the first to support artists both from the KPOP and the Western music industry. Similar to out analysis of ARIMAX and VAR models, we'll use ARCH/GARCH models in order to answer the following questions:\n\n1. Can we use the \"Big 3\" to predict the stock prices of HYBE? \n2. Can we use the Western record labels to predict the stock prices of HYBE? \n\nAnswering these two questions can help us better understand the strength of HYBE in both markets as well as which industry seems to be be the direction in which the most successful record label will delve into. \n\n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Gathering\"\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n\n#Converting to time series \nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n#Visualize\nautoplot(df_ts)\n```\n\nFrom the plot, we can see that all four stock prices are not stationary, as they all experience volatility to some extent. In terms of HYBE, we can see the largest magnitude and most volatility in the data, with sharp upward and downward trends in the data. Thus, we'll see if using \"The Big Three\" (SM, JYP, YG) and accounting for volatility in an ARCH/GARCH model will provide better predictions. \n\nThus, to begin the ARCH/GARCH analysis, we'll first need to create a regression model of these variables. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df)) \ntrain <- df[train_indices, ] \ntest <- df[-train_indices, ]\n\n#First, fitting the model: \nmodel <- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n```\n\nSince all the variables are significant, we'll continue with this model to find its residuals. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\n\nlm_predictions <- predict(model, newdata = test)\nr_squared <- cor(test$HYBE_Price, lm_predictions)^2\nrmse <- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\nprint(paste(\"RMSE:\", rmse))\n\nlm.residuals <- residuals(model)\nacf(lm.residuals)\npacf(lm.residuals)\nadf.test(lm.residuals)\n```\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals. \n\nNext, using these residuals, we'll first find the best ARMA/AR/ARIMA model. We'll do this is two ways, first using auto.arima and then manually by running through multiple models. \n\n### Using auto.arima\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\narima_model <- auto.arima(lm.residuals)\nsummary(arima_model)\n```\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output. \n\n### Manual AR/ARMA/ARIMA Model\n```{r}\n#| code-fold: true\n#| code-summary: \"Manual AR/ARMA/ARIMA code\"\n#| warning: false\n\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1<=10) #usual threshold\n    {\n      model<- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n#Best model: \ntemp[which.min(temp$AIC),]\ntemp[which.min(temp$BIC),]\ntemp[which.min(temp$AICc),]\n```\n\nThus, from the manually approach, the model with the lowest AIC value is AR(1). Thus, we'll use this model within our approach to find the best ARCH/GARCH model. \n\n### ARCH/GARCH\n\nSince the original data doesn't show time-varying volatility within stock prices, we'll go forward without testing the GARCH model. Thus, the two models we'll look at is AR + ARCH and just ARCH in order to find the best one at predicting HYBE prices given its volatility. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"ARCH selection\"\n#| warning: false\n\nbest_ar_model <- Arima(lm.residuals,order=c(1,0,0))\nar.res <- best_ar_model$residuals\n\nacf(ar.res^2)\npacf(ar.res^2)\n\narch_model <- list() ## set counter\ncc <- 1\n\nfor (p in 1:4) {\n  for (q in 1:4) {\n    arch_model[[cc]] <- garch(ar.res,order=c(q,p),trace=F)\n    cc <- cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nARCH_AIC <- sapply(arch_model, AIC) ## model with lowest AIC is the best\narch_model[[which(ARCH_AIC == min(ARCH_AIC))]]\n```\n\nFrom the manually calculation, we can see that the best ARCH model is ARCH(1,4). The next step now is to check which whether the AR+ARCH model or the ARCH model itself is the best. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Model selection\"\n#| warning: false\n\nsummary(garchFit(~garch(1,4), ar.res, trace = F)) \n\nsummary(garchFit(~arma(1, 0) + garch(1, 4), ar.res, trace = F))\n\nfina_fit <- garchFit(~garch(1,4), ar.res, trace = F)\n\npredict(fina_fit, n.ahead = 50, plot = TRUE)\n```\n\nBased on the two models, looking at the Ljung-Box Test and the AIC values, we can say that ARCH(1,4) is the best model to predict HYBE stock prices. \n\n### Equation: \n\nBased on the model above, we'll say that equation for the model is as follows:\n\n$X_t = 2108 -0.1133z_1+ 1.475z_2 + 5.063z_3 - 1.314z_4$\n\n$y^*_t = y_t−0.00134274$\n\n$y_t = \\sigma_t \\epsilon_t$\n\n$\\sigma^2_t = 21.05589755 + 0.00000001y^2_{t-1} + 0.97494048\\sigma^2_{t-1} + 0.00000001\\sigma^2_{t-2} + 0.00000001\\sigma^2_{t-3} + 0.00000001\\sigma^2_{t-4}$\n\n## HYBE and the Western record labels:\n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Visualization\"\n\n\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\nAgain, from an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, and Universal Music Group stock prices has a stationary trend. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices <- sample(seq_len(nrow(df2)), size = 0.8 * nrow(df2)) \ntrain <- df2[train_indices, ] \ntest <- df2[-train_indices, ]\n\n#First, fitting the model: \nmodel <- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n```\n\nSince all the variables are significant, we'll continue with this model to find its residuals. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\n\nlm_predictions <- predict(model, newdata = test)\nr_squared <- cor(test$HYBE_Price, lm_predictions)^2\nrmse <- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\nprint(paste(\"RMSE:\", rmse))\n\nlm.residuals <- residuals(model)\nacf(lm.residuals)\npacf(lm.residuals)\nadf.test(lm.residuals)\n```\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals. \n\nNext, using these residuals, we'll first find the best ARMA/AR/ARIMA model. We'll do this is two ways, first using auto.arima and then manually by running through multiple models. \n\n### Using auto.arima\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\narima_model <- auto.arima(lm.residuals)\nsummary(arima_model)\n```\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output. \n\n### Manual AR/ARMA/ARIMA Model\n```{r}\n#| code-fold: true\n#| code-summary: \"Manual AR/ARMA/ARIMA code\"\n#| warning: false\n\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1<=10) #usual threshold\n    {\n      model<- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n#Best model: \ntemp[which.min(temp$AIC),]\ntemp[which.min(temp$BIC),]\ntemp[which.min(temp$AICc),]\n```\n\nFrom both auto.arima() and the manual approach, we can see that no AR/ARMA/ARIMA model was created from the residuals of the linear regression model. Thus, we can say that there is no autocorrelation in the data that would call for an AR/ARMA/ARIMA model. \n\nFrom this information, we can deduce the fact that Western record labels (Warner Group and Universal Music Group) do not have an affect on HYBE stock prices. Thus, we can say that HYBE's success and future would best be predicted by the performance of other KPOP record labels (SM, JYP, and YG). Thus, we can say that for the best financial outcomes, music companies may be looking to KPOP groups and their marketing and music strategies in the coming future. ","srcMarkdownNoYaml":"\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(quantmod)\nlibrary(tidyverse)\nlibrary(imputeTS)\nlibrary(vars)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(TSA)\nlibrary(fGarch) \nlibrary(dynlm)\nlibrary(dygraphs)\n```\n\nIn order to conduct a financial time series analysis, we'll continue to look at the stock prices between US majority music labels and KPOP public record labels. In particular, let's take a look at HYBE, a music label that is the first to support artists both from the KPOP and the Western music industry. Similar to out analysis of ARIMAX and VAR models, we'll use ARCH/GARCH models in order to answer the following questions:\n\n1. Can we use the \"Big 3\" to predict the stock prices of HYBE? \n2. Can we use the Western record labels to predict the stock prices of HYBE? \n\nAnswering these two questions can help us better understand the strength of HYBE in both markets as well as which industry seems to be be the direction in which the most successful record label will delve into. \n\n## HYBE and the KPOP record labels:\n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Gathering\"\n\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP <- data.frame(UMGP$UMGP.Adjusted)\nUMGP <- UMGP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date <- as.Date(min(UMGP$Date))  \nend_date <- as.Date(max(UMGP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nUMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]\ndf_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]\nimputed_time_series <- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP <- data.frame(imputed_time_series)\n\n#---\n\nSONY <- data.frame(SONY$SONY.Adjusted)\nSONY <- SONY %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date <- as.Date(min(SONY$Date))  \nend_date <- as.Date(max(SONY$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]\ndf_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]\nimputed_time_series <- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY <- data.frame(imputed_time_series)\n\n#---\n\nHYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE <- HYBE %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(HYBE_Price = X352820.KS.Adjusted) %>%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date <- as.Date(min(HYBE$Date))  \nend_date <- as.Date(max(HYBE$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nHYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]\ndf_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]\nimputed_time_series <- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE <- data.frame(imputed_time_series)\n\n#--- \n\nSM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM <- SM %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(SM_Price = X041510.KQ.Adjusted) %>%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date <- as.Date(min(SM$Date))  \nend_date <- as.Date(max(SM$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nSM <- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]\ndf_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]\nimputed_time_series <- na_ma(SM, k = 4, weighting = \"exponential\")\nSM <- data.frame(imputed_time_series)\n\n#---\n\nYG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG <- YG %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(YG_Price = X122870.KQ.Adjusted) %>%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date <- as.Date(min(YG$Date))  \nend_date <- as.Date(max(YG$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nYG <- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]\ndf_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]\nimputed_time_series <- na_ma(YG, k = 4, weighting = \"exponential\")\nYG <- data.frame(imputed_time_series)\n\n#---\n\nJYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP <- JYP %>%\n  rownames_to_column(var = \"Date\") %>%\n  mutate(Date = as.Date(Date)) %>%\n  rename(JYP_Price = X035900.KQ.Adjusted) %>%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date <- as.Date(min(JYP$Date))  \nend_date <- as.Date(max(JYP$Date))    \ndate_range <- seq(start_date, end_date, by = \"1 day\")\ndate_dataset <- data.frame(Date = date_range)\nJYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]\ndf_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]\nimputed_time_series <- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP <- data.frame(imputed_time_series)\n\nstock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names <- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf <- HYBE %>%\n  left_join(SM, by = 'Date') %>%\n  left_join(YG, by = 'Date') %>%\n  left_join(JYP, by = 'Date')\n\n#Converting to time series \nhybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts <- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) <- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n#Visualize\nautoplot(df_ts)\n```\n\nFrom the plot, we can see that all four stock prices are not stationary, as they all experience volatility to some extent. In terms of HYBE, we can see the largest magnitude and most volatility in the data, with sharp upward and downward trends in the data. Thus, we'll see if using \"The Big Three\" (SM, JYP, YG) and accounting for volatility in an ARCH/GARCH model will provide better predictions. \n\nThus, to begin the ARCH/GARCH analysis, we'll first need to create a regression model of these variables. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df)) \ntrain <- df[train_indices, ] \ntest <- df[-train_indices, ]\n\n#First, fitting the model: \nmodel <- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n```\n\nSince all the variables are significant, we'll continue with this model to find its residuals. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\n\nlm_predictions <- predict(model, newdata = test)\nr_squared <- cor(test$HYBE_Price, lm_predictions)^2\nrmse <- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\nprint(paste(\"RMSE:\", rmse))\n\nlm.residuals <- residuals(model)\nacf(lm.residuals)\npacf(lm.residuals)\nadf.test(lm.residuals)\n```\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals. \n\nNext, using these residuals, we'll first find the best ARMA/AR/ARIMA model. We'll do this is two ways, first using auto.arima and then manually by running through multiple models. \n\n### Using auto.arima\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\narima_model <- auto.arima(lm.residuals)\nsummary(arima_model)\n```\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output. \n\n### Manual AR/ARMA/ARIMA Model\n```{r}\n#| code-fold: true\n#| code-summary: \"Manual AR/ARMA/ARIMA code\"\n#| warning: false\n\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1<=10) #usual threshold\n    {\n      model<- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n#Best model: \ntemp[which.min(temp$AIC),]\ntemp[which.min(temp$BIC),]\ntemp[which.min(temp$AICc),]\n```\n\nThus, from the manually approach, the model with the lowest AIC value is AR(1). Thus, we'll use this model within our approach to find the best ARCH/GARCH model. \n\n### ARCH/GARCH\n\nSince the original data doesn't show time-varying volatility within stock prices, we'll go forward without testing the GARCH model. Thus, the two models we'll look at is AR + ARCH and just ARCH in order to find the best one at predicting HYBE prices given its volatility. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"ARCH selection\"\n#| warning: false\n\nbest_ar_model <- Arima(lm.residuals,order=c(1,0,0))\nar.res <- best_ar_model$residuals\n\nacf(ar.res^2)\npacf(ar.res^2)\n\narch_model <- list() ## set counter\ncc <- 1\n\nfor (p in 1:4) {\n  for (q in 1:4) {\n    arch_model[[cc]] <- garch(ar.res,order=c(q,p),trace=F)\n    cc <- cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nARCH_AIC <- sapply(arch_model, AIC) ## model with lowest AIC is the best\narch_model[[which(ARCH_AIC == min(ARCH_AIC))]]\n```\n\nFrom the manually calculation, we can see that the best ARCH model is ARCH(1,4). The next step now is to check which whether the AR+ARCH model or the ARCH model itself is the best. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Model selection\"\n#| warning: false\n\nsummary(garchFit(~garch(1,4), ar.res, trace = F)) \n\nsummary(garchFit(~arma(1, 0) + garch(1, 4), ar.res, trace = F))\n\nfina_fit <- garchFit(~garch(1,4), ar.res, trace = F)\n\npredict(fina_fit, n.ahead = 50, plot = TRUE)\n```\n\nBased on the two models, looking at the Ljung-Box Test and the AIC values, we can say that ARCH(1,4) is the best model to predict HYBE stock prices. \n\n### Equation: \n\nBased on the model above, we'll say that equation for the model is as follows:\n\n$X_t = 2108 -0.1133z_1+ 1.475z_2 + 5.063z_3 - 1.314z_4$\n\n$y^*_t = y_t−0.00134274$\n\n$y_t = \\sigma_t \\epsilon_t$\n\n$\\sigma^2_t = 21.05589755 + 0.00000001y^2_{t-1} + 0.97494048\\sigma^2_{t-1} + 0.00000001\\sigma^2_{t-2} + 0.00000001\\sigma^2_{t-3} + 0.00000001\\sigma^2_{t-4}$\n\n## HYBE and the Western record labels:\n\n```{r}\n#| code-fold: true\n#| warning: false\n#| code-summary: \"Data Visualization\"\n\n\ndf2 <- HYBE %>%\n  left_join(UMGP, by = 'Date') %>%\n  left_join(SONY, by = 'Date') %>%\n  drop_na()\n\nhybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts <- cbind(hybe, umgp, sony)\ncolnames(df2_ts) <- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n```\n\nAgain, from an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, and Universal Music Group stock prices has a stationary trend. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices <- sample(seq_len(nrow(df2)), size = 0.8 * nrow(df2)) \ntrain <- df2[train_indices, ] \ntest <- df2[-train_indices, ]\n\n#First, fitting the model: \nmodel <- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n```\n\nSince all the variables are significant, we'll continue with this model to find its residuals. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\n\nlm_predictions <- predict(model, newdata = test)\nr_squared <- cor(test$HYBE_Price, lm_predictions)^2\nrmse <- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\nprint(paste(\"RMSE:\", rmse))\n\nlm.residuals <- residuals(model)\nacf(lm.residuals)\npacf(lm.residuals)\nadf.test(lm.residuals)\n```\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals. \n\nNext, using these residuals, we'll first find the best ARMA/AR/ARIMA model. We'll do this is two ways, first using auto.arima and then manually by running through multiple models. \n\n### Using auto.arima\n```{r}\n#| code-fold: true\n#| code-summary: \"Auto.arima() code\"\n#| warning: false\n\narima_model <- auto.arima(lm.residuals)\nsummary(arima_model)\n```\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output. \n\n### Manual AR/ARMA/ARIMA Model\n```{r}\n#| code-fold: true\n#| code-summary: \"Manual AR/ARMA/ARIMA code\"\n#| warning: false\n\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1<=10) #usual threshold\n    {\n      model<- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n#Best model: \ntemp[which.min(temp$AIC),]\ntemp[which.min(temp$BIC),]\ntemp[which.min(temp$AICc),]\n```\n\nFrom both auto.arima() and the manual approach, we can see that no AR/ARMA/ARIMA model was created from the residuals of the linear regression model. Thus, we can say that there is no autocorrelation in the data that would call for an AR/ARMA/ARIMA model. \n\nFrom this information, we can deduce the fact that Western record labels (Warner Group and Universal Music Group) do not have an affect on HYBE stock prices. Thus, we can say that HYBE's success and future would best be predicted by the performance of other KPOP record labels (SM, JYP, and YG). Thus, we can say that for the best financial outcomes, music companies may be looking to KPOP groups and their marketing and music strategies in the coming future. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"financial-ts.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"minty","title":"Financial Time Series Models (ARCH/GARCH)"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}