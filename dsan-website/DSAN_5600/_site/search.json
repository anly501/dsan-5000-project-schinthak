[
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA and ARIMA Models",
    "section": "",
    "text": "ARMA and ARIMA Models are used in time-series in order to forecast the time series object at had. Thus, we will take a look at both the globalization index as well as the stock fluctuation of HYBE Co. in order to forecast values and make future predictions. These metrics will help us determine the impact globalization in conjunction with record labels like HYBE have within the United States and the Western music world."
  },
  {
    "objectID": "arma.html#globalization-index",
    "href": "arma.html#globalization-index",
    "title": "ARMA and ARIMA Models",
    "section": "Globalization Index:",
    "text": "Globalization Index:\nFrom our Exploratory Data Analysis, we noticed the following information:\n\nPrior to differncing, the ACF plot show several lags above the significance bands, indicating a non-stationary relationship.\nThe Augmented Dickey-Fuller Test confirmed that the data itself was NOT stationary.\n\nThus, using this information, we will move on to differencing to fit the data to become stationary:\n\n\nDifferencing Code\ndff_global &lt;- diff(global_ts)\np1 &lt;- ggAcf(dff_global)+ggtitle(\"ACF Plot for First Differenced Globalization Index\")\np2 &lt;- ggPacf(dff_global)+ggtitle(\"PACF Plot for First Differenced Globalization Index\")\n\ngrid.arrange(p1, p2, nrow=2)\n\n\n\n\n\nFrom the ACF and PACF plots of the first differenced time series object, we can now see that lag values are all contained inside the significance bands, meaning that this distribution is in fact, stationary.\nLet’s see the Augumented Dickey-Fuller score for the first differenced data:\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dff_global\nDickey-Fuller = -3.244, Lag order = 3, p-value = 0.09043\nalternative hypothesis: stationary\n\n\nUnfortunetly, since the ADF p-value is 0.09043 &gt; 0.05, we fail to reject the null hypothesis. From teh ADF test, the distribution is still stationary. However, because this score is not allows accurate, we will assume the results of the ACF plots are correct to avoid over-differencing.\nThus from the plot, we can also state the following:\n\nSince the ACF plot doesn’t have any significant peaks, q = 0.\nSince the PACF plot doesn’t have any significant peaks, p = 0.\nSince we differenced once, d = 1.\n\nWhile q and p are zero based on the graph, it’s better to run through a multitude of options in order to find the best model for this data.\nNow, let’s move to creating the ARIMA model:\n\n\nARIMA Model\nset.seed(123)\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(dff_global,order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n131.6564\n135.4400\n131.9173\n\n\n0\n1\n1\n104.6809\n110.3564\n105.2143\n\n\n0\n1\n2\n106.6801\n114.2474\n107.5892\n\n\n0\n1\n3\n108.6416\n118.1007\n110.0369\n\n\n0\n1\n4\n110.6011\n121.9520\n112.6011\n\n\n1\n1\n0\n121.0974\n126.7728\n121.6307\n\n\n1\n1\n1\n106.6801\n114.2474\n107.5892\n\n\n1\n1\n2\n108.6249\n118.0840\n110.0203\n\n\n1\n1\n3\n110.6009\n121.9518\n112.6009\n\n\n1\n1\n4\n112.3465\n125.5893\n115.0782\n\n\n2\n1\n0\n117.9095\n125.4768\n118.8186\n\n\n2\n1\n1\n108.6378\n118.0969\n110.0332\n\n\n2\n1\n2\n110.6237\n121.9747\n112.6237\n\n\n2\n1\n3\n112.6505\n125.8933\n115.3822\n\n\n2\n1\n4\n112.5356\n127.6701\n116.1356\n\n\n3\n1\n0\n115.7033\n125.1625\n117.0987\n\n\n3\n1\n1\n110.6820\n122.0329\n112.6820\n\n\n3\n1\n2\n112.3893\n125.6320\n115.1210\n\n\n3\n1\n3\n112.5393\n127.6739\n116.1393\n\n\n3\n1\n4\n114.8561\n131.8825\n119.4715\n\n\n4\n1\n0\n116.1590\n127.5099\n118.1590\n\n\n4\n1\n1\n112.5199\n125.7626\n115.2516\n\n\n4\n1\n2\n114.3512\n129.4858\n117.9512\n\n\n4\n1\n3\n116.3600\n133.3864\n120.9754\n\n\n4\n1\n4\n116.5290\n135.4472\n122.3184\n\n\n\n\n\nThus, we can easily view that the best model has values p=0, d=1, and q=1. Let’s find the equation\n\n\nCode\nsarima(dff_global, 0, 1, 1)\n\n\ninitial  value -0.116322 \niter   2 value -0.299568\niter   3 value -0.343985\niter   4 value -0.373735\niter   5 value -0.395774\niter   6 value -0.410452\niter   7 value -0.413059\niter   8 value -0.413103\niter   9 value -0.413515\niter  10 value -0.413515\niter  11 value -0.413517\niter  12 value -0.413518\niter  12 value -0.413518\niter  12 value -0.413518\nfinal  value -0.413518 \nconverged\ninitial  value -0.405399 \niter   2 value -0.405743\niter   3 value -0.411760\niter   4 value -0.411935\niter   5 value -0.411961\niter   6 value -0.411978\niter   7 value -0.411986\niter   8 value -0.411988\niter   9 value -0.411990\niter   9 value -0.411990\niter   9 value -0.411990\nfinal  value -0.411990 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -1.0000   -0.0136\ns.e.   0.1032    0.0062\n\nsigma^2 estimated as 0.405:  log likelihood = -49.34,  aic = 104.68\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -1.0000 0.1032 -9.6922  0.0000\nconstant  -0.0136 0.0062 -2.1783  0.0344\n\n$AIC\n[1] 2.136345\n\n$AICc\n[1] 2.141669\n\n$BIC\n[1] 2.252171\n\n\nFrom the results of the sarima() function, we call say that the equation is as follows:\n\\[\\begin{align}\nx_{t} = w_{t} -1w_{t-1} - 0.0136\n\\end{align}\\]\nFrom the model diagonotics presented, we can also say that the ACF plot of residuals shows no significance, meaning the residuals are not correlated. However, the p-values of the Ljung-Box statistic is much higher than the significance band. While this is not great, we can still move forward since the residuals are not significant and that this was the best model of the ones simulated.\nNow let’s varify with auto arima:\n\n\nSeries: dff_global \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.8793\ns.e.   0.0718\n\nsigma^2 = 0.4492:  log likelihood = -50.16\nAIC=104.32   AICc=104.58   BIC=108.1\n\n\nauto.arima() also chose the same model I concluded with, thus we will continue with this model.\nNow let’s look at the forecast:\nFirst, let’s forecast based on the best model\n\n\nForecasting\nfit &lt;- Arima(dff_global, order=c(0, 1, 1))\nautoplot(forecast(fit))\n\n\n\n\n\nFrom the looks of the forecast, we can see a very large confidence band with the values mostly following the overal trend of the differenced data. Let’s compare to the naive approaches:\n\n\nForecast comparison\nsummary(fit)\n\n\nSeries: dff_global \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.8793\ns.e.   0.0718\n\nsigma^2 = 0.4492:  log likelihood = -50.16\nAIC=104.32   AICc=104.58   BIC=108.1\n\nTraining set error measures:\n                   ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.05512 0.6566845 0.503263 140.2474 227.9084 0.6989056\n                    ACF1\nTraining set -0.05065878\n\n\nForecast comparison\nautoplot(dff_global) +\n  autolayer(meanf(dff_global, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(dff_global, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(dff_global, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  autolayer(forecast(fit, h=11),\n            series=\"Fit\", PI=FALSE) +\n  ggtitle(\"Forecasts for yearly globalization metric\") +\n  xlab(\"Year\") + ylab(\"KOF Index\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAfter mapping the model with other benchmarks, it’s clear that none of the functions, fitted or benchmarks, accurately forecasts the differenced data. The fitted data seems to the only line approximately in the same range as the overall trend, however, none of them are accurate functions. As a result of this information, it may be worth taking a look at this data un-differenced in order to get a better insight of the globalization index."
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "",
    "text": "In order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we’ll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations (“BTS | Artist | GRAMMY.com — Grammy.com”). Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\nThe next relationship we’ll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we’ll see Universal Music Group and Warner’s relationship on HYBE and whether it’s significant.\nLastly, we’ll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "financial-ts.html",
    "href": "financial-ts.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "In order to conduct a financial time series analysis, we’ll continue to look at the stock prices between US majority music labels and KPOP public record labels. In particular, let’s take a look at HYBE, a music label that is the first to support artists both from the KPOP and the Western music industry. Similar to out analysis of ARIMAX and VAR models, we’ll use ARCH/GARCH models in order to answer the following questions:\nAnswering these two questions can help us better understand the strength of HYBE in both markets as well as which industry seems to be be the direction in which the most successful record label will delve into."
  },
  {
    "objectID": "saf.html",
    "href": "saf.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "deep-learning.html",
    "href": "deep-learning.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Streaming\n\n\n\n\n\nClick the image to view the API\nThe spotify API allows for access to all kinds of streaming data including artist music records and an analysis on their musical compponents. Using spotify data, I plan to analyze overall trends of popular music throughout the years as well as specific international artists with popular debuts in the U.S.\nFor example, here is a plot the artists Taylor Swift, BTS, and Twice and their music’s danceability over a period of 10 years.\n\n\nCode\naccess_token &lt;- get_spotify_access_token()\nartists &lt;- c(\"BTS\", \"Taylor Swift\", \"Twice\")\n\nBTS &lt;- get_artist_audio_features(\"BTS\")\nBTS_A &lt;- data.frame(BTS$artist_name,\nBTS$instrumentalness,\nBTS$valence,\nBTS$danceability,\nBTS$energy,\nBTS$loudness,\nBTS$speechiness,\nBTS$acousticness,\nBTS$liveness,\nBTS$tempo,\nBTS$track_name,\nBTS$album_name,\nBTS$album_release_year,\nBTS$album_release_date)\n\ncolnames(BTS_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\nTaylor_Swift &lt;- get_artist_audio_features(\"Taylor Swift\")\nTaylor_Swift_A &lt;- data.frame(Taylor_Swift$artist_name,\nTaylor_Swift$instrumentalness,\nTaylor_Swift$valence,\nTaylor_Swift$danceability,\nTaylor_Swift$energy,\nTaylor_Swift$loudness,\nTaylor_Swift$speechiness,\nTaylor_Swift$acousticness,\nTaylor_Swift$liveness,\nTaylor_Swift$tempo,\nTaylor_Swift$track_name,\nTaylor_Swift$album_name,\nTaylor_Swift$album_release_year,\nTaylor_Swift$album_release_date)\n\ncolnames(Taylor_Swift_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\n\nTwice &lt;- get_artist_audio_features(\"Twice\")\nTwice_A &lt;- data.frame(Twice$artist_name,\nTwice$instrumentalness,\nTwice$valence,\nTwice$danceability,\nTwice$energy,\nTwice$loudness, \nTwice$speechiness,\nTwice$acousticness,\nTwice$liveness,\nTwice$tempo,\nTwice$track_name,\nTwice$album_name,\nTwice$album_release_year,\nTwice$album_release_date)\n\ncolnames(Twice_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\nartists &lt;- rbind(BTS_A, Taylor_Swift_A, Twice_A)\nartists$date &lt;- as.Date(artists$date, format = \"%Y-%m-%d\")\n\nfig &lt;- plot_ly(artists, x = ~date, y = ~danceability, color = ~artist_name, \n               type = 'scatter', mode = 'markers', size = ~speechiness) \n\nfig &lt;- fig %&gt;% layout(xaxis = list(title = \"Album Released Date\"),\n                      yaxis = list(title =\"Danceability\"), \n                      title = \"Danceability - Taylor Swift/BTS/Twice\")\n\nfig\n\n\n\n\n\n\nWe can see that the prior to the introduction of international stars BTS and Twice, American artist Taylor Swift had a lower degree of danceability and speechiness. Both increased as BTS and Twice entered the market, noting a slight shift in the music trends.\n\n\nMusic Charts\n\n\n\n\n\nClick the image to download the dataset\nOne of the longest music charting services is the Billboard Chart. Every week, starting from 1958, the Billboard charts have documented the 100 top songs in the U.S. Thus, I will be using this data to anaylze the most popular songs throughout 1958-2021. Augmenting this data with genres of the songs and orgins of the artist can help further describe the globalization trends in music charting.\n\n\nMusic Stock:\nUsing the Quantmod package, we can analyze the stock prices of several music record companies over a course of multiple years. This will allow us to identify specific trends within the music industry as well as interpret shareholder’s opinions of globalization news throughout history.\n\n\nCode\n# basic example of ohlc charts\ndf &lt;- data.frame(Date=index(UMGNF),coredata(UMGNF))\ndf &lt;- tail(df, 365)\n\nfig &lt;- df %&gt;% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~UMGNF.Open, close = ~UMGNF.Close,\n          high = ~UMGNF.High, low = ~UMGNF.Low) \nfig &lt;- fig %&gt;% layout(title = \"Universal Music Group Candlestick Chart\")\n\nfig\n\n\n\n\n\n\n\n\nQuantifying Globalization\n\n\n\n\n\nClick here to download the data\nThe KOF Globalization index is a way to quantify globalization of a country. This metric was started in the 1970’s as countries increasingly began to embrace globalization. This metric can be used in tandum with music trends in order to decipher if musical globalization is happening concurrently with general globalization.\n\n\nLive Music\n\n\n\n\n\nUsing revenue data from the world tours of all 8 artists over their careers (up until the last decade), we will be able to analyze the economic impact of concerts in the United States, both from Western and KPOP artists.\n\n\nKorean Tourism\n\n\n\n\n\nIn order to expand upon globalization, we’ll also use tourism statistics inbound South Korea as a ways to measure interest in the country over time. I’ve gathered both yearly data of a count of those who visited South Korea purely for tourism as well as monthly data of international arrivals (as a number of passengers) into Incheon Airport, the largest international airport in the country.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Now, I will be conducting a Time Series EDA analysis on the globalization indexes and the record label stock prices. Due to the structure of the other data, I cannot conduct further time series EDA on those sets.\nImporting Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)"
  },
  {
    "objectID": "eda.html#globalization",
    "href": "eda.html#globalization",
    "title": "Exploratory Data Analysis",
    "section": "Globalization:",
    "text": "Globalization:\nLet’s begin by anaylzing globalization of the United States from 1970 to 2022. I will be using the general globalization index for this time series anaylsis. First I will begin by filtering the data and creating a time series object in R. This will allow us to plot the time series data for initial analysis.\n\nTime Series plot:\n\n\nCode\n# Import dataset\nglobal &lt;- read_csv('globalization.csv')\n\n# Filter information\nglobal &lt;- global %&gt;%\n  filter(country == 'United States') %&gt;%\n  select(year, KOFGI) %&gt;%\n  mutate(year = as.Date(year))\n\n# Create time series\nglobal_ts &lt;-ts(global$KOFGI, star=decimal_date(as.Date(\"1970-01-01\", format = \"%Y-%m-%d\")), frequency = 1)\n\n# Create time series plot\nglobal_plot &lt;- plot(as.ts(global_ts), main = \"Time Series of KOF Globalization Index within the United States\",\n                  xlab = 'Time', ylab = 'KOF Index')\n\n\n\n\n\nCode\n# Show plot\nggplotly(global_plot)\n\n\n\n\n\nFrom the plot of the globalization index in the United States, we can see a strong positive upward trend. In terms of seasonality and cyclic patterns, we are unable to see such patterns in the plot. Additionally, we can see very slight peaks in the data in 1986 and 2009, however, they are not enough to conclude any patterns in the data. Thus,we can say this plot is neither additive nor multiplicative.\nNext, we’ll take a look at a moving average smoothing plot to obtain some information on potential crossings.\n\n\nMoving Average Smoothing:\n\n\nSMA Code\nimport pandas as pd\nimport datetime\nimport plotly \nfrom datetime import datetime \nfrom pandas_datareader import data as pdr\nimport plotly.offline as pyo\nimport plotly.graph_objects as go \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nglobal_data = pd.read_csv('globalization.csv')\nglobal_data = global_data[global_data['country'] == 'United States']\nglobal_data['year'] = pd.to_datetime(global_data['year'], format='%Y')\n\nglobal_data['5yma'] = global_data['KOFGI'].rolling(window = 5).mean()\nglobal_data['10yma'] = global_data['KOFGI'].rolling(window = 10).mean()\nglobal_data['20yma'] = global_data['KOFGI'].rolling(window = 20).mean()\n\nfig = px.line(global_data, x='year', y=['KOFGI', '5yma', '10yma', '20yma'],\n              labels={'value': 'Yearly Data', 'variable': 'Moving Average'},\n              title='Yearly Data and Moving Averages for the Globalization Index')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\nThis plot shows us the smoothing moving average for the yearly KOF globalization index data. The three types of smoothing I chose was 5-year, 10-year, and 20-year moving averages. As we can see in the graph, all smoothing lines show a positive upward trend across the time interval (the past 50 years). There is also no crossing between the smoothing lines, possibly indicating that the data had a constant upward trend with no seasonality or cyclical trends throughout.\nNext, let’s take a look at a few lag plots of data on itself as well ACF and PACF in order to identify possible signs of stationarity.\n\n\nLag-plot:\n\n\nCode\ngglagplot(global_ts, do.lines=FALSE)+ggtitle(\"Lag Plot for the KOF Globalization Index\")\n\n\n\n\n\nWe can see in lags 1,2, and 3 a very strong positive linear relationship, meaning a positive autocorrelation in the lags. From lag 4 and onward, the trend is still strongly positive, but less linear, suggesting a weaker autocorrelation. We also don’t see any groupings in the lags, suggesting that there is no seasonality in the data.\n\n\nACF & PACF:\n\n\nCode\nggAcf(global_ts)+ggtitle(\"ACF Plot for Globalization Index\")\n\n\n\n\n\nCode\nggPacf(global_ts)+ggtitle(\"PACF Plot for Globalization Index\")\n\n\n\n\n\nLooking at the ACF and PACF plots, we get a better understanding this time series. The ACF plot shows the present lag is significantly correlated with the first 12 years, after which it become significantly uncorrelated. Additionally, the PACF shows a stationary plot, due to the PACF values being contained in the significance bands. Thus, we can say that there is in fact strong autocorrelation in this time series data, however correlation is not present within the residuals.\n\n\nDickey-Fuller Test\n\n\nCode\nglobal_test &lt;- adf.test(global_ts)\nprint(global_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  global_ts\nDickey-Fuller = 0.59021, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\nThe Dickey-Fuller Test, with tests the alternative hypothesis that the time series is stationary, concluded a p-value of 0.99. Since 0.99 &gt; 0.05, we do not have enough evidence and thus, fail to rejec the null hypothesis, meaning that the time series object is not stationary.\n\n\nStationary:\nTherefore, in order to obtain stationary data to runs an ARMA and AMRIMA model on, we will need to compare differenced and detrended data to find which approach produces stationary data.\n\n\nCode\nfit = lm(global_ts~time(global_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(global_ts, 50, main=\"Original Data: Globalization Index\")\nplot2 &lt;- ggAcf(resid(fit), 50, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(global_ts), 50, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\nFrom this plot, we can clearly see that the first differenced data results in a stationary plot, with the ACF values inside the significance bands. Since the first difference was able to coerce the data to be stationary, we can also say that the original data was linearly trended. Thus, moving forward, we will use first differencing on the globalization index in order to model this value."
  },
  {
    "objectID": "eda.html#stock-prices-looking-at-hybe-entertainment",
    "href": "eda.html#stock-prices-looking-at-hybe-entertainment",
    "title": "Exploratory Data Analysis",
    "section": "Stock Prices: Looking at HYBE Entertainment",
    "text": "Stock Prices: Looking at HYBE Entertainment\n\nTime Series plot:\nNext, since we saw, through the initial data visualization, the prevelance of Kpop and specifically BTS on the western music industry, we will take a look at HYBE stock prices through further time series EDA.\nPrimarily, we will clean our data such that missing dates corresponding to weekends and holidays where the stock market is closed will be estimated through exponential prediction. After which we will take the data and transform it into a time series object to plot.\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\nname &lt;- getSymbols('352820.KS', from = \"2019-01-01\", to = \"2023-09-01\")\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X352820.KS.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nstart_date &lt;- as.Date(\"2020-10-15\")\nend_date &lt;- as.Date(\"2023-09-01\")\n\nall_dates &lt;- data.frame(Date = seq(start_date, end_date, by = \"days\"))\n\nmerged_data &lt;- all_dates %&gt;%\n  left_join(HYBE, by = \"Date\")\n\nwrite.csv(merged_data, 'HYBE_cleaned_data.csv')\n\nimputed_time_series &lt;- na_ma(merged_data, k = 4, weighting = \"exponential\")\ndf_HYBE &lt;-data.frame(imputed_time_series)\ndf_HYBE$Date &lt;-as.Date(df_HYBE$Date,format = \"%Y-%m-%d\")\n\n# Create time series\nHYBE_ts &lt;-ts(df_HYBE$Price, star=decimal_date(as.Date(\"2020-10-15\", format = \"%Y-%m-%d\")), frequency = 365.25)\n\n# Create time series plot\nHYBE_plot &lt;- plot(as.ts(HYBE_ts), main = \"Time Series of HYBE Stock Prices\",\n                  xlab = 'Time', ylab = 'Price (USD)')\n\n\n\n\n\nUnlike the globalization index, the HYBE stock price fluctuates quite frequently in the smaller range of time. From 2021 to 2022, we can see a strong positive trend with slight seasonality. However, from 2022 onwards we see a sharp downward trend and with varying degrees of peaks. Thus, the uneven nature of the peaks and troughs results in data that is neither additive or multiplicative. Additionally, since the dataset is smaller, we cannot say anything of certain regarding cyclical patterns.\nNow, let’s look at the SMA graph for HYBE stock to identify potential crossings.\n\n\nMoving Average Smoothing:\n\n\nSMA Code\nHYBE_data = pd.read_csv('HYBE_cleaned_data.csv')\n\nHYBE_data.drop(HYBE_data.columns[0], axis=1, inplace = True)\nHYBE_data['Date'] = pd.to_datetime(HYBE_data['Date'])\nHYBE_data.dropna(axis  = 0, inplace = True)\n\nHYBE_data['3wma'] = HYBE_data['Price'].rolling(window = 15).mean()\nHYBE_data['20wma'] = HYBE_data['Price'].rolling(window = 100).mean()\nHYBE_data['50wma'] = HYBE_data['Price'].rolling(window = 250).mean()\n\nfig = px.line(HYBE_data, x='Date', y=['Price', '3wma', '20wma', '50wma'],\n              labels={'value': 'Daily Data', 'variable': 'Weekly Moving Average'},\n              title='Smoothing Moving Averages for HYBE Stock')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\nThis plot depicts the smoothing moving average of HYBE stock prices. The smoothing windows I chose was 3-week, 20-week, and 50-week on the daily data. As we can see, the 3-week smoothing line is closely correlated with the actual prices, as expected. We can also see some crossings between the 3 smoothing moving averages. Primarily, we can see a crossing in September of 2021, where the 3-week SMA briefly crosses under and over the 20-week line. The crossing over of a shorter SMA, also called a golden cross, indicates that a postive trend in prices was to be expected, which is what occured. This is most likely due to the announcement of BTS performing at the 2021 Grammy Music Awards. Additionally, we can see a significant crossing again in April of 2022. This is when the 3-week and 20-week line cross under the 50-week. When a shorter term SMA cross under the longer SMA, we can infer a drop in stock prices, also called a Death Cross. This inevitably did happen, most likely due to talk of BTS’s enlistment into the Korean military as a part of mandatory service, which was officially announced June of that year.\nNext, let’s take a look at a few lag plots of data on itself as well ACF and PACF in order to identify possible signs of stationarity.\n\n\nLag-plot:\n\n\nCode\ngglagplot(HYBE_ts, do.lines=FALSE)+\n  ggtitle(\"Lag Plot for HYBE Stock Prices\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nThese lag plots show similar results to that of the globalization index. The forst four lags have a very strong positive linear correlation, suggesting autocorrelation amongst those lags. From lag 5 onwards, we still see a string linear correlation, however we can also see a small circular pattern forming in the lag plots, suggesting a possibility of single-cycle sinusodial data.\n\n\nACF & PACF:\n\n\nCode\nggAcf(HYBE_ts, lag.max = 30)+ggtitle(\"ACF Plot for HYBE Stock Prices\")\n\n\n\n\n\nCode\nggPacf(HYBE_ts, lag.max = 30)+ggtitle(\"PACF Plot for HYBE Stock Prices\")\n\n\n\n\n\nSimilar to the globalization index, the ACF plot shows the present lag is significantly correlated with all other present lags in the plot, since all values are well above the siginificance bands. Additionally, the PACF shows a stationary plot, due to the PACF values being contained in the significance bands. Thus, we can say that there is in fact strong autocorrelation in this time series data, however correlation is not present within the residuals.\n\n\nDickey-Fuller Test:\n\n\nCode\nHYBE_test &lt;- adf.test(HYBE_ts)\nprint(HYBE_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  HYBE_ts\nDickey-Fuller = -2.0109, Lag order = 10, p-value = 0.5737\nalternative hypothesis: stationary\n\n\nThe Dickey-Fuller Test resulted in a p-value of 0.5737. Since 0.5737 &gt; 0.05, we can fail to reject the null hypothesis and say that the time series object is not stationary.\n\n\nStationary:\nGiven that the original data is not stationary through the Dickey-Fuller test, we will use the detrending and differencing methods to coerse the data to become stationary.\n\n\nCode\nfit = lm(HYBE_ts~time(HYBE_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(HYBE_ts, lag.max = 30, main=\"Original Data: HYBE Stock Prices\")\nplot2 &lt;- ggAcf(resid(fit), lag.max = 30, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(HYBE_ts), lag.max = 30, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3, ncol=3)\n\n\n\n\n\nHowever, after trying both detrending and first difference methods, both result in ACF plots showing autocorrelation and non-stationary tendencies. Thus, we will try the second differencing approach.\n\n\nCode\nplot4 &lt;- ggAcf(diff(diff(HYBE_ts)), lag.max = 30, main=\"Second Differenced Data\")\nplot4\n\n\n\n\n\nWith the second difference, we were able to get the HYBE stock prices to become stationary. Therefore, we could also suggest the original data has quadratic trending behavior.\nThus, going forward, we can use the second difference of the HYBE stock prices for modeling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5600: Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data-vizes.html",
    "href": "data-vizes.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Problem 1:\nThis plot depicts the stock prices for large fashion companies pre and post the COVID-19 pandemic. The fashion brands included are Adidas, Christian Dior, H&M, Nike, Puma, and Ross. Looking a combination of high end and low end brands, we can see that all companies faced a massive downtick in price evaluations in March of 2020, signaling the announcement of the global COVID-19 pandemic. Something to point out is that Christian Dior had a spike in price around mid-2021, resulting in a high uptick compared to the other brands. I was unable to find a reason as to why this uptick occured, but we can assume it was due to the highend nature of the brand.\n\n\n\n\n\n\n\nProblem 2:\nThe following graph depicts the precipication in Washington D.C. from January 2021 - January 2022. The graph shows periodic spikes throughout the year, however, the most precipitation occured in the third quarter of the year, with the highest amount occuring on August 20, 2021 at 3.76. The driest part of the year was the last quarter.\n\n\n\n\n\n\n\n\nProblem 3:\nBelow, is a graph depicting the unemployement rate for women in the United States of America throughout the 2000’s. This data was sourced from FRED. From this graph we can see 2 major historical moments that caused women’s unemployement. The 2008 market crash caused the first steep incline in unemployment. However, the market was able to recover such that the women who did work were able to in the 2010’s. The next major event was the COVID-19 global pandemic. Due to a global shutdown, there was a major spike in unemployement. This may be due to several women also being mothers and needing to stay home for childcare as a result of childcare services being unavailable at the time.\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Music as a representation of culture has been defined since the birth of human connection and communities for centuries. In the past few decades, we have seen a transformation in music listening and sharing, from physical records to digital purchases to modern day streaming. Specifically in the United States (U.S.), popular music has, for the most part, been contained to English speaking, American artists. In recent years, however, the rise of KPOP, through groups like BTS, becoming increasingly popular in Western markets, to the point where it is considered part of the Pop genre Wang. With this new found accessibility of KPOP music through streaming, there comes the question of how music listening and cultural exchange has changed in the U.S.\nWe can define such a question as the effect of KPOP on the Western Music Industry and the impacts of this on globalization as a whole. At the moment, there is literature to discuss both viewpoints of music globalization; the first being that streaming has expanded the average U.S. consumer’s music palette from traditional American music, while the second suggests that streaming has only narrowed the average listener’s taste through algorithmic suggestions and playlists Bello and Garcia (2021). Cultural Divergence in popular music suggests that while clustering in music tastes can be noted for specific regions of the world, we do see an “upward trend in music consumption diversity that started in 2017 and spans across platforms” Bello and Garcia (2021).\nHowever, in order to fully realize the question of KPOP’s effect on the western music industry through time, I propose analyzing the following: music charting on Billboard and iTunes, stock prices for music record companies, live music statistics and sales data, and quantifying globalization over time.\n\nThese four analytical venues will provide an in-depth analysis to further understand whether the trend of globalization of cultures has in fact translated to music listening and whether the average American has positive sentiment to this overall trend.\n\n\n\nWhen do we see a shift where the KPOP industry makes an impact on the western music industry?\nHow does music charting influence music created today?\nTo what extent has KPOP in the United States led to a positive or negative sentiment among the general population?\nHas KPOP changed the overall trends of popular music in America (i.e musical elements, danceability, etc.)?\nWhat combined conclusions can be drawn from the four analytical venues demonstrated in the big-ideas chart?\nGiven the impact of the KPOP concerts within the West, can a time-series analysis forecast future sales from KPOP artists?\nWhat are some long term trends in music stock prices?\nHow has the availability of KPOP music changed over time and how does that translate to globalization?\nAre there any cyclical trends present in popular genres within the United States in the past 10 years?\nIs the globalization of music in the U.S. simply a fad of our time or a product of technical advancements in music dispersion?"
  },
  {
    "objectID": "introduction.html#thought-provoking-questions",
    "href": "introduction.html#thought-provoking-questions",
    "title": "Introduction",
    "section": "",
    "text": "When do we see a shift where the KPOP industry makes an impact on the western music industry?\nHow does music charting influence music created today?\nTo what extent has KPOP in the United States led to a positive or negative sentiment among the general population?\nHas KPOP changed the overall trends of popular music in America (i.e musical elements, danceability, etc.)?\nWhat combined conclusions can be drawn from the four analytical venues demonstrated in the big-ideas chart?\nGiven the impact of the KPOP concerts within the West, can a time-series analysis forecast future sales from KPOP artists?\nWhat are some long term trends in music stock prices?\nHow has the availability of KPOP music changed over time and how does that translate to globalization?\nAre there any cyclical trends present in popular genres within the United States in the past 10 years?\nIs the globalization of music in the U.S. simply a fad of our time or a product of technical advancements in music dispersion?"
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The purpose of data visualization is to find possible trends, patterns, and anomalies within the datasets. Primarily, let’s take a look at globalization as a general topic through the lense of the KOF Globalization Index."
  },
  {
    "objectID": "data-visualization.html#kof-globalization",
    "href": "data-visualization.html#kof-globalization",
    "title": "Data Visualization",
    "section": "KOF Globalization",
    "text": "KOF Globalization\nThe KOF Globalization index quantifies globalization within a country through a multitude of lenses. We will be looking at the general globalization index values as well as technology, culture, and TV and Media, since it all correlates to sphere of music and music streaming in the modern world.\nThrough R, I cleaning the dataset from KOF and used Tableau to create interative global maps for each respective index system.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal &lt;- read_excel(\"KOFGI_2022_public.xlsx\")\nglobal &lt;- global %&gt;%\n  select(code, country, year, KOFGI, KOFInGIdf, KOFInGIdj, KOFCuGIdf) %&gt;%\n  drop_na(c(KOFGI, KOFInGIdf, KOFInGIdj, KOFCuGIdf))\n\nwrite.csv(global, \"globalization.csv\", row.names=FALSE)\n\n\nFrom this cleaned data, we can visualize the globalization index over time for general globalization as well as indexes specifc to culture and entertainment.\nPlease allow for the visualization to load when moving the slider as it handles a large amount of data\n\n\n\n\n\n\n   \n\n\n\n\nFrom the graphs, we can a gradual increase in the globalization index over the past 50 years. Specifically, North America, Western Europe, and Australia have the highest globalization indexes across all index types throughout the 50 years. Additionally, we can see that in 1990, Russia’s globalization index was added, resulting in the world’s largest populous countries to be added to the index. Once we get to 2000, we continue to see the highest globalization values in North America, Western Europe (specifically UK and France), and Australia, with new additions like South Korea and Japan with higher values. By 2020, the aforementioned countries have even higher index values, most in the 80’s and 90’s out of 100. China, Russia, Brazil, and India are also most notable countries with very high globalization values, especially in the technology category."
  },
  {
    "objectID": "data-visualization.html#record-label-stocks",
    "href": "data-visualization.html#record-label-stocks",
    "title": "Data Visualization",
    "section": "Record Label Stocks:",
    "text": "Record Label Stocks:\nNext, in order to better understand globalization within the music industry, we will take a look at large record label stock prices over time. I chose the most popular record labels who house some of the largest musicians in the industry.\n\nUniversal Music Group Inc. (UMGP)\nWarner Music Group Corp. (WMG)\nHYBE Co. (352820.KS)\nSM Entertainment (041510.KQ)\nYG Entertainment (122870.KQ)\nJYP Entertainment (035900.KQ)\n\nI used plotly in R to show this interactive line plot.\n\n\nCode\nlibrary(plotly)\nlibrary(quantmod)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"WMG\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = UMGP.Adjusted)\n\nWMG &lt;- data.frame(WMG$WMG.Adjusted)\nWMG &lt;- WMG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = WMG.Adjusted)\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X352820.KS.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nstock_dataframes &lt;- list(UMGP, WMG, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"WMG\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\np &lt;- plot_ly()\n\n# Add traces for each stock to the plot\nfor (i in 1:length(stock_dataframes)) {\n  stock_df &lt;- stock_dataframes[[i]]\n  p &lt;- add_trace(p, x = stock_df$Date, y = stock_df$Price, mode = 'lines', name = stock_names[i])\n}\n\n# Customize the layout if needed\np &lt;- layout(p, title = \"Stock Prices of Music Record Label\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Price (USD)\"))\n\n# Show the plot\np\n\n\n\n\n\n\nFrom the line plot, we can analyze these record labels as they enter the market to today. Starting with the oldest public record label, SONY seems to have a seasonal downward trend from 2000 - 2001, however that quickly ends to a more irregular and constant movement. We can see a large spike again in the early 2022, possibly due to SONY’s press release for the vison-s suv, a self driving car Lawler (2022). Of the other stocks, a notable one to note is the introduction of HYBE Co. in late 2020. HYBE, a South Korean Entertainment company, founded by Bang Shi Hyuk, is notable for its creation of the current largest boy band in the industy, BTS. With their international Billboard Hot 100 #1 song, Dynamite, releasing in August of 2020, the band a stirred enough interest for the company’s entrance in the market to be a success at $188 USD. The price reached a high in November of 2021 at $306 USD, most likely due to BTS’s Artist of the Year award at the 2021 American Music Awards and their Grammy nomination. However, since that peak, the price has gone down significantly in June of 2022, likely a result of the company’s largest artists, BTS, announcing a hiatus of group activities due to military enlistment “HYBE Shares Rise as World Contemplates Company Sans BTS” (2022). Amonst the remaining tickers, we can a gradual positive trend with no seasonality and a slight spike around early 2022, as artists were announcing in person activities after the Covid-19 pandemic. Thus, we can see through these stock prices that while American record companies seem to a large and consistent history in the market, new companies like HYBE and SM from South Korea have started to take space in the public markets."
  },
  {
    "objectID": "data-visualization.html#billboard-charts",
    "href": "data-visualization.html#billboard-charts",
    "title": "Data Visualization",
    "section": "Billboard Charts:",
    "text": "Billboard Charts:\nThe Billboard Hot 100 is a chart created by billboard used to rank the top 100 songs in the United States per week. In order to see the trends in musical globalization, I will be taking a look artists who acheived number 1’s on the Billboard Hot 100 and the number of weeks those songs stayed on the chart from 2010 - 2021. I used Tableau to show this interactive bubble graph.\n\n\nCode\ncharts &lt;- read_csv('charts.csv')\ncharts &lt;- charts %&gt;%\n  filter(rank == 1) %&gt;%\n  select(-`last-week`, -`peak-rank`)\n\nwrite.csv(charts, \"number1_charts.csv\", row.names=FALSE)\n\n\n\n\n\n\n\n\n    \n\n\n\n\nFrom this visualization we notice that first instance of a non-Western artist debuting at number 1 on the Billboard Hot 100 is in 2017 with Luis Fonsi, Daddy Yankee, and Justin Bieber with Despacito. The song stayed on the charts for 78 weeks, being song with the most weeks on the Billboard Hot 100 in this timeframe. The next non-Western artist would be BTS, as mentioned previously, with their song Dynamite, first debuting number one in September of 2020. BTS appears again in June 2021, with their song Butter which stayed on the charts for 15 weeks. Latin Artists Bad Bunny and J Balvin also made a debut on the charts at number 1 with the song I Like It along side Cardi B for a total of 12 weeks on the charts. Thus, we can see that after 2015, we saw an increaing number of number 1’s from non-Western artists on the America charts, a sign of general positive feedback from the public regarding the globalization of music."
  },
  {
    "objectID": "data-visualization.html#spotify-data",
    "href": "data-visualization.html#spotify-data",
    "title": "Data Visualization",
    "section": "Spotify Data:",
    "text": "Spotify Data:\nLastly, we will take another look at Spotify Data in order to analyze music trends from popular Non-Western artists and popular Western artists. This small example shows energy over time between artists BTS, Taylor Swift, and Daddy Yankee.\n\n\nCode\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(spotifyr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(plotly)\n\nSys.setenv(SPOTIFY_CLIENT_ID = \"48875e31f589401f83c6bd43005d94f7\")\nSys.setenv(SPOTIFY_CLIENT_SECRET = \"d215e4ea690d4b9b9c1c5e0afbb113a5\")\n\naccess_token &lt;- get_spotify_access_token()\nartists &lt;- c(\"BTS\", \"Taylor Swift\", \"Daddy Yankee\")\n\nBTS &lt;- get_artist_audio_features(\"BTS\")\nBTS_A &lt;- data.frame(BTS$artist_name,\nBTS$instrumentalness,\nBTS$valence,\nBTS$danceability,\nBTS$energy,\nBTS$loudness,\nBTS$speechiness,\nBTS$acousticness,\nBTS$liveness,\nBTS$tempo,\nBTS$track_name,\nBTS$album_name,\nBTS$album_release_year,\nBTS$album_release_date)\n\ncolnames(BTS_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\nTaylor_Swift &lt;- get_artist_audio_features(\"Taylor Swift\")\nTaylor_Swift_A &lt;- data.frame(Taylor_Swift$artist_name,\nTaylor_Swift$instrumentalness,\nTaylor_Swift$valence,\nTaylor_Swift$danceability,\nTaylor_Swift$energy,\nTaylor_Swift$loudness,\nTaylor_Swift$speechiness,\nTaylor_Swift$acousticness,\nTaylor_Swift$liveness,\nTaylor_Swift$tempo,\nTaylor_Swift$track_name,\nTaylor_Swift$album_name,\nTaylor_Swift$album_release_year,\nTaylor_Swift$album_release_date)\n\ncolnames(Taylor_Swift_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\n\nDaddy_Yankee &lt;- get_artist_audio_features(\"Daddy Yankee\")\nDaddy_Yankee_A &lt;- data.frame(Daddy_Yankee$artist_name,\nDaddy_Yankee$instrumentalness,\nDaddy_Yankee$valence,\nDaddy_Yankee$danceability,\nDaddy_Yankee$energy,\nDaddy_Yankee$loudness, \nDaddy_Yankee$speechiness,\nDaddy_Yankee$acousticness,\nDaddy_Yankee$liveness,\nDaddy_Yankee$tempo,\nDaddy_Yankee$track_name,\nDaddy_Yankee$album_name,\nDaddy_Yankee$album_release_year,\nDaddy_Yankee$album_release_date)\n\ncolnames(Daddy_Yankee_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n\"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n\"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n\nartists &lt;- rbind(BTS_A, Taylor_Swift_A, Daddy_Yankee_A)\nartists$date &lt;- as.Date(artists$date, format = \"%Y-%m-%d\")\n\nfig &lt;- plot_ly(artists, x = ~date, y = ~energy, color = ~artist_name, \n               type = 'scatter', mode = 'markers', size = ~speechiness) \n\nfig &lt;- fig %&gt;% layout(xaxis = list(title = \"Album Released Date\"),\n                      yaxis = list(title =\"Energy\"), \n                      title = \"Energy - Taylor Swift/BTS/Daddy Yankee\")\n\nfig\n\n\n\n\n\n\nSimilar to the visualization in Data Sources, we can see that with both BTS and Daddy Yankee, energy is consistently very high throughout their careers, with fewer spread in comparison to Taylor Swift. This can indicate that, especially as musical globalization begins to shape the American music industry, we can see an influx in high energy songs, similar to successful non-Western acts like BTS and Daddy Yankee, across genres as shown with the juxtaposition of Taylor Swift, who has a larger spread of energy throughout her discography."
  },
  {
    "objectID": "arimax.html#the-kpop-record-labels-var",
    "href": "arimax.html#the-kpop-record-labels-var",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "(1) The KPOP Record Labels (VAR):",
    "text": "(1) The KPOP Record Labels (VAR):\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"WMG\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nWMG &lt;- data.frame(WMG$WMG.Adjusted)\nWMG &lt;- WMG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(WMG_Price = WMG.Adjusted)\n\n\nstart_date &lt;- as.Date(min(WMG$Date))  \nend_date &lt;- as.Date(max(WMG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nWMG &lt;- merge(WMG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- WMG[which(rowSums(is.na(WMG)) &gt; 0),]\ndf_na_cols &lt;- WMG[, which(colSums(is.na(WMG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(WMG, k = 4, weighting = \"exponential\")\nWMG &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, WMG, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"WMG\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n\n\nConverting to Time Series\n\n\nCode\nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n\n\n\nVisualizing the data:\n\n\nCode\nautoplot(df_ts)\n\n\n\n\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let’s continue with the VAR model to see what the multivariate relationship is.\n\n\nVARselect\n\n\nCode\nVARselect(df_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  3.142389  3.114099  3.117499  3.112310  3.088174  3.092878  3.107702\nHQ(n)   3.185625  3.186159  3.218383  3.242018  3.246706  3.280234  3.323882\nSC(n)   3.256375  3.304076  3.383466  3.454268  3.506123  3.586817  3.677632\nFPE(n) 23.159131 22.513187 22.589948 22.473198 21.937533 22.041324 22.370993\n               8         9        10\nAIC(n)  3.111249  3.118763  3.131486\nHQ(n)   3.356254  3.392591  3.434139\nSC(n)   3.757171  3.840675  3.929389\nFPE(n) 22.451153 22.621304 22.912010\n\n\nWe can see that the p-values detected from VARselect() are 5 and 1.\n\n\nInitial selection:\n\n\nCode\nsummary(vars::VAR(df_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1051 \nLog Likelihood: -7636.31 \nRoots of the characteristic polynomial:\n0.9954 0.9902 0.9811 0.9811\nCall:\nvars::VAR(y = df_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9908097  0.0048019 206.337   &lt;2e-16 ***\nsm.l1    0.0022993  0.0195271   0.118    0.906    \nyg.l1    0.0622194  0.0400453   1.554    0.121    \njyp.l1  -0.0302953  0.0197591  -1.533    0.126    \nconst   -0.0663658  0.8191955  -0.081    0.935    \ntrend    0.0005578  0.0014217   0.392    0.695    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.582 on 1045 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9909 \nF-statistic: 2.277e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0036634  0.0017083   2.145   0.0322 *  \nsm.l1    0.9801908  0.0069467 141.101   &lt;2e-16 ***\nyg.l1   -0.0231121  0.0142460  -1.622   0.1050    \njyp.l1   0.0110615  0.0070293   1.574   0.1159    \nconst    0.3284611  0.2914272   1.127   0.2600    \ntrend    0.0011339  0.0005058   2.242   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.63 on 1045 degrees of freedom\nMultiple R-Squared: 0.994,  Adjusted R-squared: 0.994 \nF-statistic: 3.491e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005472  0.0010454   0.523   0.6008    \nsm.l1    0.0025344  0.0042514   0.596   0.5512    \nyg.l1    0.9868482  0.0087185 113.190   &lt;2e-16 ***\njyp.l1  -0.0007041  0.0043019  -0.164   0.8700    \nconst    0.2949613  0.1783516   1.654   0.0985 .  \ntrend    0.0001396  0.0003095   0.451   0.6520    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9976 on 1045 degrees of freedom\nMultiple R-Squared: 0.986,  Adjusted R-squared: 0.9859 \nF-statistic: 1.471e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005536  0.0011980   0.462 0.644124    \nsm.l1   -0.0135935  0.0048718  -2.790 0.005363 ** \nyg.l1    0.0167605  0.0099909   1.678 0.093729 .  \njyp.l1   0.9896949  0.0049297 200.762  &lt; 2e-16 ***\nconst   -0.2443391  0.2043815  -1.196 0.232161    \ntrend    0.0012519  0.0003547   3.530 0.000434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.143 on 1045 degrees of freedom\nMultiple R-Squared: 0.9966, Adjusted R-squared: 0.9966 \nF-statistic: 6.159e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 20.996 2.1862 2.1450 1.8659\nsm    2.186 2.6572 0.7519 0.9340\nyg    2.145 0.7519 0.9952 0.7025\njyp   1.866 0.9340 0.7025 1.3069\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.2927 0.4692 0.3562\nsm   0.2927 1.0000 0.4624 0.5012\nyg   0.4692 0.4624 1.0000 0.6160\njyp  0.3562 0.5012 0.6160 1.0000\n\n\nCode\nsummary(vars::VAR(df_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1047 \nLog Likelihood: -7463.522 \nRoots of the characteristic polynomial:\n0.9936 0.9907 0.9741 0.9741 0.6725 0.6725 0.6258 0.6258 0.5949 0.5705 0.5705 0.5578 0.5578 0.553 0.4157 0.4157 0.362 0.362 0.2872 0.09888\nCall:\nvars::VAR(y = df_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8597953  0.0355589  24.179  &lt; 2e-16 ***\nsm.l1   -0.0845122  0.0985933  -0.857  0.39155    \nyg.l1    0.1053591  0.1874006   0.562  0.57409    \njyp.l1   0.0478898  0.1612418   0.297  0.76652    \nhybe.l2  0.1418576  0.0470883   3.013  0.00265 ** \nsm.l2    0.0710565  0.1387798   0.512  0.60876    \nyg.l2   -0.0204218  0.2498248  -0.082  0.93487    \njyp.l2   0.0673012  0.2132995   0.316  0.75243    \nhybe.l3  0.1008928  0.0474237   2.127  0.03362 *  \nsm.l3    0.1305088  0.1397295   0.934  0.35052    \nyg.l3   -0.1482943  0.2519800  -0.589  0.55632    \njyp.l3  -0.2200135  0.2140360  -1.028  0.30423    \nhybe.l4  0.0367459  0.0459422   0.800  0.42399    \nsm.l4    0.0086685  0.1398948   0.062  0.95060    \nyg.l4   -0.2691600  0.2509303  -1.073  0.28368    \njyp.l4   0.1964530  0.2111857   0.930  0.35247    \nhybe.l5 -0.1487634  0.0337524  -4.407 1.16e-05 ***\nsm.l5   -0.1202141  0.0998846  -1.204  0.22905    \nyg.l5    0.3805559  0.1889051   2.015  0.04421 *  \njyp.l5  -0.1128827  0.1602207  -0.705  0.48125    \nconst    0.4715235  0.7981067   0.591  0.55478    \ntrend   -0.0002577  0.0013922  -0.185  0.85317    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.319 on 1025 degrees of freedom\nMultiple R-Squared: 0.9921, Adjusted R-squared: 0.9919 \nF-statistic:  6101 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0114542  0.0134067   0.854  0.39310    \nsm.l1    0.9790130  0.0371724  26.337  &lt; 2e-16 ***\nyg.l1   -0.1442679  0.0706553  -2.042  0.04142 *  \njyp.l1  -0.0175303  0.0607927  -0.288  0.77313    \nhybe.l2 -0.0084490  0.0177536  -0.476  0.63425    \nsm.l2    0.0736360  0.0523239   1.407  0.15964    \nyg.l2    0.1218543  0.0941910   1.294  0.19606    \njyp.l2  -0.0382295  0.0804199  -0.475  0.63462    \nhybe.l3 -0.0002227  0.0178801  -0.012  0.99006    \nsm.l3    0.0244439  0.0526820   0.464  0.64275    \nyg.l3    0.0541715  0.0950036   0.570  0.56866    \njyp.l3  -0.0345022  0.0806976  -0.428  0.66907    \nhybe.l4  0.0101214  0.0173215   0.584  0.55913    \nsm.l4   -0.1002341  0.0527443  -1.900  0.05766 .  \nyg.l4   -0.0253390  0.0946078  -0.268  0.78888    \njyp.l4   0.1016257  0.0796230   1.276  0.20213    \nhybe.l5 -0.0094405  0.0127256  -0.742  0.45835    \nsm.l5   -0.0011058  0.0376593  -0.029  0.97658    \nyg.l5   -0.0209738  0.0712226  -0.294  0.76845    \njyp.l5  -0.0024879  0.0604077  -0.041  0.96716    \nconst    0.1965731  0.3009087   0.653  0.51373    \ntrend    0.0013967  0.0005249   2.661  0.00791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.628 on 1025 degrees of freedom\nMultiple R-Squared: 0.9941, Adjusted R-squared: 0.994 \nF-statistic:  8268 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -1.266e-03  8.096e-03  -0.156 0.875731    \nsm.l1   -4.700e-03  2.245e-02  -0.209 0.834207    \nyg.l1    8.482e-01  4.267e-02  19.881  &lt; 2e-16 ***\njyp.l1   8.270e-03  3.671e-02   0.225 0.821811    \nhybe.l2  2.944e-03  1.072e-02   0.275 0.783682    \nsm.l2   -2.157e-02  3.160e-02  -0.683 0.494890    \nyg.l2    2.194e-01  5.688e-02   3.857 0.000122 ***\njyp.l2  -4.823e-03  4.856e-02  -0.099 0.920899    \nhybe.l3  2.151e-03  1.080e-02   0.199 0.842116    \nsm.l3    4.958e-02  3.181e-02   1.558 0.119453    \nyg.l3    1.158e-03  5.737e-02   0.020 0.983902    \njyp.l3   6.828e-03  4.873e-02   0.140 0.888594    \nhybe.l4  2.441e-03  1.046e-02   0.233 0.815503    \nsm.l4   -3.504e-02  3.185e-02  -1.100 0.271569    \nyg.l4    2.502e-03  5.713e-02   0.044 0.965078    \njyp.l4   8.681e-03  4.808e-02   0.181 0.856762    \nhybe.l5 -5.463e-03  7.684e-03  -0.711 0.477337    \nsm.l5    1.507e-02  2.274e-02   0.662 0.507801    \nyg.l5   -9.023e-02  4.301e-02  -2.098 0.036153 *  \njyp.l5  -1.735e-02  3.648e-02  -0.476 0.634446    \nconst    3.983e-01  1.817e-01   2.192 0.028616 *  \ntrend    4.017e-05  3.170e-04   0.127 0.899178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9834 on 1025 degrees of freedom\nMultiple R-Squared: 0.9866, Adjusted R-squared: 0.9863 \nF-statistic:  3591 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0006532  0.0091949   0.071  0.94338    \nsm.l1   -0.0145845  0.0254945  -0.572  0.56740    \nyg.l1   -0.0662954  0.0484585  -1.368  0.17158    \njyp.l1   0.8855542  0.0416943  21.239  &lt; 2e-16 ***\nhybe.l2 -0.0014774  0.0121762  -0.121  0.90345    \nsm.l2   -0.0432859  0.0358860  -1.206  0.22802    \nyg.l2    0.1533706  0.0646002   2.374  0.01777 *  \njyp.l2   0.1381693  0.0551555   2.505  0.01240 *  \nhybe.l3  0.0088203  0.0122629   0.719  0.47214    \nsm.l3    0.0991813  0.0361316   2.745  0.00616 ** \nyg.l3   -0.1022738  0.0651575  -1.570  0.11681    \njyp.l3  -0.0491761  0.0553459  -0.889  0.37447    \nhybe.l4 -0.0071445  0.0118798  -0.601  0.54771    \nsm.l4   -0.1109944  0.0361743  -3.068  0.00221 ** \nyg.l4    0.1607349  0.0648861   2.477  0.01340 *  \njyp.l4   0.0908549  0.0546088   1.664  0.09647 .  \nhybe.l5 -0.0001754  0.0087278  -0.020  0.98397    \nsm.l5    0.0571925  0.0258284   2.214  0.02703 *  \nyg.l5   -0.1325737  0.0488475  -2.714  0.00676 ** \njyp.l5  -0.0738799  0.0414302  -1.783  0.07484 .  \nconst   -0.1891913  0.2063761  -0.917  0.35950    \ntrend    0.0011485  0.0003600   3.190  0.00146 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.117 on 1025 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9968 \nF-statistic: 1.531e+04 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 18.656 2.1148 2.0584 1.8330\nsm    2.115 2.6519 0.7396 0.9206\nyg    2.058 0.7396 0.9670 0.6693\njyp   1.833 0.9206 0.6693 1.2474\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.3007 0.4846 0.3800\nsm   0.3007 1.0000 0.4619 0.5061\nyg   0.4846 0.4619 1.0000 0.6094\njyp  0.3800 0.5061 0.6094 1.0000\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don’t see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM.\nThus, before we continue with the model, we will also verify through a CV test.\n\n\nCross Validation:\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df_ts[-(start:end), ]\n  test_model &lt;- df_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n\n[1] \"The best model is: \"\n\n\nCode\nprint(best_model)\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n 0.898826902 -0.129257139  0.240081729 -0.195873591  0.086817669  0.112162459 \n       yg.l2       jyp.l2        const        trend \n-0.145841108  0.163410772  0.119069039  0.001963864 \n\n\nEstimated coefficients for equation sm: \n======================================= \nCall:\nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003282289  0.979354101 -0.124633554 -0.030561719  0.006619618 -0.001280822 \n       yg.l2       jyp.l2        const        trend \n 0.099485625  0.041169059  0.424147339  0.001889801 \n\n\nEstimated coefficients for equation yg: \n======================================= \nCall:\nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n      hybe.l1         sm.l1         yg.l1        jyp.l1       hybe.l2 \n-0.0017546456 -0.0129013663  0.8872940815 -0.0132784925  0.0020911541 \n        sm.l2         yg.l2        jyp.l2         const         trend \n 0.0157819564  0.1016893422  0.0123618307  0.2539521440  0.0001621699 \n\n\nEstimated coefficients for equation jyp: \n======================================== \nCall:\njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003714104 -0.013862592 -0.042708139  0.845587797  0.003228421 -0.002809650 \n       yg.l2       jyp.l2        const        trend \n 0.065350796  0.141050674 -0.223955478  0.002160854 \n\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2.\n\n\nModel Creation:\n\n\nCode\nvar_model_1 &lt;- VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 271.32, df = 160, p-value = 9.23e-08\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 154.19, df = 112, p-value = 0.005085\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies.\n\n\nForecasting:\n\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction."
  },
  {
    "objectID": "arimax.html#the-kpop-record-labels---var",
    "href": "arimax.html#the-kpop-record-labels---var",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(1) The KPOP Record Labels - VAR:",
    "text": "(1) The KPOP Record Labels - VAR:\nFirstly, let’s gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed.\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"WMG\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nWMG &lt;- data.frame(WMG$WMG.Adjusted)\nWMG &lt;- WMG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(WMG_Price = WMG.Adjusted)\n\n\nstart_date &lt;- as.Date(min(WMG$Date))  \nend_date &lt;- as.Date(max(WMG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nWMG &lt;- merge(WMG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- WMG[which(rowSums(is.na(WMG)) &gt; 0),]\ndf_na_cols &lt;- WMG[, which(colSums(is.na(WMG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(WMG, k = 4, weighting = \"exponential\")\nWMG &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, WMG, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"WMG\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n\n\nConverting to Time Series\nNext, we’ll take all the KPOP entertainment companies’ stock prices and convert them into time series objects.\n\n\nCode\nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n\n\n\nVisualizing the data:\n\n\nCode\nautoplot(df_ts)\n\n\n\n\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let’s continue with the VAR model to see what the multivariate relationship is.\n\n\nVARselect\n\n\nCode\nVARselect(df_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  3.142389  3.114099  3.117499  3.112310  3.088174  3.092878  3.107702\nHQ(n)   3.185625  3.186159  3.218383  3.242018  3.246706  3.280234  3.323882\nSC(n)   3.256375  3.304076  3.383466  3.454268  3.506123  3.586817  3.677632\nFPE(n) 23.159131 22.513187 22.589948 22.473198 21.937533 22.041324 22.370993\n               8         9        10\nAIC(n)  3.111249  3.118763  3.131486\nHQ(n)   3.356254  3.392591  3.434139\nSC(n)   3.757171  3.840675  3.929389\nFPE(n) 22.451153 22.621304 22.912010\n\n\nWe can see that the p-values detected from VARselect() are 5 and 1.\n\n\nInitial selection:\n\n\nCode\nsummary(vars::VAR(df_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1051 \nLog Likelihood: -7636.31 \nRoots of the characteristic polynomial:\n0.9954 0.9902 0.9811 0.9811\nCall:\nvars::VAR(y = df_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9908097  0.0048019 206.337   &lt;2e-16 ***\nsm.l1    0.0022993  0.0195271   0.118    0.906    \nyg.l1    0.0622194  0.0400453   1.554    0.121    \njyp.l1  -0.0302953  0.0197591  -1.533    0.126    \nconst   -0.0663658  0.8191955  -0.081    0.935    \ntrend    0.0005578  0.0014217   0.392    0.695    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.582 on 1045 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9909 \nF-statistic: 2.277e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0036634  0.0017083   2.145   0.0322 *  \nsm.l1    0.9801908  0.0069467 141.101   &lt;2e-16 ***\nyg.l1   -0.0231121  0.0142460  -1.622   0.1050    \njyp.l1   0.0110615  0.0070293   1.574   0.1159    \nconst    0.3284611  0.2914272   1.127   0.2600    \ntrend    0.0011339  0.0005058   2.242   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.63 on 1045 degrees of freedom\nMultiple R-Squared: 0.994,  Adjusted R-squared: 0.994 \nF-statistic: 3.491e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005472  0.0010454   0.523   0.6008    \nsm.l1    0.0025344  0.0042514   0.596   0.5512    \nyg.l1    0.9868482  0.0087185 113.190   &lt;2e-16 ***\njyp.l1  -0.0007041  0.0043019  -0.164   0.8700    \nconst    0.2949613  0.1783516   1.654   0.0985 .  \ntrend    0.0001396  0.0003095   0.451   0.6520    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9976 on 1045 degrees of freedom\nMultiple R-Squared: 0.986,  Adjusted R-squared: 0.9859 \nF-statistic: 1.471e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005536  0.0011980   0.462 0.644124    \nsm.l1   -0.0135935  0.0048718  -2.790 0.005363 ** \nyg.l1    0.0167605  0.0099909   1.678 0.093729 .  \njyp.l1   0.9896949  0.0049297 200.762  &lt; 2e-16 ***\nconst   -0.2443391  0.2043815  -1.196 0.232161    \ntrend    0.0012519  0.0003547   3.530 0.000434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.143 on 1045 degrees of freedom\nMultiple R-Squared: 0.9966, Adjusted R-squared: 0.9966 \nF-statistic: 6.159e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 20.996 2.1862 2.1450 1.8659\nsm    2.186 2.6572 0.7519 0.9340\nyg    2.145 0.7519 0.9952 0.7025\njyp   1.866 0.9340 0.7025 1.3069\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.2927 0.4692 0.3562\nsm   0.2927 1.0000 0.4624 0.5012\nyg   0.4692 0.4624 1.0000 0.6160\njyp  0.3562 0.5012 0.6160 1.0000\n\n\nCode\nsummary(vars::VAR(df_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1047 \nLog Likelihood: -7463.522 \nRoots of the characteristic polynomial:\n0.9936 0.9907 0.9741 0.9741 0.6725 0.6725 0.6258 0.6258 0.5949 0.5705 0.5705 0.5578 0.5578 0.553 0.4157 0.4157 0.362 0.362 0.2872 0.09888\nCall:\nvars::VAR(y = df_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8597953  0.0355589  24.179  &lt; 2e-16 ***\nsm.l1   -0.0845122  0.0985933  -0.857  0.39155    \nyg.l1    0.1053591  0.1874006   0.562  0.57409    \njyp.l1   0.0478898  0.1612418   0.297  0.76652    \nhybe.l2  0.1418576  0.0470883   3.013  0.00265 ** \nsm.l2    0.0710565  0.1387798   0.512  0.60876    \nyg.l2   -0.0204218  0.2498248  -0.082  0.93487    \njyp.l2   0.0673012  0.2132995   0.316  0.75243    \nhybe.l3  0.1008928  0.0474237   2.127  0.03362 *  \nsm.l3    0.1305088  0.1397295   0.934  0.35052    \nyg.l3   -0.1482943  0.2519800  -0.589  0.55632    \njyp.l3  -0.2200135  0.2140360  -1.028  0.30423    \nhybe.l4  0.0367459  0.0459422   0.800  0.42399    \nsm.l4    0.0086685  0.1398948   0.062  0.95060    \nyg.l4   -0.2691600  0.2509303  -1.073  0.28368    \njyp.l4   0.1964530  0.2111857   0.930  0.35247    \nhybe.l5 -0.1487634  0.0337524  -4.407 1.16e-05 ***\nsm.l5   -0.1202141  0.0998846  -1.204  0.22905    \nyg.l5    0.3805559  0.1889051   2.015  0.04421 *  \njyp.l5  -0.1128827  0.1602207  -0.705  0.48125    \nconst    0.4715235  0.7981067   0.591  0.55478    \ntrend   -0.0002577  0.0013922  -0.185  0.85317    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.319 on 1025 degrees of freedom\nMultiple R-Squared: 0.9921, Adjusted R-squared: 0.9919 \nF-statistic:  6101 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0114542  0.0134067   0.854  0.39310    \nsm.l1    0.9790130  0.0371724  26.337  &lt; 2e-16 ***\nyg.l1   -0.1442679  0.0706553  -2.042  0.04142 *  \njyp.l1  -0.0175303  0.0607927  -0.288  0.77313    \nhybe.l2 -0.0084490  0.0177536  -0.476  0.63425    \nsm.l2    0.0736360  0.0523239   1.407  0.15964    \nyg.l2    0.1218543  0.0941910   1.294  0.19606    \njyp.l2  -0.0382295  0.0804199  -0.475  0.63462    \nhybe.l3 -0.0002227  0.0178801  -0.012  0.99006    \nsm.l3    0.0244439  0.0526820   0.464  0.64275    \nyg.l3    0.0541715  0.0950036   0.570  0.56866    \njyp.l3  -0.0345022  0.0806976  -0.428  0.66907    \nhybe.l4  0.0101214  0.0173215   0.584  0.55913    \nsm.l4   -0.1002341  0.0527443  -1.900  0.05766 .  \nyg.l4   -0.0253390  0.0946078  -0.268  0.78888    \njyp.l4   0.1016257  0.0796230   1.276  0.20213    \nhybe.l5 -0.0094405  0.0127256  -0.742  0.45835    \nsm.l5   -0.0011058  0.0376593  -0.029  0.97658    \nyg.l5   -0.0209738  0.0712226  -0.294  0.76845    \njyp.l5  -0.0024879  0.0604077  -0.041  0.96716    \nconst    0.1965731  0.3009087   0.653  0.51373    \ntrend    0.0013967  0.0005249   2.661  0.00791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.628 on 1025 degrees of freedom\nMultiple R-Squared: 0.9941, Adjusted R-squared: 0.994 \nF-statistic:  8268 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -1.266e-03  8.096e-03  -0.156 0.875731    \nsm.l1   -4.700e-03  2.245e-02  -0.209 0.834207    \nyg.l1    8.482e-01  4.267e-02  19.881  &lt; 2e-16 ***\njyp.l1   8.270e-03  3.671e-02   0.225 0.821811    \nhybe.l2  2.944e-03  1.072e-02   0.275 0.783682    \nsm.l2   -2.157e-02  3.160e-02  -0.683 0.494890    \nyg.l2    2.194e-01  5.688e-02   3.857 0.000122 ***\njyp.l2  -4.823e-03  4.856e-02  -0.099 0.920899    \nhybe.l3  2.151e-03  1.080e-02   0.199 0.842116    \nsm.l3    4.958e-02  3.181e-02   1.558 0.119453    \nyg.l3    1.158e-03  5.737e-02   0.020 0.983902    \njyp.l3   6.828e-03  4.873e-02   0.140 0.888594    \nhybe.l4  2.441e-03  1.046e-02   0.233 0.815503    \nsm.l4   -3.504e-02  3.185e-02  -1.100 0.271569    \nyg.l4    2.502e-03  5.713e-02   0.044 0.965078    \njyp.l4   8.681e-03  4.808e-02   0.181 0.856762    \nhybe.l5 -5.463e-03  7.684e-03  -0.711 0.477337    \nsm.l5    1.507e-02  2.274e-02   0.662 0.507801    \nyg.l5   -9.023e-02  4.301e-02  -2.098 0.036153 *  \njyp.l5  -1.735e-02  3.648e-02  -0.476 0.634446    \nconst    3.983e-01  1.817e-01   2.192 0.028616 *  \ntrend    4.017e-05  3.170e-04   0.127 0.899178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9834 on 1025 degrees of freedom\nMultiple R-Squared: 0.9866, Adjusted R-squared: 0.9863 \nF-statistic:  3591 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0006532  0.0091949   0.071  0.94338    \nsm.l1   -0.0145845  0.0254945  -0.572  0.56740    \nyg.l1   -0.0662954  0.0484585  -1.368  0.17158    \njyp.l1   0.8855542  0.0416943  21.239  &lt; 2e-16 ***\nhybe.l2 -0.0014774  0.0121762  -0.121  0.90345    \nsm.l2   -0.0432859  0.0358860  -1.206  0.22802    \nyg.l2    0.1533706  0.0646002   2.374  0.01777 *  \njyp.l2   0.1381693  0.0551555   2.505  0.01240 *  \nhybe.l3  0.0088203  0.0122629   0.719  0.47214    \nsm.l3    0.0991813  0.0361316   2.745  0.00616 ** \nyg.l3   -0.1022738  0.0651575  -1.570  0.11681    \njyp.l3  -0.0491761  0.0553459  -0.889  0.37447    \nhybe.l4 -0.0071445  0.0118798  -0.601  0.54771    \nsm.l4   -0.1109944  0.0361743  -3.068  0.00221 ** \nyg.l4    0.1607349  0.0648861   2.477  0.01340 *  \njyp.l4   0.0908549  0.0546088   1.664  0.09647 .  \nhybe.l5 -0.0001754  0.0087278  -0.020  0.98397    \nsm.l5    0.0571925  0.0258284   2.214  0.02703 *  \nyg.l5   -0.1325737  0.0488475  -2.714  0.00676 ** \njyp.l5  -0.0738799  0.0414302  -1.783  0.07484 .  \nconst   -0.1891913  0.2063761  -0.917  0.35950    \ntrend    0.0011485  0.0003600   3.190  0.00146 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.117 on 1025 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9968 \nF-statistic: 1.531e+04 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 18.656 2.1148 2.0584 1.8330\nsm    2.115 2.6519 0.7396 0.9206\nyg    2.058 0.7396 0.9670 0.6693\njyp   1.833 0.9206 0.6693 1.2474\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.3007 0.4846 0.3800\nsm   0.3007 1.0000 0.4619 0.5061\nyg   0.4846 0.4619 1.0000 0.6094\njyp  0.3800 0.5061 0.6094 1.0000\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don’t see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM.\nThus, before we continue with the model, we will also verify through a CV test.\n\n\nCross Validation:\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df_ts[-(start:end), ]\n  test_model &lt;- df_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n\n[1] \"The best model is: \"\n\n\nCode\nprint(best_model)\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n 0.898826902 -0.129257139  0.240081729 -0.195873591  0.086817669  0.112162459 \n       yg.l2       jyp.l2        const        trend \n-0.145841108  0.163410772  0.119069039  0.001963864 \n\n\nEstimated coefficients for equation sm: \n======================================= \nCall:\nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003282289  0.979354101 -0.124633554 -0.030561719  0.006619618 -0.001280822 \n       yg.l2       jyp.l2        const        trend \n 0.099485625  0.041169059  0.424147339  0.001889801 \n\n\nEstimated coefficients for equation yg: \n======================================= \nCall:\nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n      hybe.l1         sm.l1         yg.l1        jyp.l1       hybe.l2 \n-0.0017546456 -0.0129013663  0.8872940815 -0.0132784925  0.0020911541 \n        sm.l2         yg.l2        jyp.l2         const         trend \n 0.0157819564  0.1016893422  0.0123618307  0.2539521440  0.0001621699 \n\n\nEstimated coefficients for equation jyp: \n======================================== \nCall:\njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003714104 -0.013862592 -0.042708139  0.845587797  0.003228421 -0.002809650 \n       yg.l2       jyp.l2        const        trend \n 0.065350796  0.141050674 -0.223955478  0.002160854 \n\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2.\n\n\nModel Creation:\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 271.32, df = 160, p-value = 9.23e-08\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 154.19, df = 112, p-value = 0.005085\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies.\n\n\nForecasting:\n\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction."
  },
  {
    "objectID": "arimax.html#kpop-and-the-western-industry---var",
    "href": "arimax.html#kpop-and-the-western-industry---var",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(2) KPOP and the Western industry - VAR:",
    "text": "(2) KPOP and the Western industry - VAR:\nSimilarly, we’ll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE’s artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three.\nWe’ll follow the same steps as before the get some initial p values from VARselect().\n\n\nCode\n#Creating a subset of only Korean Record label stock data\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(WMG, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nwmg &lt;- ts(df2$WMG_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, wmg)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"wmg\")\n\nautoplot(df2_ts)\n\n\n\n\n\nFrom an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we’ll continue with the VAR model to work on forecasting.\n\nVARselect\n\n\nCode\nVARselect(df2_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      5      1      5 \n\n$criteria\n                 1           2         3           4           5           6\nAIC(n) -2.78789774 -2.81967391 -2.829136 -2.84935594 -2.86912386 -2.85995667\nHQ(n)  -2.76085322 -2.77640268 -2.769638 -2.77363129 -2.77717250 -2.75177860\nSC(n)  -2.71660182 -2.70560043 -2.672285 -2.64972736 -2.62671772 -2.57477298\nFPE(n)  0.06155049  0.05962544  0.059064  0.05788187  0.05674912  0.05727205\n                 7           8           9          10\nAIC(n) -2.85554727 -2.85877318 -2.84846364 -2.84125864\nHQ(n)  -2.73114249 -2.71814168 -2.69160543 -2.66817372\nSC(n)  -2.52758602 -2.48803438 -2.43494729 -2.38496473\nFPE(n)  0.05752557  0.05734085  0.05793576  0.05835557\n\n\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let’s continue by analyzing the residuals squared errors.\n\n\nInitial selection:\n\n\nCode\nsummary(vars::VAR(df2_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, wmg \nDeterministic variables: both \nSample size: 1050 \nLog Likelihood: -3025.645 \nRoots of the characteristic polynomial:\n0.9954 0.9811 0.9811\nCall:\nvars::VAR(y = df2_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9874002  0.0038359 257.409  &lt; 2e-16 ***\numgp.l1 -0.4134390  0.2988988  -1.383  0.16690    \nwmg.l1   0.1682297  0.0396215   4.246 2.37e-05 ***\nconst   -3.5345321  1.2007940  -2.943  0.00332 ** \ntrend    0.0009025  0.0005039   1.791  0.07356 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.548 on 1045 degrees of freedom\nMultiple R-Squared: 0.991,  Adjusted R-squared: 0.991 \nF-statistic: 2.89e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  1.060e-04  7.167e-05   1.479    0.139    \numgp.l1  9.853e-01  5.584e-03 176.445   &lt;2e-16 ***\nwmg.l1   1.418e-04  7.403e-04   0.192    0.848    \nconst   -1.131e-02  2.243e-02  -0.504    0.614    \ntrend   -5.273e-06  9.414e-06  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08497 on 1045 degrees of freedom\nMultiple R-Squared: 0.9817, Adjusted R-squared: 0.9816 \nF-statistic: 1.401e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wmg: \n==================================== \nwmg = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -3.539e-04  5.561e-04  -0.636  0.52460    \numgp.l1  1.007e-01  4.333e-02   2.324  0.02034 *  \nwmg.l1   9.848e-01  5.744e-03 171.454  &lt; 2e-16 ***\nconst    5.536e-01  1.741e-01   3.181  0.00151 ** \ntrend   -9.958e-05  7.305e-05  -1.363  0.17310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6593 on 1045 degrees of freedom\nMultiple R-Squared: 0.9852, Adjusted R-squared: 0.9851 \nF-statistic: 1.734e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          hybe      umgp      wmg\nhybe 20.683699 -0.009843 0.023627\numgp -0.009843  0.007220 0.001471\nwmg   0.023627  0.001471 0.434663\n\nCorrelation matrix of residuals:\n         hybe     umgp     wmg\nhybe  1.00000 -0.02547 0.00788\numgp -0.02547  1.00000 0.02625\nwmg   0.00788  0.02625 1.00000\n\n\nCode\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, wmg \nDeterministic variables: both \nSample size: 1046 \nLog Likelihood: -2895.264 \nRoots of the characteristic polynomial:\n0.995 0.9817 0.9817 0.6589 0.6499 0.6499 0.6072 0.6072 0.5731 0.5731 0.5694 0.531 0.531 0.4435 0.4147\nCall:\nvars::VAR(y = df2_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8536278  0.0308478  27.672  &lt; 2e-16 ***\numgp.l1 -1.0652461  1.6099455  -0.662 0.508333    \nwmg.l1   0.2668806  0.2038881   1.309 0.190841    \nhybe.l2  0.1477791  0.0407233   3.629 0.000299 ***\numgp.l2  2.4380127  2.1402852   1.139 0.254924    \nwmg.l2   0.0341023  0.2699214   0.126 0.899486    \nhybe.l3  0.0827366  0.0409220   2.022 0.043454 *  \numgp.l3 -1.5539197  2.0986070  -0.740 0.459194    \nwmg.l3   0.1884273  0.2707225   0.696 0.486576    \nhybe.l4  0.0347094  0.0398020   0.872 0.383384    \numgp.l4  2.6506064  2.1383552   1.240 0.215423    \nwmg.l4  -0.1352535  0.2702575  -0.500 0.616857    \nhybe.l5 -0.1286473  0.0294432  -4.369 1.37e-05 ***\numgp.l5 -2.8808902  1.6099556  -1.789 0.073841 .  \nwmg.l5  -0.2280257  0.2048504  -1.113 0.265911    \nconst   -2.3625237  1.1766428  -2.008 0.044920 *  \ntrend    0.0004436  0.0004824   0.920 0.357984    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.285 on 1029 degrees of freedom\nMultiple R-Squared: 0.9922, Adjusted R-squared: 0.992 \nF-statistic:  8136 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -3.726e-04  5.956e-04  -0.625  0.53179    \numgp.l1  8.751e-01  3.109e-02  28.152  &lt; 2e-16 ***\nwmg.l1   5.762e-03  3.937e-03   1.464  0.14359    \nhybe.l2  2.137e-04  7.863e-04   0.272  0.78582    \numgp.l2  4.105e-04  4.133e-02   0.010  0.99208    \nwmg.l2  -1.151e-02  5.212e-03  -2.209  0.02742 *  \nhybe.l3  8.423e-05  7.901e-04   0.107  0.91513    \numgp.l3  2.658e-01  4.052e-02   6.560 8.52e-11 ***\nwmg.l3   5.069e-03  5.227e-03   0.970  0.33239    \nhybe.l4  9.731e-05  7.685e-04   0.127  0.89927    \numgp.l4 -7.039e-02  4.129e-02  -1.705  0.08854 .  \nwmg.l4   2.927e-03  5.218e-03   0.561  0.57497    \nhybe.l5  8.739e-05  5.685e-04   0.154  0.87785    \numgp.l5 -8.534e-02  3.109e-02  -2.745  0.00615 ** \nwmg.l5  -2.142e-03  3.955e-03  -0.542  0.58823    \nconst   -1.084e-02  2.272e-02  -0.477  0.63332    \ntrend   -5.483e-06  9.315e-06  -0.589  0.55625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08274 on 1029 degrees of freedom\nMultiple R-Squared: 0.9829, Adjusted R-squared: 0.9826 \nF-statistic:  3691 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wmg: \n==================================== \nwmg = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  3.441e-03  4.701e-03   0.732  0.46436    \numgp.l1 -1.025e-01  2.454e-01  -0.418  0.67612    \nwmg.l1   8.640e-01  3.107e-02  27.806  &lt; 2e-16 ***\nhybe.l2 -1.988e-03  6.206e-03  -0.320  0.74882    \numgp.l2  4.874e-01  3.262e-01   1.494  0.13540    \nwmg.l2   1.159e-01  4.114e-02   2.817  0.00493 ** \nhybe.l3  4.261e-04  6.236e-03   0.068  0.94554    \numgp.l3 -3.273e-01  3.198e-01  -1.023  0.30643    \nwmg.l3   9.549e-02  4.126e-02   2.315  0.02083 *  \nhybe.l4 -6.217e-04  6.066e-03  -0.102  0.91838    \numgp.l4 -2.888e-01  3.259e-01  -0.886  0.37574    \nwmg.l4  -1.480e-02  4.119e-02  -0.359  0.71950    \nhybe.l5 -1.587e-03  4.487e-03  -0.354  0.72371    \numgp.l5  3.438e-01  2.454e-01   1.401  0.16139    \nwmg.l5  -7.780e-02  3.122e-02  -2.492  0.01286 *  \nconst    6.149e-01  1.793e-01   3.429  0.00063 ***\ntrend   -1.127e-04  7.352e-05  -1.533  0.12555    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.653 on 1029 degrees of freedom\nMultiple R-Squared: 0.9856, Adjusted R-squared: 0.9854 \nF-statistic:  4406 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           hybe      umgp        wmg\nhybe 18.3621987 -0.011192 -0.0006354\numgp -0.0111920  0.006846  0.0021158\nwmg  -0.0006354  0.002116  0.4264670\n\nCorrelation matrix of residuals:\n           hybe     umgp        wmg\nhybe  1.0000000 -0.03157 -0.0002271\numgp -0.0315674  1.00000  0.0391587\nwmg  -0.0002271  0.03916  1.0000000\n\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and WMG are very low, however the error for HYBE is larger at at approximately 4. Thus, we’ll continue model selection through cross validation.\n\n\nCross Validation:\n\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df2_ts[-(start:end), ]\n  test_model &lt;- df2_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n[1] \"The best model is: \"\n\nprint(best_model)\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n 0.8757355472 -0.5532759827  0.3849696776  0.1403298086  4.5910325367 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n-0.8396373501  0.0350757902 -4.3266286388  0.3937674301  0.0166586992 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n 4.3390220796 -0.1515542538  0.0008274006 -3.3109206930 -0.1738489911 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-0.1691033996 -1.9451279645  0.9263803006  0.0852654811  3.9147275659 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n-0.5634778231  0.0021288838 -3.2067511668  0.1311322839 -1.2217711949 \n        trend \n 0.0006281564 \n\n\nEstimated coefficients for equation umgp: \n========================================= \nCall:\numgp = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n-3.392018e-04  9.557157e-01  7.330741e-03  1.061675e-03 -8.060663e-02 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n-1.271127e-02 -6.255760e-04  2.753141e-01  6.337801e-03  1.307251e-04 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n-7.821245e-02 -9.189091e-04  2.612016e-04 -1.501988e-01 -3.891641e-04 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-1.531753e-03  1.617081e-01 -1.037428e-03  2.553486e-03 -8.735350e-02 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n-8.636327e-03 -1.434416e-03 -9.115993e-03  9.984357e-03 -1.173168e-03 \n        trend \n-7.565877e-06 \n\n\nEstimated coefficients for equation wmg: \n======================================== \nCall:\nwmg = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n 9.167662e-03 -3.782794e-02  9.150911e-01 -9.296198e-03  5.155083e-01 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n 3.452792e-02 -2.743506e-05 -3.874236e-01  6.970375e-02  6.309112e-07 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n-1.532448e-01  9.323962e-03  2.302537e-03  1.819176e-01 -5.698217e-02 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-9.515242e-03  4.195772e-01 -6.757774e-03  2.272819e-02 -1.104684e+00 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n 1.928559e-02 -1.613771e-02  6.685347e-01 -3.901615e-03  7.823160e-01 \n        trend \n-1.475191e-04 \n\n\nCV seems to have chosen a different model where p=8. Thus, we’ll create models for p=1,5,8.\n\n\nModel Creation:\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 264.14, df = 99, p-value &lt; 2.2e-16\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\") \n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 88.378, df = 63, p-value = 0.01918\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_3 &lt;- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_3\nChi-squared = 42.802, df = 36, p-value = 0.2023\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\")\n\n\n\n\n\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 &lt; 0.05.\n\n\nForecasting:\n\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and WMG, while UMGP’s stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea.\nKnowing this downward trend in the stock prices of the biggest performing KPOP music agency, we can start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea."
  },
  {
    "objectID": "arimax.html#foreign-tourism-in-korea-on-cultural-globalization-in-the-usa",
    "href": "arimax.html#foreign-tourism-in-korea-on-cultural-globalization-in-the-usa",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(3) - Foreign tourism in Korea on Cultural Globalization in the USA",
    "text": "(3) - Foreign tourism in Korea on Cultural Globalization in the USA\nLet’s see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting.\n\nGathering the Data\nWe’ll combine the globalization index data from KOF with the South Korean tourism data from Statistica.\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP’s influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture.\n\n\nCode\nglobal_ts &lt;-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n\n\n\n\n\nFrom the graphs above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020.\n\n\nUsing Auto.Arima()\nNow, let’s move on with the ARIMAX/ARMAX model. First, we’ll create a model using auto.arima().\n\n\nCode\nfit &lt;- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  intercept  xreg\n      1.6838  -0.7370    87.7759     0\ns.e.  0.1365   0.1434     0.4453     0\n\nsigma^2 = 2.666:  log likelihood = -38.14\nAIC=86.28   AICc=90.57   BIC=91.26\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 0.2527717 1.460494 1.089386 0.2644077 1.265622 0.9198188 0.1042156\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,0) errors\nQ* = 7.4991, df = 3, p-value = 0.05758\n\nModel df: 2.   Total lags used: 5\n\n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant.\n\n\nManually Finding the Model:\nWe’ll move now to find the ARMAX model manually. Let’s start by taking creating a regression model of tourism on cultural globalization. Using that model, we’ll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we’ll validate the model through cross validation.\n\n\nCode\ndf3$tourists &lt;-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf &lt;-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg &lt;- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = KOFCuGIdf ~ tourists, data = df3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2468 -4.0753  0.9324  3.8493  8.2528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.783e+01  3.063e+00   25.41  1.5e-15 ***\ntourists    1.202e-06  2.918e-07    4.12 0.000644 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.468 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006436\n\n\n\nres.fit&lt;-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\n\n\n\nggPacf(res.fit)\n\n\n\n\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data.\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n127.2329\n130.2201\n128.7329\n\n\n0\n0\n1\n116.8798\n120.8627\n119.5464\n\n\n0\n0\n2\n111.9461\n116.9248\n116.2318\n\n\n0\n0\n3\n109.5787\n115.5531\n116.0402\n\n\n0\n0\n4\n111.3460\n118.3162\n120.6794\n\n\n1\n0\n0\n106.7209\n110.7039\n109.3876\n\n\n1\n0\n1\n108.4723\n113.4510\n112.7580\n\n\n1\n0\n2\n108.4037\n114.3781\n114.8653\n\n\n1\n0\n3\n110.3598\n117.3299\n119.6931\n\n\n1\n0\n4\n112.3462\n120.3121\n125.4371\n\n\n2\n0\n0\n108.2759\n113.2546\n112.5616\n\n\n2\n0\n1\n105.2455\n111.2199\n111.7071\n\n\n2\n0\n2\n104.8959\n111.8660\n114.2292\n\n\n2\n0\n3\n112.2294\n120.1952\n125.3203\n\n\n2\n0\n4\n114.0108\n122.9724\n132.0108\n\n\n3\n0\n0\n107.7495\n113.7239\n114.2110\n\n\n3\n0\n1\n105.8867\n112.8568\n115.2200\n\n\n3\n0\n2\n112.1006\n120.0664\n125.1915\n\n\n3\n0\n3\n113.3105\n122.2721\n131.3105\n\n\n3\n0\n4\n115.3092\n125.2666\n139.7537\n\n\n4\n0\n0\n109.2205\n116.1906\n118.5538\n\n\n4\n0\n1\n111.1023\n119.0682\n124.1932\n\n\n4\n0\n2\n113.2815\n122.2431\n131.2815\n\n\n4\n0\n3\n111.4614\n121.4187\n135.9058\n\n\n4\n0\n4\n111.5921\n122.5452\n144.5921\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q      AIC     BIC     AICc\n13 2 0 2 104.8959 111.866 114.2292\n\n\nCode\noutput[which.min(output$BIC),] \n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0). Thus, we’ll take a look at the residuals of the following models:\n\n\nCode\ncapture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n [1] \"initial  value 1.585318 \"                                                              \n [2] \"iter   2 value 1.000819\"                                                               \n [3] \"iter   3 value 0.979873\"                                                               \n [4] \"iter   4 value 0.971786\"                                                               \n [5] \"iter   5 value 0.970922\"                                                               \n [6] \"iter   6 value 0.970615\"                                                               \n [7] \"iter   7 value 0.970607\"                                                               \n [8] \"iter   8 value 0.970607\"                                                               \n [9] \"iter   8 value 0.970607\"                                                               \n[10] \"iter   8 value 0.970607\"                                                               \n[11] \"final  value 0.970607 \"                                                                \n[12] \"converged\"                                                                             \n[13] \"initial  value 1.116539 \"                                                              \n[14] \"iter   2 value 1.064095\"                                                               \n[15] \"iter   3 value 1.062560\"                                                               \n[16] \"iter   4 value 1.061681\"                                                               \n[17] \"iter   5 value 1.061650\"                                                               \n[18] \"iter   6 value 1.061650\"                                                               \n[19] \"iter   6 value 1.061650\"                                                               \n[20] \"final  value 1.061650 \"                                                                \n[21] \"converged\"                                                                             \n[22] \"$fit\"                                                                                  \n[23] \"\"                                                                                      \n[24] \"Call:\"                                                                                 \n[25] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[26] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[27] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[28] \"\"                                                                                      \n[29] \"Coefficients:\"                                                                         \n[30] \"         ar1    xmean\"                                                                 \n[31] \"      0.8652  -2.5386\"                                                                 \n[32] \"s.e.  0.1038   3.8694\"                                                                 \n[33] \"\"                                                                                      \n[34] \"sigma^2 estimated as 7.801:  log likelihood = -49.61,  aic = 105.22\"                   \n[35] \"\"                                                                                      \n[36] \"$degrees_of_freedom\"                                                                   \n[37] \"[1] 18\"                                                                                \n[38] \"\"                                                                                      \n[39] \"$ttable\"                                                                               \n[40] \"      Estimate     SE t.value p.value\"                                                 \n[41] \"ar1     0.8652 0.1038  8.3328  0.0000\"                                                 \n[42] \"xmean  -2.5386 3.8694 -0.6561  0.5201\"                                                 \n[43] \"\"                                                                                      \n[44] \"$AIC\"                                                                                  \n[45] \"[1] 5.261176\"                                                                          \n[46] \"\"                                                                                      \n[47] \"$AICc\"                                                                                 \n[48] \"[1] 5.29647\"                                                                           \n[49] \"\"                                                                                      \n[50] \"$BIC\"                                                                                  \n[51] \"[1] 5.410536\"                                                                          \n[52] \"\"                                                                                      \n\n\nCode\ncapture.output(sarima(res.fit, 2,0,2)) \n\n\n\n\n\n [1] \"initial  value 1.553778 \"                                                              \n [2] \"iter   2 value 1.430949\"                                                               \n [3] \"iter   3 value 1.042676\"                                                               \n [4] \"iter   4 value 0.993029\"                                                               \n [5] \"iter   5 value 0.974081\"                                                               \n [6] \"iter   6 value 0.956070\"                                                               \n [7] \"iter   7 value 0.949449\"                                                               \n [8] \"iter   8 value 0.944797\"                                                               \n [9] \"iter   9 value 0.942595\"                                                               \n[10] \"iter  10 value 0.941319\"                                                               \n[11] \"iter  11 value 0.940887\"                                                               \n[12] \"iter  12 value 0.940866\"                                                               \n[13] \"iter  13 value 0.940866\"                                                               \n[14] \"iter  14 value 0.940865\"                                                               \n[15] \"iter  15 value 0.940862\"                                                               \n[16] \"iter  16 value 0.940853\"                                                               \n[17] \"iter  17 value 0.940845\"                                                               \n[18] \"iter  18 value 0.940830\"                                                               \n[19] \"iter  19 value 0.940820\"                                                               \n[20] \"iter  20 value 0.940818\"                                                               \n[21] \"iter  21 value 0.940816\"                                                               \n[22] \"iter  22 value 0.940816\"                                                               \n[23] \"iter  22 value 0.940816\"                                                               \n[24] \"iter  22 value 0.940816\"                                                               \n[25] \"final  value 0.940816 \"                                                                \n[26] \"converged\"                                                                             \n[27] \"initial  value 1.014365 \"                                                              \n[28] \"iter   2 value 1.000713\"                                                               \n[29] \"iter   3 value 0.998434\"                                                               \n[30] \"iter   4 value 0.997970\"                                                               \n[31] \"iter   5 value 0.997732\"                                                               \n[32] \"iter   6 value 0.997580\"                                                               \n[33] \"iter   7 value 0.997379\"                                                               \n[34] \"iter   8 value 0.997048\"                                                               \n[35] \"iter   9 value 0.996543\"                                                               \n[36] \"iter  10 value 0.995733\"                                                               \n[37] \"iter  11 value 0.991400\"                                                               \n[38] \"iter  12 value 0.985956\"                                                               \n[39] \"iter  13 value 0.981444\"                                                               \n[40] \"iter  14 value 0.974812\"                                                               \n[41] \"iter  15 value 0.971043\"                                                               \n[42] \"iter  16 value 0.960387\"                                                               \n[43] \"iter  17 value 0.955391\"                                                               \n[44] \"iter  18 value 0.944255\"                                                               \n[45] \"iter  19 value 0.918264\"                                                               \n[46] \"iter  20 value 0.911566\"                                                               \n[47] \"iter  21 value 0.908276\"                                                               \n[48] \"iter  22 value 0.904759\"                                                               \n[49] \"iter  23 value 0.902341\"                                                               \n[50] \"iter  24 value 0.902240\"                                                               \n[51] \"iter  25 value 0.902160\"                                                               \n[52] \"iter  26 value 0.901582\"                                                               \n[53] \"iter  27 value 0.900613\"                                                               \n[54] \"iter  28 value 0.900164\"                                                               \n[55] \"iter  29 value 0.900127\"                                                               \n[56] \"iter  30 value 0.900121\"                                                               \n[57] \"iter  31 value 0.900121\"                                                               \n[58] \"iter  32 value 0.900121\"                                                               \n[59] \"iter  32 value 0.900121\"                                                               \n[60] \"iter  32 value 0.900121\"                                                               \n[61] \"final  value 0.900121 \"                                                                \n[62] \"converged\"                                                                             \n[63] \"$fit\"                                                                                  \n[64] \"\"                                                                                      \n[65] \"Call:\"                                                                                 \n[66] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[67] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[68] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[69] \"\"                                                                                      \n[70] \"Coefficients:\"                                                                         \n[71] \"         ar1      ar2      ma1     ma2   xmean\"                                        \n[72] \"      1.8206  -0.9472  -1.2918  0.2918  0.4865\"                                        \n[73] \"s.e.  0.0631   0.0603   0.3380  0.2889  0.8530\"                                        \n[74] \"\"                                                                                      \n[75] \"sigma^2 estimated as 4.709:  log likelihood = -46.38,  aic = 104.76\"                   \n[76] \"\"                                                                                      \n[77] \"$degrees_of_freedom\"                                                                   \n[78] \"[1] 15\"                                                                                \n[79] \"\"                                                                                      \n[80] \"$ttable\"                                                                               \n[81] \"      Estimate     SE  t.value p.value\"                                                \n[82] \"ar1     1.8206 0.0631  28.8298  0.0000\"                                                \n[83] \"ar2    -0.9472 0.0603 -15.7180  0.0000\"                                                \n[84] \"ma1    -1.2918 0.3380  -3.8223  0.0017\"                                                \n[85] \"ma2     0.2918 0.2889   1.0100  0.3285\"                                                \n[86] \"xmean   0.4865 0.8530   0.5703  0.5769\"                                                \n[87] \"\"                                                                                      \n[88] \"$AIC\"                                                                                  \n[89] \"[1] 5.238119\"                                                                          \n[90] \"\"                                                                                      \n[91] \"$AICc\"                                                                                 \n[92] \"[1] 5.452405\"                                                                          \n[93] \"\"                                                                                      \n[94] \"$BIC\"                                                                                  \n[95] \"[1] 5.536838\"                                                                          \n[96] \"\"                                                                                      \n\n\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we’ll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting.\n\n\nCV\n\n\nCode\nn &lt;- length(res.fit)\nk &lt;- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 &lt;- matrix(NA, 15)\nrmse2 &lt;- matrix(NA, 15)\nrmse3 &lt;- matrix(NA, 15)\n\nst &lt;- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end &lt;- st + i - 1\n  xtrain &lt;- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start &lt;- train_end + 1\n  test_end &lt;- min(st + i, tsp(res.fit)[2])\n  xtest &lt;- window(res.fit, start = test_start, end = test_end)\n\n  fit &lt;- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast &lt;- forecast(fit, h = 4)\n\n  fit2 &lt;- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 &lt;- forecast(fit2, h = 4)\n\n  fit3 &lt;- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 &lt;- forecast(fit3, h = 4)\n\n  rmse1[i] &lt;- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] &lt;- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] &lt;- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n\n\n\n\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we’ll choose to forecast Korean tourism on cultural globalization in the US via model 1.\n\n\nCode\nfit &lt;- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept  xreg\n      0.9684    84.7153     0\ns.e.  0.0398     1.3007     0\n\nsigma^2 = 5.577:  log likelihood = -45.33\nAIC=98.66   AICc=101.32   BIC=102.64\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE    MASE     ACF1\nTraining set 0.8955075 2.177229 1.295393 0.9914592 1.500592 1.09376 0.462599\n\n\n\n\nForecasting:\n\n\nCode\ntourists_fit &lt;-auto.arima(global_ts[, \"tourists\"]) \nsummary(tourists_fit)\n\n\nSeries: global_ts[, \"tourists\"] \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1     drift\n      -0.5710  627170.2\ns.e.   0.1837  185981.8\n\nsigma^2 = 1.743e+12:  log likelihood = -293.87\nAIC=593.75   AICc=595.35   BIC=596.58\n\nTraining set error measures:\n                    ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set -15386.25 1217172 961194.1 -2.96541 9.812744 0.7784607 -0.1121698\n\n\nCode\nft&lt;-forecast(tourists_fit)\n\nfcast &lt;- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n\n\n\n\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea’s tourism of foreigners will see a slight decrease. As we’ve observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done."
  },
  {
    "objectID": "hw4.html",
    "href": "hw4.html",
    "title": "hw4",
    "section": "",
    "text": "Code\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"WMG\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nWMG &lt;- data.frame(WMG$WMG.Adjusted)\nWMG &lt;- WMG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(WMG_Price = WMG.Adjusted)\n\n\nstart_date &lt;- as.Date(min(WMG$Date))  \nend_date &lt;- as.Date(max(WMG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nWMG &lt;- merge(WMG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- WMG[which(rowSums(is.na(WMG)) &gt; 0),]\ndf_na_cols &lt;- WMG[, which(colSums(is.na(WMG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(WMG, k = 4, weighting = \"exponential\")\nWMG &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, WMG, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"WMG\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n\n\n\nCode\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n\n\nConverting to Time Series\n\n\nCode\nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n\n\n\nVisualizing the data:\n\n\nCode\nautoplot(df_ts)\n\n\n\n\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let’s continue with the VAR model to see what the multivariate relationship is.\n\n\nVARselect\n\n\nCode\nVARselect(df_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  3.142389  3.114099  3.117499  3.112310  3.088174  3.092878  3.107702\nHQ(n)   3.185625  3.186159  3.218383  3.242018  3.246706  3.280234  3.323882\nSC(n)   3.256375  3.304076  3.383466  3.454268  3.506123  3.586817  3.677632\nFPE(n) 23.159131 22.513187 22.589948 22.473198 21.937533 22.041324 22.370993\n               8         9        10\nAIC(n)  3.111249  3.118763  3.131486\nHQ(n)   3.356254  3.392591  3.434139\nSC(n)   3.757171  3.840675  3.929389\nFPE(n) 22.451153 22.621304 22.912010\n\n\nWe can see that the p-values detected from VARselect() are 5 and 1.\n\n\nInitial selection:\n\n\nCode\nsummary(vars::VAR(df_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1051 \nLog Likelihood: -7636.31 \nRoots of the characteristic polynomial:\n0.9954 0.9902 0.9811 0.9811\nCall:\nvars::VAR(y = df_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9908097  0.0048019 206.337   &lt;2e-16 ***\nsm.l1    0.0022993  0.0195271   0.118    0.906    \nyg.l1    0.0622194  0.0400453   1.554    0.121    \njyp.l1  -0.0302953  0.0197591  -1.533    0.126    \nconst   -0.0663658  0.8191955  -0.081    0.935    \ntrend    0.0005578  0.0014217   0.392    0.695    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.582 on 1045 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9909 \nF-statistic: 2.277e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0036634  0.0017083   2.145   0.0322 *  \nsm.l1    0.9801908  0.0069467 141.101   &lt;2e-16 ***\nyg.l1   -0.0231121  0.0142460  -1.622   0.1050    \njyp.l1   0.0110615  0.0070293   1.574   0.1159    \nconst    0.3284611  0.2914272   1.127   0.2600    \ntrend    0.0011339  0.0005058   2.242   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.63 on 1045 degrees of freedom\nMultiple R-Squared: 0.994,  Adjusted R-squared: 0.994 \nF-statistic: 3.491e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005472  0.0010454   0.523   0.6008    \nsm.l1    0.0025344  0.0042514   0.596   0.5512    \nyg.l1    0.9868482  0.0087185 113.190   &lt;2e-16 ***\njyp.l1  -0.0007041  0.0043019  -0.164   0.8700    \nconst    0.2949613  0.1783516   1.654   0.0985 .  \ntrend    0.0001396  0.0003095   0.451   0.6520    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9976 on 1045 degrees of freedom\nMultiple R-Squared: 0.986,  Adjusted R-squared: 0.9859 \nF-statistic: 1.471e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0005536  0.0011980   0.462 0.644124    \nsm.l1   -0.0135935  0.0048718  -2.790 0.005363 ** \nyg.l1    0.0167605  0.0099909   1.678 0.093729 .  \njyp.l1   0.9896949  0.0049297 200.762  &lt; 2e-16 ***\nconst   -0.2443391  0.2043815  -1.196 0.232161    \ntrend    0.0012519  0.0003547   3.530 0.000434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.143 on 1045 degrees of freedom\nMultiple R-Squared: 0.9966, Adjusted R-squared: 0.9966 \nF-statistic: 6.159e+04 on 5 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 20.996 2.1862 2.1450 1.8659\nsm    2.186 2.6572 0.7519 0.9340\nyg    2.145 0.7519 0.9952 0.7025\njyp   1.866 0.9340 0.7025 1.3069\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.2927 0.4692 0.3562\nsm   0.2927 1.0000 0.4624 0.5012\nyg   0.4692 0.4624 1.0000 0.6160\njyp  0.3562 0.5012 0.6160 1.0000\n\n\nCode\nsummary(vars::VAR(df_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1047 \nLog Likelihood: -7463.522 \nRoots of the characteristic polynomial:\n0.9936 0.9907 0.9741 0.9741 0.6725 0.6725 0.6258 0.6258 0.5949 0.5705 0.5705 0.5578 0.5578 0.553 0.4157 0.4157 0.362 0.362 0.2872 0.09888\nCall:\nvars::VAR(y = df_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8597953  0.0355589  24.179  &lt; 2e-16 ***\nsm.l1   -0.0845122  0.0985933  -0.857  0.39155    \nyg.l1    0.1053591  0.1874006   0.562  0.57409    \njyp.l1   0.0478898  0.1612418   0.297  0.76652    \nhybe.l2  0.1418576  0.0470883   3.013  0.00265 ** \nsm.l2    0.0710565  0.1387798   0.512  0.60876    \nyg.l2   -0.0204218  0.2498248  -0.082  0.93487    \njyp.l2   0.0673012  0.2132995   0.316  0.75243    \nhybe.l3  0.1008928  0.0474237   2.127  0.03362 *  \nsm.l3    0.1305088  0.1397295   0.934  0.35052    \nyg.l3   -0.1482943  0.2519800  -0.589  0.55632    \njyp.l3  -0.2200135  0.2140360  -1.028  0.30423    \nhybe.l4  0.0367459  0.0459422   0.800  0.42399    \nsm.l4    0.0086685  0.1398948   0.062  0.95060    \nyg.l4   -0.2691600  0.2509303  -1.073  0.28368    \njyp.l4   0.1964530  0.2111857   0.930  0.35247    \nhybe.l5 -0.1487634  0.0337524  -4.407 1.16e-05 ***\nsm.l5   -0.1202141  0.0998846  -1.204  0.22905    \nyg.l5    0.3805559  0.1889051   2.015  0.04421 *  \njyp.l5  -0.1128827  0.1602207  -0.705  0.48125    \nconst    0.4715235  0.7981067   0.591  0.55478    \ntrend   -0.0002577  0.0013922  -0.185  0.85317    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.319 on 1025 degrees of freedom\nMultiple R-Squared: 0.9921, Adjusted R-squared: 0.9919 \nF-statistic:  6101 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0114542  0.0134067   0.854  0.39310    \nsm.l1    0.9790130  0.0371724  26.337  &lt; 2e-16 ***\nyg.l1   -0.1442679  0.0706553  -2.042  0.04142 *  \njyp.l1  -0.0175303  0.0607927  -0.288  0.77313    \nhybe.l2 -0.0084490  0.0177536  -0.476  0.63425    \nsm.l2    0.0736360  0.0523239   1.407  0.15964    \nyg.l2    0.1218543  0.0941910   1.294  0.19606    \njyp.l2  -0.0382295  0.0804199  -0.475  0.63462    \nhybe.l3 -0.0002227  0.0178801  -0.012  0.99006    \nsm.l3    0.0244439  0.0526820   0.464  0.64275    \nyg.l3    0.0541715  0.0950036   0.570  0.56866    \njyp.l3  -0.0345022  0.0806976  -0.428  0.66907    \nhybe.l4  0.0101214  0.0173215   0.584  0.55913    \nsm.l4   -0.1002341  0.0527443  -1.900  0.05766 .  \nyg.l4   -0.0253390  0.0946078  -0.268  0.78888    \njyp.l4   0.1016257  0.0796230   1.276  0.20213    \nhybe.l5 -0.0094405  0.0127256  -0.742  0.45835    \nsm.l5   -0.0011058  0.0376593  -0.029  0.97658    \nyg.l5   -0.0209738  0.0712226  -0.294  0.76845    \njyp.l5  -0.0024879  0.0604077  -0.041  0.96716    \nconst    0.1965731  0.3009087   0.653  0.51373    \ntrend    0.0013967  0.0005249   2.661  0.00791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.628 on 1025 degrees of freedom\nMultiple R-Squared: 0.9941, Adjusted R-squared: 0.994 \nF-statistic:  8268 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -1.266e-03  8.096e-03  -0.156 0.875731    \nsm.l1   -4.700e-03  2.245e-02  -0.209 0.834207    \nyg.l1    8.482e-01  4.267e-02  19.881  &lt; 2e-16 ***\njyp.l1   8.270e-03  3.671e-02   0.225 0.821811    \nhybe.l2  2.944e-03  1.072e-02   0.275 0.783682    \nsm.l2   -2.157e-02  3.160e-02  -0.683 0.494890    \nyg.l2    2.194e-01  5.688e-02   3.857 0.000122 ***\njyp.l2  -4.823e-03  4.856e-02  -0.099 0.920899    \nhybe.l3  2.151e-03  1.080e-02   0.199 0.842116    \nsm.l3    4.958e-02  3.181e-02   1.558 0.119453    \nyg.l3    1.158e-03  5.737e-02   0.020 0.983902    \njyp.l3   6.828e-03  4.873e-02   0.140 0.888594    \nhybe.l4  2.441e-03  1.046e-02   0.233 0.815503    \nsm.l4   -3.504e-02  3.185e-02  -1.100 0.271569    \nyg.l4    2.502e-03  5.713e-02   0.044 0.965078    \njyp.l4   8.681e-03  4.808e-02   0.181 0.856762    \nhybe.l5 -5.463e-03  7.684e-03  -0.711 0.477337    \nsm.l5    1.507e-02  2.274e-02   0.662 0.507801    \nyg.l5   -9.023e-02  4.301e-02  -2.098 0.036153 *  \njyp.l5  -1.735e-02  3.648e-02  -0.476 0.634446    \nconst    3.983e-01  1.817e-01   2.192 0.028616 *  \ntrend    4.017e-05  3.170e-04   0.127 0.899178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9834 on 1025 degrees of freedom\nMultiple R-Squared: 0.9866, Adjusted R-squared: 0.9863 \nF-statistic:  3591 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0006532  0.0091949   0.071  0.94338    \nsm.l1   -0.0145845  0.0254945  -0.572  0.56740    \nyg.l1   -0.0662954  0.0484585  -1.368  0.17158    \njyp.l1   0.8855542  0.0416943  21.239  &lt; 2e-16 ***\nhybe.l2 -0.0014774  0.0121762  -0.121  0.90345    \nsm.l2   -0.0432859  0.0358860  -1.206  0.22802    \nyg.l2    0.1533706  0.0646002   2.374  0.01777 *  \njyp.l2   0.1381693  0.0551555   2.505  0.01240 *  \nhybe.l3  0.0088203  0.0122629   0.719  0.47214    \nsm.l3    0.0991813  0.0361316   2.745  0.00616 ** \nyg.l3   -0.1022738  0.0651575  -1.570  0.11681    \njyp.l3  -0.0491761  0.0553459  -0.889  0.37447    \nhybe.l4 -0.0071445  0.0118798  -0.601  0.54771    \nsm.l4   -0.1109944  0.0361743  -3.068  0.00221 ** \nyg.l4    0.1607349  0.0648861   2.477  0.01340 *  \njyp.l4   0.0908549  0.0546088   1.664  0.09647 .  \nhybe.l5 -0.0001754  0.0087278  -0.020  0.98397    \nsm.l5    0.0571925  0.0258284   2.214  0.02703 *  \nyg.l5   -0.1325737  0.0488475  -2.714  0.00676 ** \njyp.l5  -0.0738799  0.0414302  -1.783  0.07484 .  \nconst   -0.1891913  0.2063761  -0.917  0.35950    \ntrend    0.0011485  0.0003600   3.190  0.00146 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.117 on 1025 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9968 \nF-statistic: 1.531e+04 on 21 and 1025 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 18.656 2.1148 2.0584 1.8330\nsm    2.115 2.6519 0.7396 0.9206\nyg    2.058 0.7396 0.9670 0.6693\njyp   1.833 0.9206 0.6693 1.2474\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.3007 0.4846 0.3800\nsm   0.3007 1.0000 0.4619 0.5061\nyg   0.4846 0.4619 1.0000 0.6094\njyp  0.3800 0.5061 0.6094 1.0000\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don’t see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM.\nThus, before we continue with the model, we will also verify through a CV test.\n\n\nCross Validation:\n\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df_ts[-(start:end), ]\n  test_model &lt;- df_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n[1] \"The best model is: \"\n\nprint(best_model)\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n 0.898826902 -0.129257139  0.240081729 -0.195873591  0.086817669  0.112162459 \n       yg.l2       jyp.l2        const        trend \n-0.145841108  0.163410772  0.119069039  0.001963864 \n\n\nEstimated coefficients for equation sm: \n======================================= \nCall:\nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003282289  0.979354101 -0.124633554 -0.030561719  0.006619618 -0.001280822 \n       yg.l2       jyp.l2        const        trend \n 0.099485625  0.041169059  0.424147339  0.001889801 \n\n\nEstimated coefficients for equation yg: \n======================================= \nCall:\nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n      hybe.l1         sm.l1         yg.l1        jyp.l1       hybe.l2 \n-0.0017546456 -0.0129013663  0.8872940815 -0.0132784925  0.0020911541 \n        sm.l2         yg.l2        jyp.l2         const         trend \n 0.0157819564  0.1016893422  0.0123618307  0.2539521440  0.0001621699 \n\n\nEstimated coefficients for equation jyp: \n======================================== \nCall:\njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n-0.003714104 -0.013862592 -0.042708139  0.845587797  0.003228421 -0.002809650 \n       yg.l2       jyp.l2        const        trend \n 0.065350796  0.141050674 -0.223955478  0.002160854 \n\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. ### Model Creation:\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 271.32, df = 160, p-value = 9.23e-08\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 154.19, df = 112, p-value = 0.005085\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies.\n\n\nForecasting:\n\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction.\n\n\npart 2:\n\n\nCode\n#Creating a subset of only Korean Record label stock data\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(WMG, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nwmg &lt;- ts(df2$WMG_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, wmg)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"wmg\")\n\nautoplot(df2_ts)\n\n\n\n\n\nFrom an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we’ll continue with the VAR model to work on forecasting.\n\nVARselect\n\n\nCode\nVARselect(df2_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      5      1      5 \n\n$criteria\n                 1          2           3           4           5           6\nAIC(n) -2.79861545 -2.8303916 -2.83985372 -2.86007366 -2.87984152 -2.87067434\nHQ(n)  -2.77157093 -2.7871204 -2.78035578 -2.78434901 -2.78789016 -2.76249626\nSC(n)  -2.72731952 -2.7163181 -2.68300269 -2.66044508 -2.63743538 -2.58549064\nFPE(n)  0.06089433  0.0589898  0.05843435  0.05726482  0.05614415  0.05666151\n                 7           8           9          10\nAIC(n) -2.86626490 -2.86949081 -2.85918122 -2.85197621\nHQ(n)  -2.74186011 -2.72885931 -2.70232301 -2.67889129\nSC(n)  -2.53830365 -2.49875201 -2.44566486 -2.39568230\nFPE(n)  0.05691233  0.05672957  0.05731815  0.05773348\n\n\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let’s continue by analyzing the residuals squared errors.\n\n\nInitial selection:\n\n\nCode\nsummary(vars::VAR(df2_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, wmg \nDeterministic variables: both \nSample size: 1050 \nLog Likelihood: -3020.018 \nRoots of the characteristic polynomial:\n0.9954 0.9811 0.9811\nCall:\nvars::VAR(y = df2_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9874002  0.0038359 257.409  &lt; 2e-16 ***\numgp.l1 -0.4134390  0.2988988  -1.383  0.16690    \nwmg.l1   0.1691336  0.0398344   4.246 2.37e-05 ***\nconst   -3.5345321  1.2007940  -2.943  0.00332 ** \ntrend    0.0009025  0.0005039   1.791  0.07356 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.548 on 1045 degrees of freedom\nMultiple R-Squared: 0.991,  Adjusted R-squared: 0.991 \nF-statistic: 2.89e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  1.060e-04  7.167e-05   1.479    0.139    \numgp.l1  9.853e-01  5.584e-03 176.445   &lt;2e-16 ***\nwmg.l1   1.426e-04  7.442e-04   0.192    0.848    \nconst   -1.131e-02  2.243e-02  -0.504    0.614    \ntrend   -5.273e-06  9.414e-06  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08497 on 1045 degrees of freedom\nMultiple R-Squared: 0.9817, Adjusted R-squared: 0.9816 \nF-statistic: 1.401e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wmg: \n==================================== \nwmg = hybe.l1 + umgp.l1 + wmg.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -3.521e-04  5.531e-04  -0.636  0.52459    \numgp.l1  1.001e-01  4.310e-02   2.324  0.02034 *  \nwmg.l1   9.848e-01  5.744e-03 171.454  &lt; 2e-16 ***\nconst    5.507e-01  1.731e-01   3.181  0.00151 ** \ntrend   -9.905e-05  7.266e-05  -1.363  0.17310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6558 on 1045 degrees of freedom\nMultiple R-Squared: 0.9852, Adjusted R-squared: 0.9851 \nF-statistic: 1.734e+04 on 4 and 1045 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          hybe      umgp      wmg\nhybe 20.683699 -0.009843 0.023501\numgp -0.009843  0.007220 0.001463\nwmg   0.023501  0.001463 0.430030\n\nCorrelation matrix of residuals:\n         hybe     umgp     wmg\nhybe  1.00000 -0.02547 0.00788\numgp -0.02547  1.00000 0.02625\nwmg   0.00788  0.02625 1.00000\n\n\nCode\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, wmg \nDeterministic variables: both \nSample size: 1046 \nLog Likelihood: -2889.659 \nRoots of the characteristic polynomial:\n0.995 0.9817 0.9817 0.6589 0.6499 0.6499 0.6072 0.6072 0.5731 0.5731 0.5694 0.531 0.531 0.4435 0.4147\nCall:\nvars::VAR(y = df2_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8536278  0.0308478  27.672  &lt; 2e-16 ***\numgp.l1 -1.0652459  1.6099455  -0.662 0.508334    \nwmg.l1   0.2683161  0.2049836   1.309 0.190839    \nhybe.l2  0.1477791  0.0407233   3.629 0.000299 ***\numgp.l2  2.4380126  2.1402853   1.139 0.254924    \nwmg.l2   0.0342823  0.2713718   0.126 0.899496    \nhybe.l3  0.0827366  0.0409220   2.022 0.043454 *  \numgp.l3 -1.5539205  2.0986071  -0.740 0.459194    \nwmg.l3   0.1894419  0.2721772   0.696 0.486571    \nhybe.l4  0.0347094  0.0398020   0.872 0.383384    \numgp.l4  2.6506087  2.1383552   1.240 0.215423    \nwmg.l4  -0.1359822  0.2717097  -0.500 0.616852    \nhybe.l5 -0.1286473  0.0294432  -4.369 1.37e-05 ***\numgp.l5 -2.8808918  1.6099556  -1.789 0.073841 .  \nwmg.l5  -0.2292495  0.2059511  -1.113 0.265914    \nconst   -2.3625235  1.1766427  -2.008 0.044920 *  \ntrend    0.0004436  0.0004824   0.920 0.357984    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.285 on 1029 degrees of freedom\nMultiple R-Squared: 0.9922, Adjusted R-squared: 0.992 \nF-statistic:  8136 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -3.726e-04  5.956e-04  -0.625  0.53179    \numgp.l1  8.751e-01  3.109e-02  28.152  &lt; 2e-16 ***\nwmg.l1   5.793e-03  3.958e-03   1.464  0.14358    \nhybe.l2  2.137e-04  7.863e-04   0.272  0.78582    \numgp.l2  4.105e-04  4.133e-02   0.010  0.99208    \nwmg.l2  -1.157e-02  5.240e-03  -2.209  0.02742 *  \nhybe.l3  8.423e-05  7.901e-04   0.107  0.91513    \numgp.l3  2.658e-01  4.052e-02   6.560 8.52e-11 ***\nwmg.l3   5.096e-03  5.255e-03   0.970  0.33239    \nhybe.l4  9.731e-05  7.685e-04   0.127  0.89927    \numgp.l4 -7.039e-02  4.129e-02  -1.705  0.08854 .  \nwmg.l4   2.943e-03  5.246e-03   0.561  0.57497    \nhybe.l5  8.739e-05  5.685e-04   0.154  0.87785    \numgp.l5 -8.534e-02  3.109e-02  -2.745  0.00615 ** \nwmg.l5  -2.154e-03  3.977e-03  -0.542  0.58823    \nconst   -1.084e-02  2.272e-02  -0.477  0.63331    \ntrend   -5.483e-06  9.315e-06  -0.589  0.55625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08274 on 1029 degrees of freedom\nMultiple R-Squared: 0.9829, Adjusted R-squared: 0.9826 \nF-statistic:  3691 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wmg: \n==================================== \nwmg = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  3.423e-03  4.676e-03   0.732  0.46436    \numgp.l1 -1.020e-01  2.440e-01  -0.418  0.67612    \nwmg.l1   8.640e-01  3.107e-02  27.806  &lt; 2e-16 ***\nhybe.l2 -1.977e-03  6.173e-03  -0.320  0.74882    \numgp.l2  4.848e-01  3.244e-01   1.494  0.13540    \nwmg.l2   1.159e-01  4.114e-02   2.817  0.00493 ** \nhybe.l3  4.238e-04  6.203e-03   0.068  0.94554    \numgp.l3 -3.255e-01  3.181e-01  -1.023  0.30643    \nwmg.l3   9.549e-02  4.126e-02   2.315  0.02083 *  \nhybe.l4 -6.184e-04  6.033e-03  -0.103  0.91838    \numgp.l4 -2.872e-01  3.241e-01  -0.886  0.37574    \nwmg.l4  -1.480e-02  4.119e-02  -0.359  0.71950    \nhybe.l5 -1.578e-03  4.463e-03  -0.354  0.72372    \numgp.l5  3.420e-01  2.440e-01   1.401  0.16139    \nwmg.l5  -7.780e-02  3.122e-02  -2.492  0.01286 *  \nconst    6.116e-01  1.784e-01   3.429  0.00063 ***\ntrend   -1.121e-04  7.313e-05  -1.533  0.12555    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6496 on 1029 degrees of freedom\nMultiple R-Squared: 0.9856, Adjusted R-squared: 0.9854 \nF-statistic:  4406 on 16 and 1029 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           hybe      umgp        wmg\nhybe 18.3621988 -0.011192 -0.0006314\numgp -0.0111920  0.006846  0.0021045\nwmg  -0.0006314  0.002105  0.4219207\n\nCorrelation matrix of residuals:\n           hybe     umgp        wmg\nhybe  1.0000000 -0.03157 -0.0002268\numgp -0.0315674  1.00000  0.0391586\nwmg  -0.0002268  0.03916  1.0000000\n\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and WMG are very low, however the error for HYBE is larger at at approximately 4. Thus, we’ll continue model selection through cross validation.\n\n\nCross Validation:\n\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df2_ts[-(start:end), ]\n  test_model &lt;- df2_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n[1] \"The best model is: \"\n\nprint(best_model)\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n 0.8757353675 -0.5532816086  0.3870418011  0.1403299563  4.5910405882 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n-0.8441527930  0.0350757965 -4.3266350264  0.3958843519  0.0166587539 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n 4.3390282814 -0.1523697268  0.0008273759 -3.3109233541 -0.1747831988 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-0.1691034486 -1.9451291119  0.9313591689  0.0852656719  3.9147342098 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n-0.5665090451  0.0021287351 -3.2067566290  0.1318396425 -1.2217738198 \n        trend \n 0.0006281570 \n\n\nEstimated coefficients for equation umgp: \n========================================= \nCall:\numgp = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n-3.392028e-04  9.557157e-01  7.370151e-03  1.061674e-03 -8.060659e-02 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n-1.277957e-02 -6.255746e-04  2.753141e-01  6.371838e-03  1.307248e-04 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n-7.821239e-02 -9.238434e-04  2.612030e-04 -1.501987e-01 -3.912750e-04 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-1.531754e-03  1.617081e-01 -1.042985e-03  2.553486e-03 -8.735351e-02 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n-8.682740e-03 -1.434416e-03 -9.115995e-03  1.003801e-02 -1.173192e-03 \n        trend \n-7.565872e-06 \n\n\nEstimated coefficients for equation wmg: \n======================================== \nCall:\nwmg = hybe.l1 + umgp.l1 + wmg.l1 + hybe.l2 + umgp.l2 + wmg.l2 + hybe.l3 + umgp.l3 + wmg.l3 + hybe.l4 + umgp.l4 + wmg.l4 + hybe.l5 + umgp.l5 + wmg.l5 + hybe.l6 + umgp.l6 + wmg.l6 + hybe.l7 + umgp.l7 + wmg.l7 + hybe.l8 + umgp.l8 + wmg.l8 + const + trend \n\n      hybe.l1       umgp.l1        wmg.l1       hybe.l2       umgp.l2 \n 9.118657e-03 -3.762958e-02  9.150917e-01 -9.246550e-03  5.127581e-01 \n       wmg.l2       hybe.l3       umgp.l3        wmg.l3       hybe.l4 \n 3.452749e-02 -2.725464e-05 -3.853557e-01  6.970382e-02  6.300075e-07 \n      umgp.l4        wmg.l4       hybe.l5       umgp.l5        wmg.l5 \n-1.524228e-01  9.323498e-03  2.290240e-03  1.809426e-01 -5.698194e-02 \n      hybe.l6       umgp.l6        wmg.l6       hybe.l7       umgp.l7 \n-9.464415e-03  4.173369e-01 -6.757319e-03  2.260679e-02 -1.098781e+00 \n       wmg.l7       hybe.l8       umgp.l8        wmg.l8         const \n 1.928490e-02 -1.605151e-02  6.649618e-01 -3.901367e-03  7.781349e-01 \n        trend \n-1.467307e-04 \n\n\nCV seems to have chosen a different model where p=8. Thus, we’ll create models for p=1,5,8.\n\n\nModel Creation:\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 264.14, df = 99, p-value &lt; 2.2e-16\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\") \n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 88.378, df = 63, p-value = 0.01918\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_3 &lt;- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_3\nChi-squared = 42.802, df = 36, p-value = 0.2023\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"wmg\")\n\n\n\n\n\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 &lt; 0.05.\n\n\nForecasting:\n\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and WMG, while UMGP’s stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea.\nKnowing this downward trend in the stock prices of the biggest performing KPOP music agency, we can start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea.\n\n\n\nPart 3:\nLet’s see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting.\n\nGathering the Data\nWe’ll combine the globalization index data from KOF with the South Korean tourism data from Statistica.\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP’s influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture.\n\n\nCode\nglobal_ts &lt;-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n\n\n\n\n\nFrom the graphs above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020.\n\n\nUsing Auto.Arima()\nNow, let’s move on with the ARIMAX/ARMAX model. First, we’ll create a model using auto.arima().\n\nfit &lt;- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  intercept  xreg\n      1.6838  -0.7370    87.7759     0\ns.e.  0.1365   0.1434     0.4453     0\n\nsigma^2 = 2.666:  log likelihood = -38.14\nAIC=86.28   AICc=90.57   BIC=91.26\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 0.2527717 1.460494 1.089386 0.2644077 1.265622 0.9198188 0.1042156\n\ncheckresiduals(fit)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,0) errors\nQ* = 7.4991, df = 3, p-value = 0.05758\n\nModel df: 2.   Total lags used: 5\n\n\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant.\n\n\nManually Finding the Model:\nWe’ll move now to find the ARMAX model manually. Let’s start by taking creating a regression model of tourism on cultural globalization. Using that model, we’ll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we’ll validate the model through cross validation.\n\n\nCode\ndf3$tourists &lt;-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf &lt;-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg &lt;- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = KOFCuGIdf ~ tourists, data = df3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2468 -4.0753  0.9324  3.8493  8.2528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.783e+01  3.063e+00   25.41  1.5e-15 ***\ntourists    1.202e-06  2.918e-07    4.12 0.000644 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.468 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006436\n\n\n\nres.fit&lt;-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\n\n\n\nggPacf(res.fit)\n\n\n\n\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data.\n\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n127.2329\n130.2201\n128.7329\n\n\n0\n0\n1\n116.8798\n120.8627\n119.5464\n\n\n0\n0\n2\n111.9461\n116.9248\n116.2318\n\n\n0\n0\n3\n109.5787\n115.5531\n116.0402\n\n\n0\n0\n4\n111.3460\n118.3162\n120.6794\n\n\n1\n0\n0\n106.7209\n110.7039\n109.3876\n\n\n1\n0\n1\n108.4723\n113.4510\n112.7580\n\n\n1\n0\n2\n108.4037\n114.3781\n114.8653\n\n\n1\n0\n3\n110.3598\n117.3299\n119.6931\n\n\n1\n0\n4\n112.3462\n120.3121\n125.4371\n\n\n2\n0\n0\n108.2759\n113.2546\n112.5616\n\n\n2\n0\n1\n105.2455\n111.2199\n111.7071\n\n\n2\n0\n2\n104.8959\n111.8660\n114.2292\n\n\n2\n0\n3\n112.2294\n120.1952\n125.3203\n\n\n2\n0\n4\n114.0108\n122.9724\n132.0108\n\n\n3\n0\n0\n107.7495\n113.7239\n114.2110\n\n\n3\n0\n1\n105.8867\n112.8568\n115.2200\n\n\n3\n0\n2\n112.1006\n120.0664\n125.1915\n\n\n3\n0\n3\n113.3105\n122.2721\n131.3105\n\n\n3\n0\n4\n115.3092\n125.2666\n139.7537\n\n\n4\n0\n0\n109.2205\n116.1906\n118.5538\n\n\n4\n0\n1\n111.1023\n119.0682\n124.1932\n\n\n4\n0\n2\n113.2815\n122.2431\n131.2815\n\n\n4\n0\n3\n111.4614\n121.4187\n135.9058\n\n\n4\n0\n4\n111.5921\n122.5452\n144.5921\n\n\n\n\n\n\noutput[which.min(output$AIC),] \n\n   p d q      AIC     BIC     AICc\n13 2 0 2 104.8959 111.866 114.2292\n\noutput[which.min(output$BIC),] \n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\noutput[which.min(output$AICc),]\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\n\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0). Thus, we’ll take a look at the residuals of the following models:\n\ncapture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n [1] \"initial  value 1.585318 \"                                                              \n [2] \"iter   2 value 1.000819\"                                                               \n [3] \"iter   3 value 0.979873\"                                                               \n [4] \"iter   4 value 0.971786\"                                                               \n [5] \"iter   5 value 0.970922\"                                                               \n [6] \"iter   6 value 0.970615\"                                                               \n [7] \"iter   7 value 0.970607\"                                                               \n [8] \"iter   8 value 0.970607\"                                                               \n [9] \"iter   8 value 0.970607\"                                                               \n[10] \"iter   8 value 0.970607\"                                                               \n[11] \"final  value 0.970607 \"                                                                \n[12] \"converged\"                                                                             \n[13] \"initial  value 1.116539 \"                                                              \n[14] \"iter   2 value 1.064095\"                                                               \n[15] \"iter   3 value 1.062560\"                                                               \n[16] \"iter   4 value 1.061681\"                                                               \n[17] \"iter   5 value 1.061650\"                                                               \n[18] \"iter   6 value 1.061650\"                                                               \n[19] \"iter   6 value 1.061650\"                                                               \n[20] \"final  value 1.061650 \"                                                                \n[21] \"converged\"                                                                             \n[22] \"$fit\"                                                                                  \n[23] \"\"                                                                                      \n[24] \"Call:\"                                                                                 \n[25] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[26] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[27] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[28] \"\"                                                                                      \n[29] \"Coefficients:\"                                                                         \n[30] \"         ar1    xmean\"                                                                 \n[31] \"      0.8652  -2.5386\"                                                                 \n[32] \"s.e.  0.1038   3.8694\"                                                                 \n[33] \"\"                                                                                      \n[34] \"sigma^2 estimated as 7.801:  log likelihood = -49.61,  aic = 105.22\"                   \n[35] \"\"                                                                                      \n[36] \"$degrees_of_freedom\"                                                                   \n[37] \"[1] 18\"                                                                                \n[38] \"\"                                                                                      \n[39] \"$ttable\"                                                                               \n[40] \"      Estimate     SE t.value p.value\"                                                 \n[41] \"ar1     0.8652 0.1038  8.3328  0.0000\"                                                 \n[42] \"xmean  -2.5386 3.8694 -0.6561  0.5201\"                                                 \n[43] \"\"                                                                                      \n[44] \"$AIC\"                                                                                  \n[45] \"[1] 5.261176\"                                                                          \n[46] \"\"                                                                                      \n[47] \"$AICc\"                                                                                 \n[48] \"[1] 5.29647\"                                                                           \n[49] \"\"                                                                                      \n[50] \"$BIC\"                                                                                  \n[51] \"[1] 5.410536\"                                                                          \n[52] \"\"                                                                                      \n\ncapture.output(sarima(res.fit, 2,0,2)) \n\n\n\n\n [1] \"initial  value 1.553778 \"                                                              \n [2] \"iter   2 value 1.430949\"                                                               \n [3] \"iter   3 value 1.042676\"                                                               \n [4] \"iter   4 value 0.993029\"                                                               \n [5] \"iter   5 value 0.974081\"                                                               \n [6] \"iter   6 value 0.956070\"                                                               \n [7] \"iter   7 value 0.949449\"                                                               \n [8] \"iter   8 value 0.944797\"                                                               \n [9] \"iter   9 value 0.942595\"                                                               \n[10] \"iter  10 value 0.941319\"                                                               \n[11] \"iter  11 value 0.940887\"                                                               \n[12] \"iter  12 value 0.940866\"                                                               \n[13] \"iter  13 value 0.940866\"                                                               \n[14] \"iter  14 value 0.940865\"                                                               \n[15] \"iter  15 value 0.940862\"                                                               \n[16] \"iter  16 value 0.940853\"                                                               \n[17] \"iter  17 value 0.940845\"                                                               \n[18] \"iter  18 value 0.940830\"                                                               \n[19] \"iter  19 value 0.940820\"                                                               \n[20] \"iter  20 value 0.940818\"                                                               \n[21] \"iter  21 value 0.940816\"                                                               \n[22] \"iter  22 value 0.940816\"                                                               \n[23] \"iter  22 value 0.940816\"                                                               \n[24] \"iter  22 value 0.940816\"                                                               \n[25] \"final  value 0.940816 \"                                                                \n[26] \"converged\"                                                                             \n[27] \"initial  value 1.014365 \"                                                              \n[28] \"iter   2 value 1.000713\"                                                               \n[29] \"iter   3 value 0.998434\"                                                               \n[30] \"iter   4 value 0.997970\"                                                               \n[31] \"iter   5 value 0.997732\"                                                               \n[32] \"iter   6 value 0.997580\"                                                               \n[33] \"iter   7 value 0.997379\"                                                               \n[34] \"iter   8 value 0.997048\"                                                               \n[35] \"iter   9 value 0.996543\"                                                               \n[36] \"iter  10 value 0.995733\"                                                               \n[37] \"iter  11 value 0.991400\"                                                               \n[38] \"iter  12 value 0.985956\"                                                               \n[39] \"iter  13 value 0.981444\"                                                               \n[40] \"iter  14 value 0.974812\"                                                               \n[41] \"iter  15 value 0.971043\"                                                               \n[42] \"iter  16 value 0.960387\"                                                               \n[43] \"iter  17 value 0.955391\"                                                               \n[44] \"iter  18 value 0.944255\"                                                               \n[45] \"iter  19 value 0.918264\"                                                               \n[46] \"iter  20 value 0.911566\"                                                               \n[47] \"iter  21 value 0.908276\"                                                               \n[48] \"iter  22 value 0.904759\"                                                               \n[49] \"iter  23 value 0.902341\"                                                               \n[50] \"iter  24 value 0.902240\"                                                               \n[51] \"iter  25 value 0.902160\"                                                               \n[52] \"iter  26 value 0.901582\"                                                               \n[53] \"iter  27 value 0.900613\"                                                               \n[54] \"iter  28 value 0.900164\"                                                               \n[55] \"iter  29 value 0.900127\"                                                               \n[56] \"iter  30 value 0.900121\"                                                               \n[57] \"iter  31 value 0.900121\"                                                               \n[58] \"iter  32 value 0.900121\"                                                               \n[59] \"iter  32 value 0.900121\"                                                               \n[60] \"iter  32 value 0.900121\"                                                               \n[61] \"final  value 0.900121 \"                                                                \n[62] \"converged\"                                                                             \n[63] \"$fit\"                                                                                  \n[64] \"\"                                                                                      \n[65] \"Call:\"                                                                                 \n[66] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[67] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[68] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[69] \"\"                                                                                      \n[70] \"Coefficients:\"                                                                         \n[71] \"         ar1      ar2      ma1     ma2   xmean\"                                        \n[72] \"      1.8206  -0.9472  -1.2918  0.2918  0.4865\"                                        \n[73] \"s.e.  0.0631   0.0603   0.3380  0.2889  0.8530\"                                        \n[74] \"\"                                                                                      \n[75] \"sigma^2 estimated as 4.709:  log likelihood = -46.38,  aic = 104.76\"                   \n[76] \"\"                                                                                      \n[77] \"$degrees_of_freedom\"                                                                   \n[78] \"[1] 15\"                                                                                \n[79] \"\"                                                                                      \n[80] \"$ttable\"                                                                               \n[81] \"      Estimate     SE  t.value p.value\"                                                \n[82] \"ar1     1.8206 0.0631  28.8298  0.0000\"                                                \n[83] \"ar2    -0.9472 0.0603 -15.7180  0.0000\"                                                \n[84] \"ma1    -1.2918 0.3380  -3.8223  0.0017\"                                                \n[85] \"ma2     0.2918 0.2889   1.0100  0.3285\"                                                \n[86] \"xmean   0.4865 0.8530   0.5703  0.5769\"                                                \n[87] \"\"                                                                                      \n[88] \"$AIC\"                                                                                  \n[89] \"[1] 5.238119\"                                                                          \n[90] \"\"                                                                                      \n[91] \"$AICc\"                                                                                 \n[92] \"[1] 5.452405\"                                                                          \n[93] \"\"                                                                                      \n[94] \"$BIC\"                                                                                  \n[95] \"[1] 5.536838\"                                                                          \n[96] \"\"                                                                                      \n\n\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we’ll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting.\n\n\nCV\n\nn &lt;- length(res.fit)\nk &lt;- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 &lt;- matrix(NA, 15)\nrmse2 &lt;- matrix(NA, 15)\nrmse3 &lt;- matrix(NA, 15)\n\nst &lt;- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end &lt;- st + i - 1\n  xtrain &lt;- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start &lt;- train_end + 1\n  test_end &lt;- min(st + i, tsp(res.fit)[2])\n  xtest &lt;- window(res.fit, start = test_start, end = test_end)\n\n  fit &lt;- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast &lt;- forecast(fit, h = 4)\n\n  fit2 &lt;- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 &lt;- forecast(fit2, h = 4)\n\n  fit3 &lt;- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 &lt;- forecast(fit3, h = 4)\n\n  rmse1[i] &lt;- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] &lt;- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] &lt;- sqrt((fcast3$mean - xtest)^2)\n}\n\nWarning in sqrt(z[[2L]] * object$sigma2): NaNs produced\n\n\nWarning in forecast.forecast_ARIMA(fit3, h = 4): Upper prediction intervals are\nnot finite.\n\nWarning in forecast.forecast_ARIMA(fit3, h = 4): Upper prediction intervals are\nnot finite.\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n\n\n\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we’ll choose to forecast Korean tourism on cultural globalization in the US via model 1.\n\nfit &lt;- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept  xreg\n      0.9684    84.7153     0\ns.e.  0.0398     1.3007     0\n\nsigma^2 = 5.577:  log likelihood = -45.33\nAIC=98.66   AICc=101.32   BIC=102.64\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE    MASE     ACF1\nTraining set 0.8955075 2.177229 1.295393 0.9914592 1.500592 1.09376 0.462599\n\n\n\n\nForecasting:\n\ntourists_fit &lt;-auto.arima(global_ts[, \"tourists\"]) \nsummary(tourists_fit)\n\nSeries: global_ts[, \"tourists\"] \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1     drift\n      -0.5710  627170.2\ns.e.   0.1837  185981.8\n\nsigma^2 = 1.743e+12:  log likelihood = -293.87\nAIC=593.75   AICc=595.35   BIC=596.58\n\nTraining set error measures:\n                    ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set -15386.25 1217172 961194.1 -2.96541 9.812744 0.7784607 -0.1121698\n\nft&lt;-forecast(tourists_fit)\n\nfcast &lt;- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n\n\n\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea’s tourism of foreigners will see a slight decrease. As we’ve observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done.\n\n\n\nSARIMA:\nIn order to understand globalization within the USA through the lens on South Korea, we will need to also look at the tourism coming into SK from abroad. An example of this is using monthly air travel passeneger data into Incheon Airport, South Korea’s largest international airport. Specifically, we’ll be focusing on international flights coming into South Korea as they best represent foreign interest of the nation.\n\nSeasonal Anlysis\n\n\nCode\nsk_passengers &lt;- read_xlsx('sk_passenger_arrivals.xlsx')\n\nsk_passengers &lt;- sk_passengers %&gt;%\n  unite(date, year, month, sep = '-') %&gt;%\n  mutate(date = as.Date(paste(date, '01', sep = '-'))) %&gt;%\n  filter(year(date) &lt; 2020) #In order to avoid the anomaly of the 2020 pandemic\n\nair_travel_ts &lt;- ts(sk_passengers$Passengers, start = c(2010, 10), \n                    frequency = 12)\n\nautoplot(air_travel_ts)+ggtitle(\"Air passenger arrivals to Incheon Airport (SK)\") \n\n\n\n\n\nFrom the plot above, we can see that there is a seasonal pattern of passengers coming into the country yearly. Please note that to avoid anomalies in forecasting, we’ll not be training on 2020 data due to the restrictions on travel from the COVID-19 global pandemic.\n\nggAcf(air_travel_ts,40)\n\n\n\ngglagplot(air_travel_ts, do.lines=FALSE, set.lags = c(12, 24, 36, 48))+\n  ggtitle(\"Air passenger arrivals to Incheon Airport (SK)\") \n\n\n\ndec2 &lt;- decompose(air_travel_ts,type = c(\"additive\", \"multiplicative\"))\nplot(dec2)\n\n\n\n\nThrough these graphs, we can see a string seasonal correlation. However, since the data is mutliplicative in nature, we may need to take the log of the data prior to differencing.\n\n\nCode\nlog(air_travel_ts) %&gt;% diff() %&gt;% ggtsdisplay()\n\n\n\n\n\nCode\nlog(air_travel_ts) %&gt;% diff(12) %&gt;% ggtsdisplay()\n\n\n\n\n\nCode\nlog(air_travel_ts) %&gt;% diff() %&gt;% diff(12) %&gt;% ggtsdisplay()\n\n\n\n\n\nFrom the ACF and PACF plots, we can see that the data is the most stationary when the original data is log transformed, first differenced, and seasonally differenced by month. From these plots, we can say that q = 2, p = 2,4, d = 1, Q = 1, and P = 1.\nIn order to find the best model, we’ll run through mutliple models and find the ones with the lowest AIC and BIC score.\n\n\nDetermining the Model\n\n\nCode\n#write a function\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=5,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=air_travel_ts)\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n2574.346\n2576.931\n2574.387\n\n\n0\n1\n0\n0\n1\n1\n2546.796\n2551.966\n2546.922\n\n\n0\n1\n0\n0\n1\n2\n2548.444\n2556.199\n2548.700\n\n\n0\n1\n0\n1\n1\n0\n2556.745\n2561.915\n2556.871\n\n\n0\n1\n0\n1\n1\n1\n2548.420\n2556.174\n2548.675\n\n\n0\n1\n0\n1\n1\n2\n2550.394\n2560.734\n2550.824\n\n\n0\n1\n0\n2\n1\n0\n2552.772\n2560.527\n2553.028\n\n\n0\n1\n0\n2\n1\n1\n2550.382\n2560.722\n2550.812\n\n\n0\n1\n1\n0\n1\n0\n2574.807\n2579.977\n2574.933\n\n\n0\n1\n1\n0\n1\n1\n2543.326\n2551.081\n2543.581\n\n\n0\n1\n1\n0\n1\n2\n2545.179\n2555.519\n2545.609\n\n\n0\n1\n1\n1\n1\n0\n2555.892\n2563.646\n2556.147\n\n\n0\n1\n1\n1\n1\n1\n2545.173\n2555.513\n2545.603\n\n\n0\n1\n1\n2\n1\n0\n2549.534\n2559.874\n2549.964\n\n\n0\n1\n2\n0\n1\n0\n2562.654\n2570.409\n2562.910\n\n\n0\n1\n2\n0\n1\n1\n2533.345\n2543.685\n2533.775\n\n\n0\n1\n2\n1\n1\n0\n2538.189\n2548.528\n2538.619\n\n\n1\n1\n0\n0\n1\n0\n2575.603\n2580.773\n2575.729\n\n\n1\n1\n0\n0\n1\n1\n2546.295\n2554.050\n2546.551\n\n\n1\n1\n0\n0\n1\n2\n2547.917\n2558.257\n2548.347\n\n\n1\n1\n0\n1\n1\n0\n2557.701\n2565.456\n2557.956\n\n\n1\n1\n0\n1\n1\n1\n2547.901\n2558.241\n2548.332\n\n\n1\n1\n0\n2\n1\n0\n2552.662\n2563.002\n2553.092\n\n\n1\n1\n1\n0\n1\n0\n2562.140\n2569.895\n2562.395\n\n\n1\n1\n1\n0\n1\n1\n2535.527\n2545.867\n2535.957\n\n\n1\n1\n1\n1\n1\n0\n2544.476\n2554.816\n2544.907\n\n\n1\n1\n2\n0\n1\n0\n2562.137\n2572.477\n2562.567\n\n\n2\n1\n0\n0\n1\n0\n2571.112\n2578.867\n2571.368\n\n\n2\n1\n0\n0\n1\n1\n2541.018\n2551.358\n2541.448\n\n\n2\n1\n0\n1\n1\n0\n2549.599\n2559.939\n2550.029\n\n\n2\n1\n1\n0\n1\n0\n2562.815\n2573.155\n2563.245\n\n\n3\n1\n0\n0\n1\n0\n2572.829\n2583.169\n2573.260\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n16 0 1 2 0 1 1 2533.345 2543.685 2533.775\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n16 0 1 2 0 1 1 2533.345 2543.685 2533.775\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n16 0 1 2 0 1 1 2533.345 2543.685 2533.775\n\n\nBased on the AIC, BIC, and AICc values, we’ll go forward with the model ARIMA(0,1,2)x(0,1,1)[12]. Let’s analyze this model for it’s residuals and siginificant values.\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(air_travel_ts, 0,1,2,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[28:60], model_output[length(model_output)], sep = \"\\n\") \n\n\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1      ma2     sma1\n      -0.2648  -0.4423  -0.6406\ns.e.   0.1007   0.1129   0.1102\n\nsigma^2 estimated as 8.456e+09:  log likelihood = -1262.67,  aic = 2533.34\n\n$degrees_of_freedom\n[1] 95\n\n$ttable\n     Estimate     SE t.value p.value\nma1   -0.2648 0.1007 -2.6304   1e-02\nma2   -0.4423 0.1129 -3.9182   2e-04\nsma1  -0.6406 0.1102 -5.8147   0e+00\n\n$AIC\n[1] 25.85046\n\n$AICc\n[1] 25.85306\n\n$BIC\n[1] 25.95596\n\nNA\nNA\nNA\nNA\n\n\nThis model seems to be a goof fit for a number of reasons. Primarily, the ACF plot shows almost no correlation, indicating that the model has harnessed everything that left is white noise. This indicates a good model fit. Additionally, the Ljung-Box statistic shows almost no autocorrelation within the model. Lastly, all coefficients within the table are significant.\n\n\nFitting the Model and Forecasting\n\n\nCode\nfit &lt;- Arima(air_travel_ts, order=c(0,1,2), seasonal=c(0,1,1))\nsummary(fit)\n\n\nSeries: air_travel_ts \nARIMA(0,1,2)(0,1,1)[12] \n\nCoefficients:\n          ma1      ma2     sma1\n      -0.2648  -0.4423  -0.6406\ns.e.   0.1007   0.1129   0.1102\n\nsigma^2 = 8.723e+09:  log likelihood = -1262.67\nAIC=2533.34   AICc=2533.77   BIC=2543.68\n\nTraining set error measures:\n                  ME     RMSE     MAE       MPE     MAPE      MASE         ACF1\nTraining set 5517.28 86406.29 53391.3 0.1750992 2.484263 0.2762785 -0.003659363\n\n\n\nsarima.for(air_travel_ts, 60, 0,1,2,0,1,1,12)\n\n\n\n\n$pred\n         Jan     Feb     Mar     Apr     May     Jun     Jul     Aug     Sep\n2020 3207700 3088739 3137521 3008503 3023479 3065866 3158244 3455060 2971110\n2021 3374324 3265209 3313992 3184974 3199950 3242337 3334714 3631530 3147581\n2022 3550794 3441680 3490462 3361444 3376420 3418807 3511185 3808001 3324051\n2023 3727265 3618150 3666933 3537915 3552891 3595278 3687655 3984471 3500522\n2024 3903735 3794621 3843403 3714385 3729362 3771748 3864126 4160942 3676993\n         Oct     Nov     Dec\n2020 3198943 2994382 3202800\n2021 3375414 3170853 3379270\n2022 3551884 3347324 3555741\n2023 3728355 3523794 3732212\n2024 3904825 3700265 3908682\n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2020  91967.55 114150.96 117287.13 120341.60 123320.43 126228.99 129072.02\n2021 154564.85 162841.24 166909.38 170880.70 174761.80 178558.55 182276.24\n2022 211638.14 220236.97 225053.12 229768.35 234388.73 238919.78 243366.49\n2023 276140.40 285028.51 290476.20 295823.59 301076.02 306238.37 311315.13\n2024 347172.02 356339.34 362340.94 368244.73 374055.36 379777.09 385413.89\n           Aug       Sep       Oct       Nov       Dec\n2020 131853.76 134579.14 137250.48 139869.37 142440.11\n2021 185919.60 189494.86 193004.62 196448.48 199833.00\n2022 247733.39 252026.94 256249.62 260399.76 264484.79\n2023 316310.42 321230.51 326077.61 330849.20 335552.95\n2024 390969.43 396449.67 401856.51 407186.86 412448.32\n\n\nOur forecast shows a positive upwards trend in international passenger arrivals into Incheon Airport, South Korea. Please note, this forecast shows five years into the future if the COVID-19 pandemic didn’t cause any anomalies within air travel. However, even knowing that the actual data isn’t the same, we can approximate that the travel industry would recover in a similar pattern, with a positive trend of people coming into Korea. Thus, this furthers the narrative of globalization, and specifically, Korean culture reaching outside its country’s borders into the West.\n\n\n\nHomework 5:\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df)), size = 0.8 * nrow(df)) \ntrain &lt;- df[train_indices, ] \ntest &lt;- df[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-87.63 -24.10   0.01  20.72  74.25 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.108e+03  1.769e+02  11.916   &lt;2e-16 ***\nDate        -1.133e-01  9.432e-03 -12.015   &lt;2e-16 ***\nSM_Price     1.475e+00  1.332e-01  11.073   &lt;2e-16 ***\nYG_Price     5.063e+00  2.302e-01  21.990   &lt;2e-16 ***\nJYP_Price   -1.314e+00  1.327e-01  -9.906   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29 on 836 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:   0.63 \nF-statistic: 358.5 on 4 and 836 DF,  p-value: &lt; 2.2e-16\n\n\nSince all the variables are significant, we’ll continue with this model to find its residuals.\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.58667065622527\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 31.5268851064457\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -8.9644, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\nUsing auto.arima\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 836:  log likelihood = -4022.7\nAIC=8047.39   AICc=8047.4   BIC=8052.13\n\nTraining set error measures:\n                       ME     RMSE      MAE MPE MAPE      MASE       ACF1\nTraining set 1.360155e-15 28.91303 23.92612 100  100 0.6951107 -0.0701053\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nManual AR/ARMA/ARIMA Model\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n8049.256\n8063.459\n8049.284\n\n\n0\n0\n1\n8047.069\n8066.007\n8047.116\n\n\n0\n0\n2\n8048.019\n8071.692\n8048.091\n\n\n0\n0\n3\n8049.359\n8077.767\n8049.460\n\n\n0\n0\n4\n8050.912\n8084.054\n8051.046\n\n\n1\n0\n0\n8046.786\n8065.725\n8046.834\n\n\n1\n0\n1\n8047.856\n8071.529\n8047.928\n\n\n1\n0\n2\n8049.746\n8078.154\n8049.847\n\n\n1\n0\n3\n8050.935\n8084.078\n8051.070\n\n\n1\n0\n4\n8052.780\n8090.657\n8052.953\n\n\n2\n0\n0\n8047.880\n8071.553\n8047.952\n\n\n2\n0\n1\n8049.757\n8078.164\n8049.858\n\n\n2\n0\n2\n8051.781\n8084.923\n8051.915\n\n\n2\n0\n3\n8050.634\n8088.511\n8050.807\n\n\n2\n0\n4\n8054.804\n8097.415\n8055.020\n\n\n3\n0\n0\n8049.542\n8077.949\n8049.642\n\n\n3\n0\n1\n8051.002\n8084.144\n8051.136\n\n\n3\n0\n2\n8050.141\n8088.017\n8050.314\n\n\n3\n0\n3\n8055.183\n8097.794\n8055.400\n\n\n3\n0\n4\n8054.533\n8101.879\n8054.798\n\n\n4\n0\n0\n8050.818\n8083.960\n8050.953\n\n\n4\n0\n1\n8052.676\n8090.552\n8052.849\n\n\n4\n0\n2\n8053.909\n8096.520\n8054.125\n\n\n4\n0\n3\n8055.875\n8103.221\n8056.140\n\n\n4\n0\n4\n8046.913\n8098.993\n8047.231\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8049.256 8063.459 8049.284\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\nThus, from the manually approach, the model with the lowest AIC value is AR(1). Thus, we’ll use this model within our approach to find the best ARCH/GARCH model.\n\n\nARCH/GARCH\nSince the original data doesn’t show time-varying volatility within stock prices, we’ll go forward without testing the GARCH model. Thus, the two models we’ll look at is AR + ARCH and just ARCH in order to find the best one at predicting HYBE prices given its volatility.\n\n\nARCH selection\nbest_ar_model &lt;- Arima(lm.residuals,order=c(1,0,0))\nar.res &lt;- best_ar_model$residuals\n\nacf(ar.res^2)\n\n\n\n\n\nARCH selection\npacf(ar.res^2)\n\n\n\n\n\nARCH selection\narch_model &lt;- list() ## set counter\ncc &lt;- 1\n\nfor (p in 1:4) {\n  for (q in 1:4) {\n    arch_model[[cc]] &lt;- garch(ar.res,order=c(q,p),trace=F)\n    cc &lt;- cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(arch_model, AIC) ## model with lowest AIC is the best\narch_model[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = ar.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4  \n6.246e+02  9.202e-12  6.121e-02  6.027e-02  6.121e-02  6.023e-02  \n\n\nFrom the manually calculation, we can see that the best ARCH model is ARCH(1,4). The next step now is to check which whether the AR+ARCH model or the ARCH model itself is the best.\n\n\nModel selection\nsummary(garchFit(~garch(1,4), ar.res, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 4), data = ar.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 4)\n&lt;environment: 0x7fce17201158&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2        beta3  \n-0.00134274  21.05589755   0.00000001   0.97494048   0.00000001   0.00000001  \n      beta4  \n 0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.343e-03   9.945e-01   -0.001    0.999\nomega   2.106e+01   1.290e+01    1.632    0.103\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.749e-01   1.978e+01    0.049    0.961\nbeta2   1.000e-08   1.715e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.602    normalized:  -4.78074 \n\nDescription:\n Wed Nov 22 21:53:37 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  18.0638061 1.195348e-04\n Shapiro-Wilk Test  R    W       0.9847035 1.076037e-07\n Ljung-Box Test     R    Q(10)   5.0485163 8.879136e-01\n Ljung-Box Test     R    Q(15)  23.2427741 7.911959e-02\n Ljung-Box Test     R    Q(20)  27.8650892 1.126317e-01\n Ljung-Box Test     R^2  Q(10)   7.4176133 6.855185e-01\n Ljung-Box Test     R^2  Q(15)   8.7640540 8.895276e-01\n Ljung-Box Test     R^2  Q(20)  18.6867078 5.422662e-01\n LM Arch Test       R    TR^2    7.5249668 8.210648e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.578127 9.617535 9.577990 9.593230 \n\n\nModel selection\nsummary(garchFit(~arma(1, 0) + garch(1, 4), ar.res, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + garch(1, 4), data = ar.res, \n    trace = F) \n\nMean and Variance Equation:\n data ~ arma(1, 0) + garch(1, 4)\n&lt;environment: 0x7fce1d6b79f8&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1        omega       alpha1        beta1        beta2  \n-0.00144214   0.00236649  21.18647953   0.00000001   0.97478384   0.00000001  \n      beta3        beta4  \n 0.00000001   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.442e-03   9.949e-01   -0.001    0.999\nar1     2.366e-03   3.450e-02    0.069    0.945\nomega   2.119e+01   1.295e+01    1.636    0.102\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.748e-01   1.946e+01    0.050    0.960\nbeta2   1.000e-08   1.567e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.441    normalized:  -4.780548 \n\nDescription:\n Wed Nov 22 21:53:37 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  17.9142074 1.288188e-04\n Shapiro-Wilk Test  R    W       0.9847546 1.124557e-07\n Ljung-Box Test     R    Q(10)   5.0407836 8.884370e-01\n Ljung-Box Test     R    Q(15)  23.1202713 8.161946e-02\n Ljung-Box Test     R    Q(20)  27.8526110 1.129345e-01\n Ljung-Box Test     R^2  Q(10)   7.4106948 6.861867e-01\n Ljung-Box Test     R^2  Q(15)   8.7613151 8.896631e-01\n Ljung-Box Test     R^2  Q(20)  18.7545990 5.378250e-01\n LM Arch Test       R    TR^2    7.4989478 8.229593e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.580122 9.625159 9.579943 9.597382 \n\n\nBased on the two models, looking at the Ljung-Box Test and the AIC values, we can say that ARCH(1,4) is the best model to predict HYBE stock prices.\n\n\nEquation:\nBased on the model above, we’ll say that equation for the model is as follows:\n\\(X_t = 2108 -0.1133z_1+ 1.475z_2 + 5.063z_3 - 1.314z_4\\)\n\\(y^*_t = y_t−0.00134274\\)\n\\(y_t = \\sigma_t \\epsilon_t\\)\n\\(\\sigma^2_t = 21.05589755 + 0.00000001y^2_{t-1} + 0.97494048\\sigma^2_{t-1} + 0.00000001\\sigma^2_{t-2} + 0.00000001\\sigma^2_{t-3} + 0.00000001\\sigma^2_{t-4}\\)\n##part 2:\n\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(WMG, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nwmg &lt;- ts(df2$WMG_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, wmg)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"wmg\")\n\nautoplot(df2_ts)\n\n\n\n\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df2)), size = 0.8 * nrow(df2)) \ntrain &lt;- df2[train_indices, ] \ntest &lt;- df2[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-74.588 -28.629  -1.115  28.270  80.866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -230.99952   90.35712  -2.557  0.01075 *  \nDate           0.01327    0.00452   2.935  0.00342 ** \nUMGP_Price    18.53846    2.67387   6.933 8.23e-12 ***\nWMG_Price      4.31327    0.32778  13.159  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.72 on 836 degrees of freedom\nMultiple R-squared:  0.4113,    Adjusted R-squared:  0.4092 \nF-statistic: 194.7 on 3 and 836 DF,  p-value: &lt; 2.2e-16\n\n\nSince all the variables are significant, we’ll continue with this model to find its residuals.\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.441803792246388\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 36.3722980380692\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -9.1461, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\n\nUsing auto.arima\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 1342:  log likelihood = -4216.61\nAIC=8435.21   AICc=8435.22   BIC=8439.94\n\nTraining set error measures:\n                       ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set 2.257879e-15 36.62861 31.29514 100  100 0.7331305 -0.03285608\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nManual AR/ARMA/ARIMA Model\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n8438.693\n8452.894\n8438.722\n\n\n0\n0\n1\n8439.816\n8458.750\n8439.864\n\n\n0\n0\n2\n8440.647\n8464.314\n8440.719\n\n\n0\n0\n3\n8442.630\n8471.031\n8442.731\n\n\n0\n0\n4\n8444.116\n8477.250\n8444.251\n\n\n1\n0\n0\n8439.751\n8458.684\n8439.798\n\n\n1\n0\n1\n8441.086\n8464.753\n8441.158\n\n\n1\n0\n2\n8441.905\n8470.306\n8442.006\n\n\n1\n0\n3\n8443.901\n8477.035\n8444.036\n\n\n1\n0\n4\n8445.168\n8483.035\n8445.341\n\n\n2\n0\n0\n8440.617\n8464.284\n8440.689\n\n\n2\n0\n1\n8441.888\n8470.288\n8441.989\n\n\n2\n0\n2\n8443.870\n8477.004\n8444.004\n\n\n2\n0\n3\n8445.826\n8483.693\n8445.999\n\n\n2\n0\n4\n8446.495\n8489.096\n8446.712\n\n\n3\n0\n0\n8442.566\n8470.966\n8442.667\n\n\n3\n0\n1\n8444.601\n8477.735\n8444.736\n\n\n3\n0\n2\n8445.860\n8483.727\n8446.034\n\n\n3\n0\n3\n8444.828\n8487.429\n8445.045\n\n\n3\n0\n4\n8442.662\n8489.997\n8442.928\n\n\n4\n0\n0\n8444.137\n8477.270\n8444.271\n\n\n4\n0\n1\n8445.375\n8483.243\n8445.549\n\n\n4\n0\n2\n8445.013\n8487.613\n8445.230\n\n\n4\n0\n3\n8441.487\n8488.821\n8441.752\n\n\n4\n0\n4\n8446.850\n8498.918\n8447.169\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nFrom both auto.arima and the manual approach, we can see that no AR/ARMA/ARIMA model was created from the residuals of the linear regression model. Thus, we can say that there is no autocorrelation in the data that would call for an AR/ARMA/ARIMA model.\nFrom this information, we can deduce the fact that Western record labels (Warner Group and Universal Music Group) do not have an affect on HYBE stock prices. Thus, we can say that HYBE’s success and future would best be predicted by the performance of other KPOP record labels (SM, JYP, and YG). Thus, we can say that for the best financial outcomes, music companies may be looking to KPOP groups and their marketing and music strategies in the coming future.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "financial-ts.html#hybe-and-the-kpop-record-labels",
    "href": "financial-ts.html#hybe-and-the-kpop-record-labels",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "HYBE and the KPOP record labels:",
    "text": "HYBE and the KPOP record labels:\n\n\nData Gathering\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"WMG\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nWMG &lt;- data.frame(WMG$WMG.Adjusted)\nWMG &lt;- WMG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(WMG_Price = WMG.Adjusted)\n\n\nstart_date &lt;- as.Date(min(WMG$Date))  \nend_date &lt;- as.Date(max(WMG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nWMG &lt;- merge(WMG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- WMG[which(rowSums(is.na(WMG)) &gt; 0),]\ndf_na_cols &lt;- WMG[, which(colSums(is.na(WMG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(WMG, k = 4, weighting = \"exponential\")\nWMG &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, WMG, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"WMG\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n#Converting to time series \nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n#Visualize\nautoplot(df_ts)\n\n\n\n\n\nFrom the plot, we can see that all four stock prices are not stationary, as they all experience volatility to some extent. In terms of HYBE, we can see the largest magnitude and most volatility in the data, with sharp upward and downward trends in the data. Thus, we’ll see if using “The Big Three” (SM, JYP, YG) and accounting for volatility in an ARCH/GARCH model will provide better predictions.\nThus, to begin the ARCH/GARCH analysis, we’ll first need to create a regression model of these variables.\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df)), size = 0.8 * nrow(df)) \ntrain &lt;- df[train_indices, ] \ntest &lt;- df[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-87.63 -24.10   0.01  20.72  74.25 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.108e+03  1.769e+02  11.916   &lt;2e-16 ***\nDate        -1.133e-01  9.432e-03 -12.015   &lt;2e-16 ***\nSM_Price     1.475e+00  1.332e-01  11.073   &lt;2e-16 ***\nYG_Price     5.063e+00  2.302e-01  21.990   &lt;2e-16 ***\nJYP_Price   -1.314e+00  1.327e-01  -9.906   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29 on 836 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:   0.63 \nF-statistic: 358.5 on 4 and 836 DF,  p-value: &lt; 2.2e-16\n\n\nSince all the variables are significant, we’ll continue with this model to find its residuals.\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.58667065622527\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 31.5268851064457\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -8.9644, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\nUsing auto.arima\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 836:  log likelihood = -4022.7\nAIC=8047.39   AICc=8047.4   BIC=8052.13\n\nTraining set error measures:\n                       ME     RMSE      MAE MPE MAPE      MASE       ACF1\nTraining set 1.360155e-15 28.91303 23.92612 100  100 0.6951107 -0.0701053\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nManual AR/ARMA/ARIMA Model\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n8049.256\n8063.459\n8049.284\n\n\n0\n0\n1\n8047.069\n8066.007\n8047.116\n\n\n0\n0\n2\n8048.019\n8071.692\n8048.091\n\n\n0\n0\n3\n8049.359\n8077.767\n8049.460\n\n\n0\n0\n4\n8050.912\n8084.054\n8051.046\n\n\n1\n0\n0\n8046.786\n8065.725\n8046.834\n\n\n1\n0\n1\n8047.856\n8071.529\n8047.928\n\n\n1\n0\n2\n8049.746\n8078.154\n8049.847\n\n\n1\n0\n3\n8050.935\n8084.078\n8051.070\n\n\n1\n0\n4\n8052.780\n8090.657\n8052.953\n\n\n2\n0\n0\n8047.880\n8071.553\n8047.952\n\n\n2\n0\n1\n8049.757\n8078.164\n8049.858\n\n\n2\n0\n2\n8051.781\n8084.923\n8051.915\n\n\n2\n0\n3\n8050.634\n8088.511\n8050.807\n\n\n2\n0\n4\n8054.804\n8097.415\n8055.020\n\n\n3\n0\n0\n8049.542\n8077.949\n8049.642\n\n\n3\n0\n1\n8051.002\n8084.144\n8051.136\n\n\n3\n0\n2\n8050.141\n8088.017\n8050.314\n\n\n3\n0\n3\n8055.183\n8097.794\n8055.400\n\n\n3\n0\n4\n8054.533\n8101.879\n8054.798\n\n\n4\n0\n0\n8050.818\n8083.960\n8050.953\n\n\n4\n0\n1\n8052.676\n8090.552\n8052.849\n\n\n4\n0\n2\n8053.909\n8096.520\n8054.125\n\n\n4\n0\n3\n8055.875\n8103.221\n8056.140\n\n\n4\n0\n4\n8046.913\n8098.993\n8047.231\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8049.256 8063.459 8049.284\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\nThus, from the manually approach, the model with the lowest AIC value is AR(1). Thus, we’ll use this model within our approach to find the best ARCH/GARCH model.\n\n\nARCH/GARCH\nSince the original data doesn’t show time-varying volatility within stock prices, we’ll go forward without testing the GARCH model. Thus, the two models we’ll look at is AR + ARCH and just ARCH in order to find the best one at predicting HYBE prices given its volatility.\n\n\nARCH selection\nbest_ar_model &lt;- Arima(lm.residuals,order=c(1,0,0))\nar.res &lt;- best_ar_model$residuals\n\nacf(ar.res^2)\n\n\n\n\n\nARCH selection\npacf(ar.res^2)\n\n\n\n\n\nARCH selection\narch_model &lt;- list() ## set counter\ncc &lt;- 1\n\nfor (p in 1:4) {\n  for (q in 1:4) {\n    arch_model[[cc]] &lt;- garch(ar.res,order=c(q,p),trace=F)\n    cc &lt;- cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(arch_model, AIC) ## model with lowest AIC is the best\narch_model[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = ar.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4  \n6.246e+02  9.202e-12  6.121e-02  6.027e-02  6.121e-02  6.023e-02  \n\n\nFrom the manually calculation, we can see that the best ARCH model is ARCH(1,4). The next step now is to check which whether the AR+ARCH model or the ARCH model itself is the best.\n\n\nModel selection\nsummary(garchFit(~garch(1,4), ar.res, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 4), data = ar.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 4)\n&lt;environment: 0x7fe8cc2f9a38&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2        beta3  \n-0.00134274  21.05589755   0.00000001   0.97494048   0.00000001   0.00000001  \n      beta4  \n 0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.343e-03   9.945e-01   -0.001    0.999\nomega   2.106e+01   1.290e+01    1.632    0.103\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.749e-01   1.978e+01    0.049    0.961\nbeta2   1.000e-08   1.715e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.602    normalized:  -4.78074 \n\nDescription:\n Wed Nov 22 21:52:31 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  18.0638061 1.195348e-04\n Shapiro-Wilk Test  R    W       0.9847035 1.076037e-07\n Ljung-Box Test     R    Q(10)   5.0485163 8.879136e-01\n Ljung-Box Test     R    Q(15)  23.2427741 7.911959e-02\n Ljung-Box Test     R    Q(20)  27.8650892 1.126317e-01\n Ljung-Box Test     R^2  Q(10)   7.4176133 6.855185e-01\n Ljung-Box Test     R^2  Q(15)   8.7640540 8.895276e-01\n Ljung-Box Test     R^2  Q(20)  18.6867078 5.422662e-01\n LM Arch Test       R    TR^2    7.5249668 8.210648e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.578127 9.617535 9.577990 9.593230 \n\n\nModel selection\nsummary(garchFit(~arma(1, 0) + garch(1, 4), ar.res, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + garch(1, 4), data = ar.res, \n    trace = F) \n\nMean and Variance Equation:\n data ~ arma(1, 0) + garch(1, 4)\n&lt;environment: 0x7fe8cbaf2a20&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1        omega       alpha1        beta1        beta2  \n-0.00144214   0.00236649  21.18647953   0.00000001   0.97478384   0.00000001  \n      beta3        beta4  \n 0.00000001   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.442e-03   9.949e-01   -0.001    0.999\nar1     2.366e-03   3.450e-02    0.069    0.945\nomega   2.119e+01   1.295e+01    1.636    0.102\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.748e-01   1.946e+01    0.050    0.960\nbeta2   1.000e-08   1.567e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.441    normalized:  -4.780548 \n\nDescription:\n Wed Nov 22 21:52:31 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  17.9142074 1.288188e-04\n Shapiro-Wilk Test  R    W       0.9847546 1.124557e-07\n Ljung-Box Test     R    Q(10)   5.0407836 8.884370e-01\n Ljung-Box Test     R    Q(15)  23.1202713 8.161946e-02\n Ljung-Box Test     R    Q(20)  27.8526110 1.129345e-01\n Ljung-Box Test     R^2  Q(10)   7.4106948 6.861867e-01\n Ljung-Box Test     R^2  Q(15)   8.7613151 8.896631e-01\n Ljung-Box Test     R^2  Q(20)  18.7545990 5.378250e-01\n LM Arch Test       R    TR^2    7.4989478 8.229593e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.580122 9.625159 9.579943 9.597382 \n\n\nBased on the two models, looking at the Ljung-Box Test and the AIC values, we can say that ARCH(1,4) is the best model to predict HYBE stock prices.\n\n\nEquation:\nBased on the model above, we’ll say that equation for the model is as follows:\n\\(X_t = 2108 -0.1133z_1+ 1.475z_2 + 5.063z_3 - 1.314z_4\\)\n\\(y^*_t = y_t−0.00134274\\)\n\\(y_t = \\sigma_t \\epsilon_t\\)\n\\(\\sigma^2_t = 21.05589755 + 0.00000001y^2_{t-1} + 0.97494048\\sigma^2_{t-1} + 0.00000001\\sigma^2_{t-2} + 0.00000001\\sigma^2_{t-3} + 0.00000001\\sigma^2_{t-4}\\)"
  },
  {
    "objectID": "financial-ts.html#hybe-and-the-western-record-labels",
    "href": "financial-ts.html#hybe-and-the-western-record-labels",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "HYBE and the Western record labels:",
    "text": "HYBE and the Western record labels:\n\n\nData Visualization\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(WMG, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nwmg &lt;- ts(df2$WMG_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, wmg)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"wmg\")\n\nautoplot(df2_ts)\n\n\n\n\n\nAgain, from an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, and Universal Music Group stock prices has a stationary trend.\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df2)), size = 0.8 * nrow(df2)) \ntrain &lt;- df2[train_indices, ] \ntest &lt;- df2[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-74.588 -28.629  -1.115  28.270  80.866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -230.99954   90.35712  -2.557  0.01075 *  \nDate           0.01327    0.00452   2.935  0.00342 ** \nUMGP_Price    18.53846    2.67387   6.933 8.23e-12 ***\nWMG_Price      4.31327    0.32778  13.159  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.72 on 836 degrees of freedom\nMultiple R-squared:  0.4113,    Adjusted R-squared:  0.4092 \nF-statistic: 194.7 on 3 and 836 DF,  p-value: &lt; 2.2e-16\n\n\nSince all the variables are significant, we’ll continue with this model to find its residuals.\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.441803823391457\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 36.3722971019637\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -9.1461, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\nUsing auto.arima\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 1342:  log likelihood = -4216.61\nAIC=8435.21   AICc=8435.22   BIC=8439.94\n\nTraining set error measures:\n                        ME     RMSE      MAE MPE MAPE      MASE        ACF1\nTraining set -1.704182e-15 36.62861 31.29514 100  100 0.7331305 -0.03285607\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nManual AR/ARMA/ARIMA Model\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n8438.693\n8452.894\n8438.722\n\n\n0\n0\n1\n8439.816\n8458.750\n8439.864\n\n\n0\n0\n2\n8440.647\n8464.314\n8440.719\n\n\n0\n0\n3\n8442.630\n8471.031\n8442.731\n\n\n0\n0\n4\n8444.116\n8477.250\n8444.251\n\n\n1\n0\n0\n8439.751\n8458.684\n8439.798\n\n\n1\n0\n1\n8441.086\n8464.753\n8441.158\n\n\n1\n0\n2\n8441.905\n8470.306\n8442.006\n\n\n1\n0\n3\n8443.901\n8477.035\n8444.036\n\n\n1\n0\n4\n8445.168\n8483.035\n8445.341\n\n\n2\n0\n0\n8440.617\n8464.284\n8440.689\n\n\n2\n0\n1\n8441.890\n8470.291\n8441.991\n\n\n2\n0\n2\n8443.870\n8477.004\n8444.004\n\n\n2\n0\n3\n8445.826\n8483.693\n8445.999\n\n\n2\n0\n4\n8446.472\n8489.072\n8446.688\n\n\n3\n0\n0\n8442.566\n8470.966\n8442.667\n\n\n3\n0\n1\n8444.601\n8477.735\n8444.736\n\n\n3\n0\n2\n8445.860\n8483.727\n8446.034\n\n\n3\n0\n3\n8444.906\n8487.507\n8445.123\n\n\n3\n0\n4\n8442.665\n8490.000\n8442.931\n\n\n4\n0\n0\n8444.137\n8477.270\n8444.271\n\n\n4\n0\n1\n8445.375\n8483.243\n8445.549\n\n\n4\n0\n2\n8445.013\n8487.614\n8445.230\n\n\n4\n0\n3\n8441.485\n8488.819\n8441.751\n\n\n4\n0\n4\n8448.382\n8500.449\n8448.700\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8438.693 8452.894 8438.722\n\n\nFrom both auto.arima() and the manual approach, we can see that no AR/ARMA/ARIMA model was created from the residuals of the linear regression model. Thus, we can say that there is no autocorrelation in the data that would call for an AR/ARMA/ARIMA model.\nFrom this information, we can deduce the fact that Western record labels (Warner Group and Universal Music Group) do not have an affect on HYBE stock prices. Thus, we can say that HYBE’s success and future would best be predicted by the performance of other KPOP record labels (SM, JYP, and YG). Thus, we can say that for the best financial outcomes, music companies may be looking to KPOP groups and their marketing and music strategies in the coming future."
  }
]