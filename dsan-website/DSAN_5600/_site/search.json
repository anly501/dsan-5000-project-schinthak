[
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "",
    "text": "ARMA and ARIMA Models are used in time-series in order to forecast the time series object at had. Thus, we will take a look at both the globalization index, the stock fluctuation of HYBE Co., Korean tourism, and popularity between KPOP and Western artists in order to forecast values and make future predictions."
  },
  {
    "objectID": "arma.html#globalization-index",
    "href": "arma.html#globalization-index",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Globalization Index:",
    "text": "Globalization Index:\nFrom our Exploratory Data Analysis, we noticed the following information:\n\nPrior to differencing, the ACF plot show several lags above the significance bands, indicating a non-stationary relationship.\nThe Augmented Dickey-Fuller Test confirmed that the data itself was NOT stationary.\nFirst Differencing will be used since the data becomes stationary.\n\nThus, using this information, we will move on to creating the model:\n\nACF and PACFManual ARIMAEquationauto.arima()ForecastingBenchmark Comparison\n\n\n\nSince the ACF plot doesn’t have any significant peaks, q = 0.\nSince the PACF plot doesn’t have any significant peaks, p = 0.\nSince we differenced once, d = 1.\n\n\n\n\n\n\n\n\nThus, we can easily view that the best model has values p=0, d=1, and q=0.\n\n\nARIMA Model\nset.seed(123)\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(global_ts,order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n104.3201\n108.1441\n104.5754\n\n\n0\n1\n1\n106.1513\n111.8874\n106.6730\n\n\n0\n1\n2\n108.1031\n115.7512\n108.9920\n\n\n0\n1\n3\n110.0940\n119.6541\n111.4576\n\n\n0\n1\n4\n111.7783\n123.2504\n113.7318\n\n\n1\n1\n0\n106.1397\n111.8758\n106.6615\n\n\n1\n1\n1\n107.1777\n114.8258\n108.0666\n\n\n1\n1\n2\n109.1339\n118.6940\n110.4975\n\n\n1\n1\n3\n110.8985\n122.3706\n112.8519\n\n\n1\n1\n4\n112.3139\n125.6981\n114.9806\n\n\n2\n1\n0\n108.0883\n115.7364\n108.9772\n\n\n2\n1\n1\n109.9835\n119.5436\n111.3471\n\n\n2\n1\n2\n111.0559\n122.5280\n113.0094\n\n\n2\n1\n3\n110.6545\n124.0386\n113.3211\n\n\n2\n1\n4\n114.3509\n129.6471\n117.8631\n\n\n3\n1\n0\n110.0612\n119.6213\n111.4248\n\n\n3\n1\n1\n110.9697\n122.4419\n112.9232\n\n\n3\n1\n2\n108.9725\n122.3566\n111.6391\n\n\n3\n1\n3\n112.2628\n127.5589\n115.7749\n\n\n3\n1\n4\n112.7589\n129.9671\n117.2589\n\n\n4\n1\n0\n111.6849\n123.1570\n113.6384\n\n\n4\n1\n1\n112.6429\n126.0271\n115.3096\n\n\n4\n1\n2\n114.5878\n129.8839\n118.0999\n\n\n4\n1\n3\n112.6720\n129.8802\n117.1720\n\n\n4\n1\n4\n114.3036\n133.4238\n119.9446\n\n\n\n\n\n\n\n[1] \"AIC:\"\n\n\n  p d q      AIC      BIC     AICc\n1 0 1 0 104.3201 108.1441 104.5754\n\n\n[1] \"BIC:\"\n\n\n  p d q      AIC      BIC     AICc\n1 0 1 0 104.3201 108.1441 104.5754\n\n\n[1] \"AICc:\"\n\n\n  p d q      AIC      BIC     AICc\n1 0 1 0 104.3201 108.1441 104.5754\n\n\n\n\nFrom the results of the sarima() function, we call say that the equation is as follows:\n\\[\\begin{align}\nx_{t} = w_{t} -1w_{t-1} + 0.4805\n\\end{align}\\]\nFrom the model diagonotics presented, we can also say that the ACF plot of residuals shows no significance, meaning the residuals are not correlated. Additionally, the p-values of the Ljung-Box statistic is much higher than the significance band, meaning that we fail to reject the null hypothesis and can say that the model is not autocorrelated.\n\n\nCode\nsarima(global_ts, 0, 1, 0)\n\n\ninitial  value -0.415738 \niter   1 value -0.415738\nfinal  value -0.415738 \nconverged\ninitial  value -0.415738 \niter   1 value -0.415738\nfinal  value -0.415738 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.4805\ns.e.    0.0933\n\nsigma^2 estimated as 0.4354:  log likelihood = -50.16,  aic = 104.32\n\n$degrees_of_freedom\n[1] 49\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.4805 0.0933  5.1486       0\n\n$AIC\n[1] 2.086402\n\n$AICc\n[1] 2.088068\n\n$BIC\n[1] 2.162883\n\n\n\n\nauto.arima() concluded that ARIMA(0,2,1) is the best model. However, due to it’s greater AIC and BIC values, we have decided to stick if ARIMA(0,1,0). Thus, we will try forecasting both that model and ARIMA(0,1,0)\n\n\nCode\nauto.arima(global_ts)\n\n\nSeries: global_ts \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.8793\ns.e.   0.0718\n\nsigma^2 = 0.4493:  log likelihood = -50.16\nAIC=104.31   AICc=104.58   BIC=108.1\n\n\nCode\nsarima(global_ts, 0, 2, 1)\n\n\ninitial  value -0.115842 \niter   2 value -0.298718\niter   3 value -0.366283\niter   4 value -0.385874\niter   5 value -0.398419\niter   6 value -0.405122\niter   7 value -0.406146\niter   8 value -0.407410\niter   9 value -0.407414\niter  10 value -0.407414\niter  10 value -0.407414\nfinal  value -0.407414 \nconverged\ninitial  value -0.395252 \niter   2 value -0.395315\niter   3 value -0.395317\niter   3 value -0.395317\niter   3 value -0.395317\nfinal  value -0.395317 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1\n      -0.8793\ns.e.   0.0718\n\nsigma^2 estimated as 0.44:  log likelihood = -50.16,  aic = 104.31\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -0.8793 0.0718 -12.2539       0\n\n$AIC\n[1] 2.128875\n\n$AICc\n[1] 2.130612\n\n$BIC\n[1] 2.206093\n\n\n\n\n\n\nForecasting\nfit &lt;- Arima(global_ts, order=c(0, 1, 0))\nautoplot(forecast(fit))\n\n\n\n\n\n\n\n\n\nForecast comparison\nautoplot(global_ts) +\n  autolayer(meanf(global_ts, h=11),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(global_ts, h=11),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(global_ts, h=11),\n            series=\"Seasonal naïve\", PI=FALSE) +\n  autolayer(forecast(fit, h=11),\n            series=\"Fit\", PI=FALSE) +\n  ggtitle(\"Forecasts for yearly globalization metric\") +\n  xlab(\"Year\") + ylab(\"KOF Index\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\nThus, from the ARIMA procedure, we found that our manual ARIMA approach was more effective in finding the model with the best AIC and BIC values than auto.arima(). From the forecast, we can clearly see that while globalization has been on an upward trend, it is predicted to level out in the next year. While this prediction is not promising, it does perform better than out benchmark fits."
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "",
    "text": "In order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we’ll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations “BTS | Artist” (2022). Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.\nThe next relationship we’ll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we’ll see Universal Music Group and Warner’s relationship on HYBE and whether it’s significant.\nLastly, we’ll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry."
  },
  {
    "objectID": "arimax.html#the-kpop-record-labels---var",
    "href": "arimax.html#the-kpop-record-labels---var",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(1) The KPOP Record Labels - VAR:",
    "text": "(1) The KPOP Record Labels - VAR:\nFirstly, let’s gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed.\n\n\nData Collection\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nSONY &lt;- data.frame(SONY$SONY.Adjusted)\nSONY &lt;- SONY %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date &lt;- as.Date(min(SONY$Date))  \nend_date &lt;- as.Date(max(SONY$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSONY &lt;- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SONY[which(rowSums(is.na(SONY)) &gt; 0),]\ndf_na_cols &lt;- SONY[, which(colSums(is.na(SONY)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n\n\nVisualizationVARselectInitial selectionCross ValidationModel CreationForecasting\n\n\nAs we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let’s continue with the VAR model to see what the multivariate relationship is.\n\n\nCode\nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\nautoplot(df_ts)\n\n\n\n\n\n\n\nWe can see that the p-values detected from VARselect() are 5 and 1.\n\n\nCode\nVARselect(df_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  3.242792  3.219427  3.228749  3.220574  3.199157  3.202723  3.219911\nHQ(n)   3.283992  3.288094  3.324882  3.344174  3.350223  3.381255  3.425910\nSC(n)   3.351712  3.400961  3.482896  3.547335  3.598531  3.674710  3.764512\nFPE(n) 25.605120 25.013843 25.248200 25.042793 24.512385 24.600279 25.027247\n               8         9        10\nAIC(n)  3.219738  3.224843  3.237679\nHQ(n)   3.453203  3.485775  3.526077\nSC(n)   3.836952  3.914670  4.000120\nFPE(n) 25.023519 25.152364 25.478278\n\n\n\n\nWe can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don’t see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM.\nThus, before we continue with the model, we will also verify through a CV test.\n\n\nCode\nsummary(vars::VAR(df_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1112 \nLog Likelihood: -8136.588 \nRoots of the characteristic polynomial:\n0.9951 0.9852 0.9848 0.9848\nCall:\nvars::VAR(y = df_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9921208  0.0045176 219.613   &lt;2e-16 ***\nsm.l1   -0.0018632  0.0188413  -0.099    0.921    \nyg.l1    0.0497758  0.0356334   1.397    0.163    \njyp.l1  -0.0266689  0.0184253  -1.447    0.148    \nconst    0.1284624  0.7858872   0.163    0.870    \ntrend    0.0008352  0.0013929   0.600    0.549    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.55 on 1106 degrees of freedom\nMultiple R-Squared: 0.9905, Adjusted R-squared: 0.9905 \nF-statistic: 2.312e+04 on 5 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0022859  0.0016335   1.399   0.1620    \nsm.l1    0.9794874  0.0068128 143.771   &lt;2e-16 ***\nyg.l1   -0.0052246  0.0128847  -0.405   0.6852    \njyp.l1   0.0040576  0.0066624   0.609   0.5426    \nconst    0.1457548  0.2841683   0.513   0.6081    \ntrend    0.0011518  0.0005037   2.287   0.0224 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.645 on 1106 degrees of freedom\nMultiple R-Squared: 0.9947, Adjusted R-squared: 0.9947 \nF-statistic: 4.144e+04 on 5 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  5.948e-04  1.022e-03   0.582   0.5606    \nsm.l1   -9.013e-05  4.261e-03  -0.021   0.9831    \nyg.l1    9.893e-01  8.059e-03 122.759   &lt;2e-16 ***\njyp.l1  -2.273e-03  4.167e-03  -0.545   0.5856    \nconst    3.077e-01  1.777e-01   1.731   0.0837 .  \ntrend    2.684e-04  3.150e-04   0.852   0.3945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.029 on 1106 degrees of freedom\nMultiple R-Squared: 0.9854, Adjusted R-squared: 0.9854 \nF-statistic: 1.496e+04 on 5 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0004462  0.0011666   0.382 0.702200    \nsm.l1   -0.0132044  0.0048656  -2.714 0.006754 ** \nyg.l1    0.0174207  0.0092019   1.893 0.058598 .  \njyp.l1   0.9889679  0.0047581 207.847  &lt; 2e-16 ***\nconst   -0.2480215  0.2029470  -1.222 0.221930    \ntrend    0.0012691  0.0003597   3.528 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.175 on 1106 degrees of freedom\nMultiple R-Squared: 0.9968, Adjusted R-squared: 0.9968 \nF-statistic: 6.933e+04 on 5 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 20.704 2.3158 2.2466 2.0437\nsm    2.316 2.7070 0.7833 0.9799\nyg    2.247 0.7833 1.0590 0.7404\njyp   2.044 0.9799 0.7404 1.3807\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.3093 0.4798 0.3822\nsm   0.3093 1.0000 0.4626 0.5069\nyg   0.4798 0.4626 1.0000 0.6123\njyp  0.3822 0.5069 0.6123 1.0000\n\n\nCode\nsummary(vars::VAR(df_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, sm, yg, jyp \nDeterministic variables: both \nSample size: 1108 \nLog Likelihood: -7965.483 \nRoots of the characteristic polynomial:\n0.9934 0.9884 0.9748 0.9748 0.6644 0.6644 0.6176 0.6176 0.5959 0.5872 0.5872 0.5846 0.5535 0.5535 0.4478 0.4478 0.368 0.361 0.361 0.2776\nCall:\nvars::VAR(y = df_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  8.666e-01  3.500e-02  24.757  &lt; 2e-16 ***\nsm.l1   -7.648e-02  9.509e-02  -0.804  0.42141    \nyg.l1    9.170e-02  1.749e-01   0.524  0.60026    \njyp.l1   4.908e-02  1.511e-01   0.325  0.74533    \nhybe.l2  1.363e-01  4.652e-02   2.931  0.00345 ** \nsm.l2    6.649e-02  1.331e-01   0.499  0.61762    \nyg.l2   -2.466e-02  2.371e-01  -0.104  0.91719    \njyp.l2  -3.686e-02  2.013e-01  -0.183  0.85474    \nhybe.l3  9.552e-02  4.668e-02   2.046  0.04097 *  \nsm.l3    1.047e-01  1.337e-01   0.784  0.43343    \nyg.l3   -1.329e-01  2.381e-01  -0.558  0.57692    \njyp.l3  -1.400e-01  2.016e-01  -0.694  0.48760    \nhybe.l4  3.769e-02  4.527e-02   0.833  0.40529    \nsm.l4    7.740e-03  1.335e-01   0.058  0.95378    \nyg.l4   -2.363e-01  2.378e-01  -0.993  0.32072    \njyp.l4   1.863e-01  1.994e-01   0.934  0.35038    \nhybe.l5 -1.445e-01  3.320e-02  -4.353 1.47e-05 ***\nsm.l5   -1.017e-01  9.564e-02  -1.064  0.28763    \nyg.l5    3.428e-01  1.753e-01   1.956  0.05077 .  \njyp.l5  -7.712e-02  1.493e-01  -0.517  0.60549    \nconst    5.604e-01  7.647e-01   0.733  0.46382    \ntrend    7.224e-05  1.366e-03   0.053  0.95783    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.309 on 1086 degrees of freedom\nMultiple R-Squared: 0.9916, Adjusted R-squared: 0.9915 \nF-statistic:  6138 on 21 and 1086 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sm: \n=================================== \nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0123021  0.0133322   0.923  0.35635    \nsm.l1    0.9698567  0.0362187  26.778  &lt; 2e-16 ***\nyg.l1   -0.1343447  0.0666337  -2.016  0.04403 *  \njyp.l1  -0.0179767  0.0575447  -0.312  0.75480    \nhybe.l2 -0.0117678  0.0177207  -0.664  0.50679    \nsm.l2    0.0861901  0.0507148   1.700  0.08951 .  \nyg.l2    0.1421564  0.0903041   1.574  0.11573    \njyp.l2  -0.0742571  0.0766642  -0.969  0.33296    \nhybe.l3 -0.0002388  0.0177806  -0.013  0.98929    \nsm.l3    0.0225817  0.0509103   0.444  0.65745    \nyg.l3    0.0346174  0.0907076   0.382  0.70281    \njyp.l3  -0.0098724  0.0768007  -0.129  0.89774    \nhybe.l4  0.0091121  0.0172430   0.528  0.59730    \nsm.l4   -0.0880014  0.0508560  -1.730  0.08384 .  \nyg.l4   -0.0424794  0.0905886  -0.469  0.63922    \njyp.l4   0.1157384  0.0759455   1.524  0.12781    \nhybe.l5 -0.0071435  0.0126460  -0.565  0.57227    \nsm.l5   -0.0160131  0.0364279  -0.440  0.66033    \nyg.l5    0.0020312  0.0667619   0.030  0.97573    \njyp.l5  -0.0110284  0.0568540  -0.194  0.84623    \nconst    0.0150316  0.2912850   0.052  0.95885    \ntrend    0.0014489  0.0005203   2.785  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.641 on 1086 degrees of freedom\nMultiple R-Squared: 0.9948, Adjusted R-squared: 0.9947 \nF-statistic:  9848 on 21 and 1086 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation yg: \n=================================== \nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -0.0025310  0.0082685  -0.306  0.75959    \nsm.l1   -0.0073974  0.0224625  -0.329  0.74198    \nyg.l1    0.8726604  0.0413256  21.117  &lt; 2e-16 ***\njyp.l1   0.0043162  0.0356887   0.121  0.90376    \nhybe.l2  0.0034849  0.0109902   0.317  0.75123    \nsm.l2   -0.0133676  0.0314528  -0.425  0.67092    \nyg.l2    0.1827504  0.0560057   3.263  0.00114 ** \njyp.l2  -0.0107837  0.0475464  -0.227  0.82062    \nhybe.l3  0.0003741  0.0110274   0.034  0.97295    \nsm.l3    0.0417774  0.0315740   1.323  0.18606    \nyg.l3    0.0189166  0.0562560   0.336  0.73674    \njyp.l3   0.0138021  0.0476311   0.290  0.77205    \nhybe.l4  0.0048569  0.0106940   0.454  0.64980    \nsm.l4   -0.0303542  0.0315404  -0.962  0.33607    \nyg.l4    0.0032207  0.0561822   0.057  0.95430    \njyp.l4   0.0025712  0.0471007   0.055  0.95647    \nhybe.l5 -0.0053013  0.0078429  -0.676  0.49923    \nsm.l5    0.0097723  0.0225923   0.433  0.66543    \nyg.l5   -0.0937187  0.0414051  -2.263  0.02380 *  \njyp.l5  -0.0098661  0.0352603  -0.280  0.77968    \nconst    0.3979779  0.1806522   2.203  0.02780 *  \ntrend    0.0001869  0.0003227   0.579  0.56262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.018 on 1086 degrees of freedom\nMultiple R-Squared: 0.9859, Adjusted R-squared: 0.9857 \nF-statistic:  3628 on 21 and 1086 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation jyp: \n==================================== \njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0027699  0.0093754   0.295  0.76771    \nsm.l1   -0.0100466  0.0254694  -0.394  0.69332    \nyg.l1   -0.0900842  0.0468577  -1.923  0.05480 .  \njyp.l1   0.8924036  0.0404662  22.053  &lt; 2e-16 ***\nhybe.l2 -0.0034290  0.0124614  -0.275  0.78324    \nsm.l2   -0.0358290  0.0356633  -1.005  0.31529    \nyg.l2    0.1554553  0.0635030   2.448  0.01452 *  \njyp.l2   0.1112857  0.0539112   2.064  0.03923 *  \nhybe.l3  0.0074929  0.0125036   0.599  0.54912    \nsm.l3    0.0775470  0.0358007   2.166  0.03052 *  \nyg.l3   -0.0954436  0.0637868  -1.496  0.13487    \njyp.l3  -0.0157842  0.0540073  -0.292  0.77014    \nhybe.l4 -0.0055911  0.0121255  -0.461  0.64482    \nsm.l4   -0.0960331  0.0357626  -2.685  0.00736 ** \nyg.l4    0.1577294  0.0637031   2.476  0.01344 *  \njyp.l4   0.0690271  0.0534059   1.293  0.19646    \nhybe.l5 -0.0009265  0.0088928  -0.104  0.91704    \nsm.l5    0.0524032  0.0256166   2.046  0.04103 *  \nyg.l5   -0.1109220  0.0469478  -2.363  0.01832 *  \njyp.l5  -0.0673070  0.0399805  -1.683  0.09257 .  \nconst   -0.2324598  0.2048354  -1.135  0.25668    \ntrend    0.0011635  0.0003659   3.180  0.00151 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.154 on 1086 degrees of freedom\nMultiple R-Squared: 0.997,  Adjusted R-squared: 0.9969 \nF-statistic: 1.705e+04 on 21 and 1086 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 18.566 2.2572 2.1782 2.0336\nsm    2.257 2.6937 0.7721 0.9678\nyg    2.178 0.7721 1.0361 0.7146\njyp   2.034 0.9678 0.7146 1.3320\n\nCorrelation matrix of residuals:\n       hybe     sm     yg    jyp\nhybe 1.0000 0.3192 0.4966 0.4089\nsm   0.3192 1.0000 0.4622 0.5109\nyg   0.4966 0.4622 1.0000 0.6083\njyp  0.4089 0.5109 0.6083 1.0000\n\n\n\n\nThe results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2.\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df_ts[-(start:end), ]\n  test_model &lt;- df_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n\n[1] \"The best model is: \"\n\n\nCode\nprint(best_model)\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n    hybe.l1       sm.l1       yg.l1      jyp.l1     hybe.l2       sm.l2 \n 0.87201090  0.07620422  0.12286552 -0.06907358  0.14729914 -0.16491675 \n      yg.l2      jyp.l2     hybe.l3       sm.l3       yg.l3      jyp.l3 \n-0.13573997  0.09858174  0.07480327  0.19763602  0.02451796 -0.21327938 \n    hybe.l4       sm.l4       yg.l4      jyp.l4     hybe.l5       sm.l5 \n 0.04568771 -0.10244127 -0.32590355  0.26826401 -0.15423436  0.02001372 \n      yg.l5      jyp.l5       const       trend \n 0.36987287 -0.11603480  1.02527774 -0.00139024 \n\n\nEstimated coefficients for equation sm: \n======================================= \nCall:\nsm = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n     hybe.l1        sm.l1        yg.l1       jyp.l1      hybe.l2        sm.l2 \n 0.013590927  0.904810180 -0.066399663 -0.017876496 -0.017337965  0.033487971 \n       yg.l2       jyp.l2      hybe.l3        sm.l3        yg.l3       jyp.l3 \n 0.098164606 -0.040342672  0.004615891  0.122606007  0.081009347 -0.076273593 \n     hybe.l4        sm.l4        yg.l4       jyp.l4      hybe.l5        sm.l5 \n 0.003105172 -0.095496986 -0.059110240  0.108857574 -0.004531457  0.009438322 \n       yg.l5       jyp.l5        const        trend \n-0.033332437  0.027366242 -0.130596272  0.001510519 \n\n\nEstimated coefficients for equation yg: \n======================================= \nCall:\nyg = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n      hybe.l1         sm.l1         yg.l1        jyp.l1       hybe.l2 \n-2.070656e-03 -2.005516e-04  8.934663e-01 -5.029658e-03  5.052568e-03 \n        sm.l2         yg.l2        jyp.l2       hybe.l3         sm.l3 \n 1.241973e-02  1.414959e-01  1.553195e-04 -1.672154e-03  5.480095e-03 \n        yg.l3        jyp.l3       hybe.l4         sm.l4         yg.l4 \n 3.398433e-02  1.783199e-02  3.108653e-03 -1.486596e-02  6.449618e-03 \n       jyp.l4       hybe.l5         sm.l5         yg.l5        jyp.l5 \n-5.388133e-03 -4.889926e-03  2.540147e-03 -9.355377e-02 -9.266047e-03 \n        const         trend \n 6.386321e-01  6.745231e-05 \n\n\nEstimated coefficients for equation jyp: \n======================================== \nCall:\njyp = hybe.l1 + sm.l1 + yg.l1 + jyp.l1 + hybe.l2 + sm.l2 + yg.l2 + jyp.l2 + hybe.l3 + sm.l3 + yg.l3 + jyp.l3 + hybe.l4 + sm.l4 + yg.l4 + jyp.l4 + hybe.l5 + sm.l5 + yg.l5 + jyp.l5 + const + trend \n\n      hybe.l1         sm.l1         yg.l1        jyp.l1       hybe.l2 \n 0.0070298491 -0.0129008256 -0.0943891017  0.9078152165 -0.0082283937 \n        sm.l2         yg.l2        jyp.l2       hybe.l3         sm.l3 \n-0.0090787198  0.1227334552  0.1154161887  0.0096857545  0.0368680862 \n        yg.l3        jyp.l3       hybe.l4         sm.l4         yg.l4 \n-0.0457953112 -0.0271650685 -0.0109405883 -0.0698842398  0.1382804710 \n       jyp.l4       hybe.l5         sm.l5         yg.l5        jyp.l5 \n 0.0798251451  0.0006612204  0.0480291483 -0.1008826046 -0.0847061863 \n        const         trend \n-0.1013654507  0.0009041013 \n\n\n\n\nBased on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies.\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 268.25, df = 160, p-value = 1.768e-07\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 161.03, df = 112, p-value = 0.001666\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"sm\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"jyp\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"yg\")\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df_ts, p=2, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\n\n\n\n\nThus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. This could mean that, if HYBE were to proceed with business decisions based on KPOP record labels, they would face a downward trend in their stock prices."
  },
  {
    "objectID": "arimax.html#kpop-and-the-western-industry---var",
    "href": "arimax.html#kpop-and-the-western-industry---var",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(2) KPOP and the Western industry - VAR:",
    "text": "(2) KPOP and the Western industry - VAR:\nSimilarly, we’ll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE’s artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three.\nWe’ll follow the same steps as before the get some initial p values from VARselect().\n\nVisualizationVARselectInitial selectionCross ValidationModel CreationForecasting\n\n\nFrom an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we’ll continue with the VAR model to work on forecasting.\n\n\nCode\n#Creating a subset of only Korean Record label stock data\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(SONY, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony &lt;- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, sony)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n\n\n\n\n\n\n\nHere, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let’s continue by analyzing the residuals squared errors.\n\n\nCode\nVARselect(df2_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      5      2      5 \n\n$criteria\n                1          2          3         4          5          6\nAIC(n) -1.3950763 -1.4375108 -1.4515832 -1.479774 -1.4963602 -1.4920916\nHQ(n)  -1.3693066 -1.3962793 -1.3948899 -1.407619 -1.4087432 -1.3890127\nSC(n)  -1.3269518 -1.3285116 -1.3017093 -1.289026 -1.2647368 -1.2195935\nFPE(n)  0.2478142  0.2375184  0.2341997  0.227690  0.2239454  0.2249045\n                7          8          9         10\nAIC(n) -1.4839639 -1.4794227 -1.4779850 -1.4706681\nHQ(n)  -1.3654232 -1.3454202 -1.3285207 -1.3057419\nSC(n)  -1.1705911 -1.1251752 -1.0828628 -1.0346711\nFPE(n)  0.2267413  0.2277751  0.2281051  0.2297832\n\n\n\n\nFrom the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we’ll continue model selection through cross validation.\n\n\nCode\nsummary(vars::VAR(df2_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, sony \nDeterministic variables: both \nSample size: 1111 \nLog Likelihood: -3972.286 \nRoots of the characteristic polynomial:\n0.9963 0.9735 0.9735\nCall:\nvars::VAR(y = df2_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + sony.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.9775055  0.0049785 196.345  &lt; 2e-16 ***\numgp.l1 -0.1810678  0.2740135  -0.661 0.508879    \nsony.l1  0.0904280  0.0200664   4.506 7.29e-06 ***\nconst   -5.4559803  1.5408192  -3.541 0.000415 ***\ntrend    0.0015295  0.0005461   2.800 0.005192 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.506 on 1106 degrees of freedom\nMultiple R-Squared: 0.9907, Adjusted R-squared: 0.9907 \nF-statistic: 2.947e+04 on 4 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + sony.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  2.330e-04  9.114e-05   2.557   0.0107 *  \numgp.l1  9.888e-01  5.016e-03 197.115   &lt;2e-16 ***\nsony.l1 -6.789e-04  3.674e-04  -1.848   0.0648 .  \nconst    4.078e-02  2.821e-02   1.446   0.1486    \ntrend   -1.835e-05  9.999e-06  -1.836   0.0667 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.0825 on 1106 degrees of freedom\nMultiple R-Squared: 0.9821, Adjusted R-squared: 0.982 \nF-statistic: 1.518e+04 on 4 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sony: \n===================================== \nsony = hybe.l1 + umgp.l1 + sony.l1 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0020771  0.0015281   1.359  0.17433    \numgp.l1  0.2268382  0.0841031   2.697  0.00710 ** \nsony.l1  0.9767838  0.0061590 158.595  &lt; 2e-16 ***\nconst    2.0234611  0.4729243   4.279 2.04e-05 ***\ntrend   -0.0005467  0.0001676  -3.261  0.00114 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.383 on 1106 degrees of freedom\nMultiple R-Squared: 0.9899, Adjusted R-squared: 0.9899 \nF-statistic: 2.72e+04 on 4 and 1106 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          hybe      umgp     sony\nhybe 20.308515 -0.006463 0.796670\numgp -0.006463  0.006807 0.005349\nsony  0.796670  0.005349 1.913190\n\nCorrelation matrix of residuals:\n         hybe     umgp    sony\nhybe  1.00000 -0.01738 0.12781\numgp -0.01738  1.00000 0.04688\nsony  0.12781  0.04688 1.00000\n\n\nCode\nsummary(vars::VAR(df2_ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: hybe, umgp, sony \nDeterministic variables: both \nSample size: 1107 \nLog Likelihood: -3827.865 \nRoots of the characteristic polynomial:\n0.995 0.975 0.975 0.6454 0.6454 0.6308 0.6066 0.6066 0.6043 0.6043 0.5732 0.5062 0.5062 0.4185 0.4185\nCall:\nvars::VAR(y = df2_ts, p = 5, type = \"both\")\n\n\nEstimation results for equation hybe: \n===================================== \nhybe = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.8585565  0.0302880  28.346  &lt; 2e-16 ***\numgp.l1 -0.8961802  1.6111709  -0.556  0.57817    \nsony.l1  0.2117222  0.0960607   2.204  0.02773 *  \nhybe.l2  0.1281907  0.0398537   3.217  0.00134 ** \numgp.l2  2.5151241  2.1335925   1.179  0.23873    \nsony.l2 -0.0942796  0.1253821  -0.752  0.45225    \nhybe.l3  0.0847483  0.0401566   2.110  0.03505 *  \numgp.l3 -1.6093088  2.0866228  -0.771  0.44073    \nsony.l3  0.0679304  0.1264693   0.537  0.59129    \nhybe.l4  0.0371282  0.0391064   0.949  0.34262    \numgp.l4  2.6130331  2.1355444   1.224  0.22137    \nsony.l4 -0.1430621  0.1247935  -1.146  0.25189    \nhybe.l5 -0.1240648  0.0290143  -4.276 2.07e-05 ***\numgp.l5 -2.7978227  1.6122588  -1.735  0.08296 .  \nsony.l5  0.0138660  0.0953958   0.145  0.88446    \nconst   -2.9624994  1.5462767  -1.916  0.05564 .  \ntrend    0.0007640  0.0005389   1.418  0.15658    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.275 on 1090 degrees of freedom\nMultiple R-Squared: 0.9917, Adjusted R-squared: 0.9916 \nF-statistic:  8185 on 16 and 1090 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation umgp: \n===================================== \numgp = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1 -6.217e-05  5.683e-04  -0.109  0.91290    \numgp.l1  8.711e-01  3.023e-02  28.815  &lt; 2e-16 ***\nsony.l1 -3.857e-03  1.802e-03  -2.140  0.03258 *  \nhybe.l2  3.841e-04  7.478e-04   0.514  0.60762    \numgp.l2  1.522e-03  4.003e-02   0.038  0.96969    \nsony.l2 -1.754e-04  2.353e-03  -0.075  0.94059    \nhybe.l3 -6.906e-05  7.535e-04  -0.092  0.92698    \numgp.l3  2.785e-01  3.915e-02   7.114 2.04e-12 ***\nsony.l3  8.672e-04  2.373e-03   0.365  0.71483    \nhybe.l4 -4.933e-05  7.337e-04  -0.067  0.94641    \numgp.l4 -7.645e-02  4.007e-02  -1.908  0.05667 .  \nsony.l4  3.132e-03  2.341e-03   1.338  0.18131    \nhybe.l5  1.236e-05  5.444e-04   0.023  0.98188    \numgp.l5 -8.437e-02  3.025e-02  -2.789  0.00538 ** \nsony.l5 -6.482e-04  1.790e-03  -0.362  0.71730    \nconst    4.427e-02  2.901e-02   1.526  0.12729    \ntrend   -2.015e-05  1.011e-05  -1.992  0.04657 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08021 on 1090 degrees of freedom\nMultiple R-Squared: 0.9833, Adjusted R-squared: 0.9831 \nF-statistic:  4013 on 16 and 1090 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation sony: \n===================================== \nsony = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nhybe.l1  0.0319761  0.0096241   3.322 0.000922 ***\numgp.l1  0.7141464  0.5119557   1.395 0.163319    \nsony.l1  0.8387330  0.0305237  27.478  &lt; 2e-16 ***\nhybe.l2 -0.0352369  0.0126637  -2.783 0.005487 ** \numgp.l2  0.4475506  0.6779571   0.660 0.509300    \nsony.l2  0.1899306  0.0398406   4.767 2.12e-06 ***\nhybe.l3  0.0001916  0.0127599   0.015 0.988022    \numgp.l3 -0.4656160  0.6630323  -0.702 0.482672    \nsony.l3  0.0710732  0.0401861   1.769 0.077240 .  \nhybe.l4  0.0026579  0.0124262   0.214 0.830668    \numgp.l4 -0.8659717  0.6785773  -1.276 0.202172    \nsony.l4 -0.0442994  0.0396536  -1.117 0.264172    \nhybe.l5  0.0026997  0.0092194   0.293 0.769706    \numgp.l5  0.3873823  0.5123013   0.756 0.449716    \nsony.l5 -0.0796289  0.0303124  -2.627 0.008736 ** \nconst    2.0884103  0.4913353   4.250 2.32e-05 ***\ntrend   -0.0005505  0.0001712  -3.215 0.001343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.358 on 1090 degrees of freedom\nMultiple R-Squared: 0.9903, Adjusted R-squared: 0.9902 \nF-statistic:  6991 on 16 and 1090 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          hybe      umgp     sony\nhybe 18.274389 -0.006924 0.804549\numgp -0.006924  0.006433 0.006025\nsony  0.804549  0.006025 1.845119\n\nCorrelation matrix of residuals:\n         hybe     umgp   sony\nhybe  1.00000 -0.02019 0.1386\numgp -0.02019  1.00000 0.0553\nsony  0.13855  0.05530 1.0000\n\n\n\n\nCV seems to have chosen a different model where p=8. Thus, we’ll create models for p=1,5,8.\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(df2_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- df2_ts[-(start:end), ]\n  test_model &lt;- df2_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_hybe &lt;- pred$fcst$hybe[,1]\n  mse &lt;- mean((pred_hybe - test_model[, \"hybe\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(\"The best model is: \")\n\n\n[1] \"The best model is: \"\n\n\nCode\nprint(best_model)\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation hybe: \n========================================= \nCall:\nhybe = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n      hybe.l1       umgp.l1       sony.l1       hybe.l2       umgp.l2 \n 0.8539053638 -1.0215452531  0.2959227054  0.1419579738  2.6318973674 \n      sony.l2       hybe.l3       umgp.l3       sony.l3       hybe.l4 \n-0.1395902070  0.0738676112 -1.6865398900  0.0609080070  0.0563202021 \n      umgp.l4       sony.l4       hybe.l5       umgp.l5       sony.l5 \n 2.1966952795 -0.0891748828 -0.1463134011 -2.6688851863 -0.0475171819 \n        const         trend \n-3.6915131493  0.0002567314 \n\n\nEstimated coefficients for equation umgp: \n========================================= \nCall:\numgp = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n      hybe.l1       umgp.l1       sony.l1       hybe.l2       umgp.l2 \n-1.584725e-04  8.720297e-01 -4.619755e-03  3.142192e-04 -3.027609e-03 \n      sony.l2       hybe.l3       umgp.l3       sony.l3       hybe.l4 \n 7.597068e-04  3.311097e-05  2.828224e-01 -2.077352e-04 -2.589178e-05 \n      umgp.l4       sony.l4       hybe.l5       umgp.l5       sony.l5 \n-8.100671e-02  4.979529e-03  2.392913e-05 -8.533814e-02 -1.475128e-03 \n        const         trend \n 4.398623e-02 -3.323961e-05 \n\n\nEstimated coefficients for equation sony: \n========================================= \nCall:\nsony = hybe.l1 + umgp.l1 + sony.l1 + hybe.l2 + umgp.l2 + sony.l2 + hybe.l3 + umgp.l3 + sony.l3 + hybe.l4 + umgp.l4 + sony.l4 + hybe.l5 + umgp.l5 + sony.l5 + const + trend \n\n      hybe.l1       umgp.l1       sony.l1       hybe.l2       umgp.l2 \n 0.0338568245  0.6513179059  0.8339387214 -0.0338793911  0.5295159038 \n      sony.l2       hybe.l3       umgp.l3       sony.l3       hybe.l4 \n 0.1535306456 -0.0082547191 -0.3642386250  0.1164845896  0.0143544606 \n      umgp.l4       sony.l4       hybe.l5       umgp.l5       sony.l5 \n-0.8837947121 -0.0471415700 -0.0024410786  0.4169332761 -0.0843310946 \n        const         trend \n 1.7243762875 -0.0001127991 \n\n\n\n\nBased on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 &lt; 0.05.\n\n\nCode\nvar_model_1 &lt;- vars::VAR(df2_ts, p=1, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 274.17, df = 99, p-value &lt; 2.2e-16\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"sony\") \n\n\n\n\n\nCode\n#--\n\nvar_model_2 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 90.726, df = 63, p-value = 0.01265\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"sony\")\n\n\n\n\n\nCode\n#--\n\nvar_model_3 &lt;- vars::VAR(df2_ts, p=8, type= \"both\", season = NULL, exog = NULL)\ngu.serial &lt;- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \ngu.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_3\nChi-squared = 55.819, df = 36, p-value = 0.01863\n\n\nCode\nplot(gu.serial, names = \"hybe\") \n\n\n\n\n\nCode\nplot(gu.serial, names = \"umgp\")\n\n\n\n\n\nCode\nplot(gu.serial, names = \"sony\")\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(1,2,3,1))\nvar_model_1 &lt;- vars::VAR(df2_ts, p=5, type= \"both\", season = NULL, exog = NULL)\n\nfit.pr &lt;- predict(var_model_1, n.ahead = 365, ci = 0.95)\nfanchart(fit.pr)\n\n\n\n\n\n\n\n\nFrom this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP’s stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea.\nKnowing this downward trend in the stock prices of the biggest performing record music agency, we may start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea."
  },
  {
    "objectID": "arimax.html#foreign-tourism-in-korea-on-cultural-globalization-in-the-usa",
    "href": "arimax.html#foreign-tourism-in-korea-on-cultural-globalization-in-the-usa",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(3) - Foreign tourism in Korea on Cultural Globalization in the USA",
    "text": "(3) - Foreign tourism in Korea on Cultural Globalization in the USA\nLet’s see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting.\nWe’ll combine the globalization index data from KOF with the South Korean tourism data from Statistica.\n\n\n# A tibble: 6 × 3\n   Year tourists KOFCuGIdf\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  2000  5320000      75.0\n2  2001  5150000      76.9\n3  2002  5350000      76.7\n4  2003  4750000      77.3\n5  2004  5810000      80.8\n6  2005  6020000      86.5\n\n\nAs discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP’s influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture.\n\nVisualizationUsing Auto.Arima()Manual ModelModel FitsCVFinal Model FitForecasting\n\n\nFrom the graph above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020.\n\n\nCode\nglobal_ts &lt;-ts(df3, start = 2000, frequency = 1)\n\nautoplot(global_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Cultural Globalization in USA and Tourism in South Korea\")\n\n\n\n\n\n\n\nNow, let’s move on with the ARIMAX/ARMAX model. First, we’ll create a model using auto.arima().\nBased on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant.\n\n\nCode\nfit &lt;- auto.arima(global_ts[, \"KOFCuGIdf\"], xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  intercept  xreg\n      1.6838  -0.7370    87.7759     0\ns.e.  0.1365   0.1434     0.4453     0\n\nsigma^2 = 2.666:  log likelihood = -38.14\nAIC=86.28   AICc=90.57   BIC=91.26\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 0.2527717 1.460494 1.089386 0.2644077 1.265622 0.9198188 0.1042156\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,0) errors\nQ* = 7.4991, df = 3, p-value = 0.05758\n\nModel df: 2.   Total lags used: 5\n\n\n\n\nWe’ll move now to find the ARMAX model manually. Let’s start by taking creating a regression model of tourism on cultural globalization. Using that model, we’ll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we’ll validate the model through cross validation.\nFrom the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data.\nFrom the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0).\n\n\nCode\ndf3$tourists &lt;-ts(df3$tourists, start= 2000, frequency = 1)\ndf3$KOFCuGIdf &lt;-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)\n\n############# First fit the linear model##########\nfit.reg &lt;- lm(KOFCuGIdf ~ tourists, data = df3)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = KOFCuGIdf ~ tourists, data = df3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2468 -4.0753  0.9324  3.8493  8.2528 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.783e+01  3.063e+00   25.41  1.5e-15 ***\ntourists    1.202e-06  2.918e-07    4.12 0.000644 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.468 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006436\n\n\n\n\nCode\nres.fit&lt;-ts(residuals(fit.reg), start= 2000, frequency = 1)\nggAcf(res.fit)\n\n\n\n\n\nCode\nggPacf(res.fit)\n\n\n\n\n\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:4)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:4)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n127.2329\n130.2201\n128.7329\n\n\n0\n0\n1\n116.8798\n120.8627\n119.5464\n\n\n0\n0\n2\n111.9461\n116.9248\n116.2318\n\n\n0\n0\n3\n109.5787\n115.5531\n116.0402\n\n\n0\n0\n4\n111.3460\n118.3162\n120.6794\n\n\n1\n0\n0\n106.7209\n110.7039\n109.3876\n\n\n1\n0\n1\n108.4723\n113.4510\n112.7580\n\n\n1\n0\n2\n108.4037\n114.3781\n114.8653\n\n\n1\n0\n3\n110.3598\n117.3299\n119.6931\n\n\n1\n0\n4\n112.3462\n120.3121\n125.4371\n\n\n2\n0\n0\n108.2759\n113.2546\n112.5616\n\n\n2\n0\n1\n105.2455\n111.2199\n111.7071\n\n\n2\n0\n2\n104.8959\n111.8660\n114.2292\n\n\n2\n0\n3\n112.2294\n120.1952\n125.3203\n\n\n2\n0\n4\n114.0108\n122.9724\n132.0108\n\n\n3\n0\n0\n107.7495\n113.7239\n114.2110\n\n\n3\n0\n1\n105.8867\n112.8568\n115.2200\n\n\n3\n0\n2\n112.1006\n120.0664\n125.1915\n\n\n3\n0\n3\n113.3105\n122.2721\n131.3105\n\n\n3\n0\n4\n115.3092\n125.2666\n139.7537\n\n\n4\n0\n0\n109.2205\n116.1906\n118.5538\n\n\n4\n0\n1\n111.1023\n119.0682\n124.1932\n\n\n4\n0\n2\n113.2815\n122.2431\n131.2815\n\n\n4\n0\n3\n111.4614\n121.4187\n135.9058\n\n\n4\n0\n4\n111.5921\n122.5452\n144.5921\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q      AIC     BIC     AICc\n13 2 0 2 104.8959 111.866 114.2292\n\n\nCode\noutput[which.min(output$BIC),] \n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 106.7209 110.7039 109.3876\n\n\n\n\nFrom the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we’ll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting.\n\n\nCode\ncapture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n [1] \"initial  value 1.585318 \"                                                              \n [2] \"iter   2 value 1.000819\"                                                               \n [3] \"iter   3 value 0.979873\"                                                               \n [4] \"iter   4 value 0.971786\"                                                               \n [5] \"iter   5 value 0.970922\"                                                               \n [6] \"iter   6 value 0.970615\"                                                               \n [7] \"iter   7 value 0.970607\"                                                               \n [8] \"iter   8 value 0.970607\"                                                               \n [9] \"iter   8 value 0.970607\"                                                               \n[10] \"iter   8 value 0.970607\"                                                               \n[11] \"final  value 0.970607 \"                                                                \n[12] \"converged\"                                                                             \n[13] \"initial  value 1.116539 \"                                                              \n[14] \"iter   2 value 1.064095\"                                                               \n[15] \"iter   3 value 1.062560\"                                                               \n[16] \"iter   4 value 1.061681\"                                                               \n[17] \"iter   5 value 1.061650\"                                                               \n[18] \"iter   6 value 1.061650\"                                                               \n[19] \"iter   6 value 1.061650\"                                                               \n[20] \"final  value 1.061650 \"                                                                \n[21] \"converged\"                                                                             \n[22] \"$fit\"                                                                                  \n[23] \"\"                                                                                      \n[24] \"Call:\"                                                                                 \n[25] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[26] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[27] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[28] \"\"                                                                                      \n[29] \"Coefficients:\"                                                                         \n[30] \"         ar1    xmean\"                                                                 \n[31] \"      0.8652  -2.5386\"                                                                 \n[32] \"s.e.  0.1038   3.8694\"                                                                 \n[33] \"\"                                                                                      \n[34] \"sigma^2 estimated as 7.801:  log likelihood = -49.61,  aic = 105.22\"                   \n[35] \"\"                                                                                      \n[36] \"$degrees_of_freedom\"                                                                   \n[37] \"[1] 18\"                                                                                \n[38] \"\"                                                                                      \n[39] \"$ttable\"                                                                               \n[40] \"      Estimate     SE t.value p.value\"                                                 \n[41] \"ar1     0.8652 0.1038  8.3328  0.0000\"                                                 \n[42] \"xmean  -2.5386 3.8694 -0.6561  0.5201\"                                                 \n[43] \"\"                                                                                      \n[44] \"$AIC\"                                                                                  \n[45] \"[1] 5.261176\"                                                                          \n[46] \"\"                                                                                      \n[47] \"$AICc\"                                                                                 \n[48] \"[1] 5.29647\"                                                                           \n[49] \"\"                                                                                      \n[50] \"$BIC\"                                                                                  \n[51] \"[1] 5.410536\"                                                                          \n[52] \"\"                                                                                      \n\n\nCode\ncapture.output(sarima(res.fit, 2,0,2)) \n\n\n\n\n\n [1] \"initial  value 1.553778 \"                                                              \n [2] \"iter   2 value 1.430949\"                                                               \n [3] \"iter   3 value 1.042676\"                                                               \n [4] \"iter   4 value 0.993029\"                                                               \n [5] \"iter   5 value 0.974081\"                                                               \n [6] \"iter   6 value 0.956070\"                                                               \n [7] \"iter   7 value 0.949449\"                                                               \n [8] \"iter   8 value 0.944797\"                                                               \n [9] \"iter   9 value 0.942595\"                                                               \n[10] \"iter  10 value 0.941319\"                                                               \n[11] \"iter  11 value 0.940887\"                                                               \n[12] \"iter  12 value 0.940866\"                                                               \n[13] \"iter  13 value 0.940866\"                                                               \n[14] \"iter  14 value 0.940865\"                                                               \n[15] \"iter  15 value 0.940862\"                                                               \n[16] \"iter  16 value 0.940853\"                                                               \n[17] \"iter  17 value 0.940845\"                                                               \n[18] \"iter  18 value 0.940830\"                                                               \n[19] \"iter  19 value 0.940820\"                                                               \n[20] \"iter  20 value 0.940818\"                                                               \n[21] \"iter  21 value 0.940816\"                                                               \n[22] \"iter  22 value 0.940816\"                                                               \n[23] \"iter  22 value 0.940816\"                                                               \n[24] \"iter  22 value 0.940816\"                                                               \n[25] \"final  value 0.940816 \"                                                                \n[26] \"converged\"                                                                             \n[27] \"initial  value 1.014365 \"                                                              \n[28] \"iter   2 value 1.000713\"                                                               \n[29] \"iter   3 value 0.998434\"                                                               \n[30] \"iter   4 value 0.997970\"                                                               \n[31] \"iter   5 value 0.997732\"                                                               \n[32] \"iter   6 value 0.997580\"                                                               \n[33] \"iter   7 value 0.997379\"                                                               \n[34] \"iter   8 value 0.997048\"                                                               \n[35] \"iter   9 value 0.996543\"                                                               \n[36] \"iter  10 value 0.995733\"                                                               \n[37] \"iter  11 value 0.991400\"                                                               \n[38] \"iter  12 value 0.985956\"                                                               \n[39] \"iter  13 value 0.981444\"                                                               \n[40] \"iter  14 value 0.974812\"                                                               \n[41] \"iter  15 value 0.971043\"                                                               \n[42] \"iter  16 value 0.960387\"                                                               \n[43] \"iter  17 value 0.955391\"                                                               \n[44] \"iter  18 value 0.944255\"                                                               \n[45] \"iter  19 value 0.918264\"                                                               \n[46] \"iter  20 value 0.911566\"                                                               \n[47] \"iter  21 value 0.908276\"                                                               \n[48] \"iter  22 value 0.904759\"                                                               \n[49] \"iter  23 value 0.902341\"                                                               \n[50] \"iter  24 value 0.902240\"                                                               \n[51] \"iter  25 value 0.902160\"                                                               \n[52] \"iter  26 value 0.901582\"                                                               \n[53] \"iter  27 value 0.900613\"                                                               \n[54] \"iter  28 value 0.900164\"                                                               \n[55] \"iter  29 value 0.900127\"                                                               \n[56] \"iter  30 value 0.900121\"                                                               \n[57] \"iter  31 value 0.900121\"                                                               \n[58] \"iter  32 value 0.900121\"                                                               \n[59] \"iter  32 value 0.900121\"                                                               \n[60] \"iter  32 value 0.900121\"                                                               \n[61] \"final  value 0.900121 \"                                                                \n[62] \"converged\"                                                                             \n[63] \"$fit\"                                                                                  \n[64] \"\"                                                                                      \n[65] \"Call:\"                                                                                 \n[66] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[67] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[68] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[69] \"\"                                                                                      \n[70] \"Coefficients:\"                                                                         \n[71] \"         ar1      ar2      ma1     ma2   xmean\"                                        \n[72] \"      1.8206  -0.9472  -1.2918  0.2918  0.4865\"                                        \n[73] \"s.e.  0.0631   0.0603   0.3380  0.2889  0.8530\"                                        \n[74] \"\"                                                                                      \n[75] \"sigma^2 estimated as 4.709:  log likelihood = -46.38,  aic = 104.76\"                   \n[76] \"\"                                                                                      \n[77] \"$degrees_of_freedom\"                                                                   \n[78] \"[1] 15\"                                                                                \n[79] \"\"                                                                                      \n[80] \"$ttable\"                                                                               \n[81] \"      Estimate     SE  t.value p.value\"                                                \n[82] \"ar1     1.8206 0.0631  28.8298  0.0000\"                                                \n[83] \"ar2    -0.9472 0.0603 -15.7180  0.0000\"                                                \n[84] \"ma1    -1.2918 0.3380  -3.8223  0.0017\"                                                \n[85] \"ma2     0.2918 0.2889   1.0100  0.3285\"                                                \n[86] \"xmean   0.4865 0.8530   0.5703  0.5769\"                                                \n[87] \"\"                                                                                      \n[88] \"$AIC\"                                                                                  \n[89] \"[1] 5.238119\"                                                                          \n[90] \"\"                                                                                      \n[91] \"$AICc\"                                                                                 \n[92] \"[1] 5.452405\"                                                                          \n[93] \"\"                                                                                      \n[94] \"$BIC\"                                                                                  \n[95] \"[1] 5.536838\"                                                                          \n[96] \"\"                                                                                      \n\n\n\n\nFrom the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we’ll choose to forecast Korean tourism on cultural globalization in the US via model 1.\n\n\nCode\nn &lt;- length(res.fit)\nk &lt;- 5  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 &lt;- matrix(NA, 15)\nrmse2 &lt;- matrix(NA, 15)\nrmse3 &lt;- matrix(NA, 15)\n\nst &lt;- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:15) {\n  # Define the training set\n  train_end &lt;- st + i - 1\n  xtrain &lt;- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start &lt;- train_end + 1\n  test_end &lt;- min(st + i, tsp(res.fit)[2])\n  xtest &lt;- window(res.fit, start = test_start, end = test_end)\n\n  fit &lt;- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast &lt;- forecast(fit, h = 4)\n\n  fit2 &lt;- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast2 &lt;- forecast(fit2, h = 4)\n\n  fit3 &lt;- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast3 &lt;- forecast(fit3, h = 4)\n\n  rmse1[i] &lt;- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] &lt;- sqrt((fcast2$mean - xtest)^2)\n  rmse3[i] &lt;- sqrt((fcast3$mean - xtest)^2)\n}\n\nplot(1:15, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:15, rmse1, type = \"l\", col = 3)\nlines(1:15, rmse3, type = \"l\", col = 4)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\", \"fit3\"), col = 2:4, lty = 1)\n\n\n\n\n\n\n\n\n\nCode\nfit &lt;- Arima(global_ts[, \"KOFCuGIdf\"], order=c(1,0,0), xreg = global_ts[, \"tourists\"])\nsummary(fit)\n\n\nSeries: global_ts[, \"KOFCuGIdf\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept  xreg\n      0.9684    84.7153     0\ns.e.  0.0398     1.3007     0\n\nsigma^2 = 5.577:  log likelihood = -45.33\nAIC=98.66   AICc=101.32   BIC=102.64\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE    MASE     ACF1\nTraining set 0.8955075 2.177229 1.295393 0.9914592 1.500592 1.09376 0.462599\n\n\n\n\n\n\nCode\ntourists_fit &lt;- auto.arima(global_ts[, \"tourists\"]) \nft &lt;- forecast(tourists_fit)\n\nfcast &lt;- forecast(fit, xreg=ft$mean)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Globalization\")\n\n\n\n\n\nCode\nsummary(tourists_fit)\n\n\nSeries: global_ts[, \"tourists\"] \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1     drift\n      -0.5710  627170.2\ns.e.   0.1837  185981.8\n\nsigma^2 = 1.743e+12:  log likelihood = -293.87\nAIC=593.75   AICc=595.35   BIC=596.58\n\nTraining set error measures:\n                    ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set -15386.25 1217172 961194.1 -2.96541 9.812744 0.7784607 -0.1121698\n\n\n\n\n\nWe can see that in the next 10 years, globalization within the US with regards to Korea’s tourism of foreigners will see a slight decrease. As we’ve observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done."
  },
  {
    "objectID": "arimax.html#kpop-and-musical-characteristsics",
    "href": "arimax.html#kpop-and-musical-characteristsics",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(4) KPOP and Musical Characteristsics",
    "text": "(4) KPOP and Musical Characteristsics\nAs we saw in Data Visualization, KPOP as a genre seems to be heavily correlated with loudness, energy, and valence. This energetic sound is something that is very characteristic of KPOP, and thus, it will be insightful to note is these factors someone change the prediction of the popularity metric.\nPlease note: As a reminder, due to the recent changes in the Spotify API, popularity score is no longer available for all songs. Thus, in order to represent all the songs of an artist, we extrapolated with linear regression. Additionally, since songs releases are no consistent, we do not have as much data to work with for the four artsist we’re analyzing.\nTo analyze KPOP as a whole, we’ll be taking the average of all metrics per year as well as the average popularity score.\n\n\nData Collection\nkpop_artists &lt;- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists &lt;- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\nkpop_arimax &lt;- spotify %&gt;%\n  filter(artist_name %in% kpop_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n\n\nVisualizationauto.arima()Manual ModelModel FitsCVForecasting\n\n\nAll musical characteristics do not seem to have a significant trend or patterns in the data.\n\nkpop_ts &lt;-ts(kpop_arimax, start = 2013, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(kpop_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for KPOP Artists\") \n\n\n\n\n\n\nThe auto.arima() model, similar to our ARIMA model, produced ARIMA(0,0,0). Thus, we’ll continue to explore a linear regression with the model ARIMA(0,0,0).\n\nkpop_arimax$instrumentalness &lt;-ts(kpop_arimax$instrumentalness, start= 2013, frequency = 1)\nkpop_arimax$valence &lt;-ts(kpop_arimax$valence, start= 2013, frequency = 1)\nkpop_arimax$danceability &lt;-ts(kpop_arimax$danceability, start= 2013, frequency = 1)\nkpop_arimax$energy &lt;-ts(kpop_arimax$energy, start= 2013, frequency = 1)\nkpop_arimax$loudness &lt;-ts(kpop_arimax$loudness, start= 2013, frequency = 1)\nkpop_arimax$speechiness &lt;-ts(kpop_arimax$speechiness, start= 2013, frequency = 1)\nkpop_arimax$acousticness &lt;-ts(kpop_arimax$acousticness, start= 2013, frequency = 1)\nkpop_arimax$liveness &lt;-ts(kpop_arimax$liveness, start= 2013, frequency = 1)\nkpop_arimax$tempo &lt;-ts(kpop_arimax$tempo, start= 2013, frequency = 1)\nkpop_arimax$popularity &lt;-ts(kpop_arimax$popularity, start= 2013, frequency = 1)\n\n\nfit &lt;- auto.arima(kpop_ts[, \"popularity\"], xreg = kpop_ts[, c(2:10)])\nsummary(fit)\n\nSeries: kpop_ts[, \"popularity\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      intercept  instrumentalness  valence  danceability    energy  loudness\n        45.7616           84.5679  17.6992       -3.2121  -57.3930    0.2070\ns.e.     4.9998           20.5542   6.7811        3.7263   19.9552    0.8031\n      speechiness  acousticness  liveness   tempo\n           9.2107       -3.3351   18.6917  0.5278\ns.e.       6.2002        2.8620    6.1590  0.1225\n\nsigma^2 = 0.3514:  log likelihood = 3.33\nAIC=15.34   AICc=-248.66   BIC=19.71\n\nTraining set error measures:\n                      ME      RMSE       MAE           MPE      MAPE       MASE\nTraining set 8.53615e-10 0.1787393 0.1266793 -0.0005679029 0.1624506 0.08782325\n                   ACF1\nTraining set -0.5743035\n\ncheckresiduals(fit)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 5.0725, df = 3, p-value = 0.1666\n\nModel df: 0.   Total lags used: 3\n\n\n\n\nUnfortunetly, while the residuals are not autocorrelated, none of the predictors were deemed significant to the popularity score. Therefore, for the purposes of modeling, we’ll continue was the smallest p-valed predictors intrumentalness and tempo.\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = kpop_arimax)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = popularity ~ instrumentalness + valence + danceability + \n    energy + loudness + speechiness + acousticness + liveness + \n    tempo, data = kpop_arimax)\n\nResiduals:\n        1         2         3         4         5         6         7         8 \n-0.144760  0.139854  0.008302  0.081926 -0.019960  0.055501  0.212295 -0.461506 \n        9        10        11 \n 0.198858 -0.018464 -0.052047 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)       45.7616    16.5817   2.760    0.221\ninstrumentalness  84.5679    68.1670   1.241    0.432\nvalence           17.6992    22.4892   0.787    0.576\ndanceability      -3.2121    12.3582  -0.260    0.838\nenergy           -57.3930    66.1806  -0.867    0.545\nloudness           0.2070     2.6635   0.078    0.951\nspeechiness        9.2107    20.5626   0.448    0.732\nacousticness      -3.3351     9.4916  -0.351    0.785\nliveness          18.6917    20.4261   0.915    0.528\ntempo              0.5278     0.4063   1.299    0.418\n\nResidual standard error: 0.5928 on 1 degrees of freedom\nMultiple R-squared:  0.9922,    Adjusted R-squared:  0.9217 \nF-statistic: 14.07 on 9 and 1 DF,  p-value: 0.2042\n\n\n\n\nCode\nres.fit&lt;-ts(residuals(fit.reg), start= 2013, frequency = 1)\nggAcf(res.fit)\n\n\n\n\n\nCode\nggPacf(res.fit)\n\n\n\n\n\n\n\n\nCall:\nlm(formula = popularity ~ instrumentalness + tempo, data = kpop_arimax)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0145 -1.0731  0.4156  0.6839  2.4913 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       21.8460    16.3513   1.336  0.21829   \ninstrumentalness  57.1824    28.9419   1.976  0.08359 . \ntempo              0.4607     0.1367   3.370  0.00978 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.472 on 8 degrees of freedom\nMultiple R-squared:  0.6134,    Adjusted R-squared:  0.5168 \nF-statistic: 6.347 on 2 and 8 DF,  p-value: 0.02233\n\n\n\n\n\n\n\n\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n41.94197\n43.13566\n45.37054\n\n\n0\n0\n1\n43.94143\n45.53301\n50.60809\n\n\n0\n0\n2\n43.37163\n45.36111\n55.37163\n\n\n1\n0\n0\n43.94155\n45.53313\n50.60821\n\n\n1\n0\n1\n45.56530\n47.55478\n57.56530\n\n\n1\n0\n2\n45.32442\n47.71179\n66.32442\n\n\n2\n0\n0\n45.83803\n47.82750\n57.83803\n\n\n2\n0\n1\n44.47938\n46.86675\n65.47938\n\n\n2\n0\n2\n42.30636\n45.09162\n79.63969\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 41.94197 43.13566 45.37054\n\n\nCode\noutput[which.min(output$BIC),] \n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 41.94197 43.13566 45.37054\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 41.94197 43.13566 45.37054\n\n\n\n\nThe only model selected was ARIMA(0,0,0). However, due to a lack of data, we were unable to capture model diagnostic output for the model ARIMA(0,0,0) on the residuals. Thus, we’ll continue this linear analysis.\n\ncapture.output(sarima(res.fit, 0,0,0))\n\n\n\nSince we’re only looking at one model, there is no need for cross validation. We’ll continue to forecasting this model.\n\n\n\nfit &lt;- Arima(kpop_ts[, \"popularity\"], order=c(0,0,0), xreg = kpop_ts[, c('instrumentalness', 'tempo')])\n\nmusic_fit &lt;- auto.arima(kpop_ts[, \"instrumentalness\"]) \nft &lt;- forecast(music_fit)\n\nmusic_fit &lt;- auto.arima(kpop_ts[, \"tempo\"]) \nft2 &lt;- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            TEMPO = ft2$mean)\n\nfcast &lt;- forecast(fit, xreg=xreg)\n\nWarning in forecast.forecast_ARIMA(fit, xreg = xreg): xreg contains different\ncolumn names from the xreg used in training. Please check that the regressors\nare in the same order.\n\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n\n\n\n\n\n\nUnfortunetly, due to the limitations of the dataset, we were unable to accurately predict popularity based on musical characteristics. However, some insights were that while tempo and instrumentalness were siginificant on their own at 90% confidence to the popularity, as a whole, the musical characteristics weren’t significant to the popularity."
  },
  {
    "objectID": "arimax.html#western-artists-discography-and-musical-characteristsics",
    "href": "arimax.html#western-artists-discography-and-musical-characteristsics",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(5) Western Artists’ Discography and Musical Characteristsics",
    "text": "(5) Western Artists’ Discography and Musical Characteristsics\nAs western artists have dominated the global sphere for decades, it would be interesting to understand if there are specific musical qualities that attribute to this popularity and fame.\n\n\nData Collection\nwestern_arimax &lt;- spotify %&gt;%\n  filter(artist_name %in% western_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n\n\nVisualizationauto.arima()Manual ModelModel FitsCVFinal Model FitForecasting\n\n\nSimilarly to KPOP, we cannot see ask specific trends of patterns in this data.\n\n\nCode\nwestern_ts &lt;-ts(western_arimax, start = 2003, frequency = 1)\n\n#options(repr.plot.width=10, repr.plot.height=20)\n\nautoplot(western_ts[,c(2:10)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Musical Characeristics and Popularity for Western Artists\") \n\n\n\n\n\n\n\nBased on our data, auto.arima() suggests the model ARIMA(0,0,0). This may be due to the fact that musical characteristics do not effect popularity. We’ll look to out manual approach for other conclusions.\n\n\nCode\nwestern_arimax$instrumentalness &lt;-ts(western_arimax$instrumentalness, start= 2003, frequency = 1)\nwestern_arimax$valence &lt;-ts(western_arimax$valence, start= 2003, frequency = 1)\nwestern_arimax$danceability &lt;-ts(western_arimax$danceability, start= 2003, frequency = 1)\nwestern_arimax$energy &lt;-ts(western_arimax$energy, start= 2003, frequency = 1)\nwestern_arimax$loudness &lt;-ts(western_arimax$loudness, start= 2003, frequency = 1)\nwestern_arimax$speechiness &lt;-ts(western_arimax$speechiness, start= 2003, frequency = 1)\nwestern_arimax$acousticness &lt;-ts(western_arimax$acousticness, start= 2003, frequency = 1)\nwestern_arimax$liveness &lt;-ts(western_arimax$liveness, start= 2003, frequency = 1)\nwestern_arimax$tempo &lt;-ts(western_arimax$tempo, start= 2003, frequency = 1)\nwestern_arimax$popularity &lt;-ts(western_arimax$popularity, start= 2003, frequency = 1)\n\n\nfit &lt;- auto.arima(western_ts[, \"popularity\"], xreg = western_ts[, c(2:10)])\nsummary(fit)\n\n\nSeries: western_ts[, \"popularity\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      intercept  instrumentalness  valence  danceability   energy  loudness\n        29.2442           25.4228   2.7944       19.9559  22.4109   -3.0023\ns.e.     6.1730            4.9852   2.5866        4.9131   4.8018    0.2193\n      speechiness  acousticness  liveness   tempo\n          -10.654        3.4309    4.8872  0.0460\ns.e.        2.557        1.7774    3.5788  0.0196\n\nsigma^2 = 0.5384:  log likelihood = -13.98\nAIC=49.96   AICc=87.67   BIC=60.35\n\nTraining set error measures:\n                        ME      RMSE       MAE          MPE      MAPE\nTraining set -1.709042e-12 0.5050281 0.3581341 -0.003699454 0.4203044\n                   MASE       ACF1\nTraining set 0.09900813 -0.2665946\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 12.278, df = 4, p-value = 0.0154\n\nModel df: 0.   Total lags used: 4\n\n\n\n\nAfter regressing on the musical characteristics, it was found that instrumentalness, danceability, energy, loudness, and speechiness were significant to the popularity score. The residuals of the narrowed done model were approximately siginificant. Thus, using those residuals, the model we found manually was ARIMA(0,0,1), which produces a much smaller AIC, BIC, and high p-values in the Ljung-Box statistic test.\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = western_ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = popularity ~ instrumentalness + valence + danceability + \n    energy + loudness + speechiness + acousticness + liveness + \n    tempo, data = western_ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.21304 -0.22353 -0.02568  0.18434  1.03858 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       29.24416    8.96907   3.261  0.00983 ** \ninstrumentalness  25.42282    7.24327   3.510  0.00662 ** \nvalence            2.79438    3.75825   0.744  0.47612    \ndanceability      19.95588    7.13849   2.796  0.02086 *  \nenergy            22.41091    6.97679   3.212  0.01062 *  \nloudness          -3.00235    0.31865  -9.422 5.86e-06 ***\nspeechiness      -10.65397    3.71518  -2.868  0.01855 *  \nacousticness       3.43089    2.58246   1.329  0.21670    \nliveness           4.88717    5.19989   0.940  0.37183    \ntempo              0.04601    0.02841   1.619  0.13980    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7338 on 9 degrees of freedom\nMultiple R-squared:  0.9739,    Adjusted R-squared:  0.9478 \nF-statistic: 37.31 on 9 and 9 DF,  p-value: 4.605e-06\n\n\n\n\nCode\nres.fit&lt;-ts(residuals(fit.reg), start= 2003, frequency = 1)\nggAcf(res.fit)\n\n\n\n\n\nCode\nggPacf(res.fit)\n\n\n\n\n\n\n\n\nCall:\nlm(formula = popularity ~ instrumentalness + danceability + energy + \n    loudness + speechiness, data = western_arimax)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1513 -0.4876  0.0740  0.2559  1.2475 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       41.7766     6.8559   6.094 3.82e-05 ***\ninstrumentalness  27.6554     6.7584   4.092  0.00127 ** \ndanceability      12.0556     4.4759   2.693  0.01842 *  \nenergy            22.4650     5.6606   3.969  0.00160 ** \nloudness          -3.0750     0.2824 -10.888 6.65e-08 ***\nspeechiness       -9.7780     3.3741  -2.898  0.01246 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7755 on 13 degrees of freedom\nMultiple R-squared:  0.9579,    Adjusted R-squared:  0.9417 \nF-statistic: 59.15 on 5 and 13 DF,  p-value: 1.753e-08\n\n\n\n\n\n\n\n\n\n\nCode\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*9),nrow=9) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:2)# p=0,1,2,3,4 : 5\n{\n  for(q in 0:2)# q=0,1,2,3,4 :5\n  {\n    model&lt;- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n43.03641\n45.86973\n44.63641\n\n\n0\n0\n1\n32.70444\n36.48220\n35.56158\n\n\n0\n0\n2\n34.36021\n39.08240\n38.97559\n\n\n1\n0\n0\n42.10240\n45.88016\n44.95955\n\n\n1\n0\n1\n34.58239\n39.30459\n39.19778\n\n\n1\n0\n2\n36.69395\n42.36058\n43.69395\n\n\n2\n0\n0\n38.46446\n43.18666\n43.07985\n\n\n2\n0\n1\n34.17954\n39.84618\n41.17954\n\n\n2\n0\n2\n36.16657\n42.77764\n46.34839\n\n\n\n\n\n\n\nCode\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC     BIC     AICc\n2 0 0 1 32.70444 36.4822 35.56158\n\n\nCode\noutput[which.min(output$BIC),] \n\n\n  p d q      AIC     BIC     AICc\n2 0 0 1 32.70444 36.4822 35.56158\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC     BIC     AICc\n2 0 0 1 32.70444 36.4822 35.56158\n\n\n\n\nThe model ARIMA(0,0,1) works siginificantly better than ARIMA(0,0,0) due to the lower AIC, BIC, and AICc numbers.\n\n\nCode\ncapture.output(sarima(res.fit, 0,0,1)) \n\n\n\n\n\n  [1] \"initial  value -0.444053 \"                                                             \n  [2] \"iter   2 value -0.584053\"                                                              \n  [3] \"iter   3 value -0.711732\"                                                              \n  [4] \"iter   4 value -0.761423\"                                                              \n  [5] \"iter   5 value -0.803378\"                                                              \n  [6] \"iter   6 value -0.824293\"                                                              \n  [7] \"iter   7 value -0.835056\"                                                              \n  [8] \"iter   8 value -0.850105\"                                                              \n  [9] \"iter   9 value -0.856745\"                                                              \n [10] \"iter  10 value -0.873162\"                                                              \n [11] \"iter  11 value -0.907580\"                                                              \n [12] \"iter  12 value -0.939204\"                                                              \n [13] \"iter  13 value -0.939504\"                                                              \n [14] \"iter  14 value -0.942382\"                                                              \n [15] \"iter  15 value -0.947395\"                                                              \n [16] \"iter  16 value -0.957184\"                                                              \n [17] \"iter  17 value -0.958494\"                                                              \n [18] \"iter  18 value -0.960669\"                                                              \n [19] \"iter  19 value -0.961846\"                                                              \n [20] \"iter  20 value -0.963423\"                                                              \n [21] \"iter  21 value -0.964349\"                                                              \n [22] \"iter  22 value -0.966120\"                                                              \n [23] \"iter  23 value -0.966716\"                                                              \n [24] \"iter  24 value -0.968789\"                                                              \n [25] \"iter  25 value -0.969010\"                                                              \n [26] \"iter  26 value -0.971421\"                                                              \n [27] \"iter  27 value -0.971905\"                                                              \n [28] \"iter  28 value -0.971958\"                                                              \n [29] \"iter  29 value -0.972358\"                                                              \n [30] \"iter  30 value -0.972492\"                                                              \n [31] \"iter  31 value -0.972897\"                                                              \n [32] \"iter  32 value -0.973022\"                                                              \n [33] \"iter  33 value -0.973421\"                                                              \n [34] \"iter  34 value -0.973551\"                                                              \n [35] \"iter  35 value -0.973946\"                                                              \n [36] \"iter  36 value -0.974080\"                                                              \n [37] \"iter  37 value -0.974471\"                                                              \n [38] \"iter  38 value -0.974609\"                                                              \n [39] \"iter  39 value -0.974995\"                                                              \n [40] \"iter  40 value -0.975136\"                                                              \n [41] \"iter  41 value -0.975518\"                                                              \n [42] \"iter  42 value -0.975664\"                                                              \n [43] \"iter  43 value -0.976041\"                                                              \n [44] \"iter  44 value -0.976190\"                                                              \n [45] \"iter  45 value -0.976562\"                                                              \n [46] \"iter  46 value -0.976716\"                                                              \n [47] \"iter  47 value -0.977083\"                                                              \n [48] \"iter  48 value -0.977242\"                                                              \n [49] \"iter  49 value -0.977604\"                                                              \n [50] \"iter  50 value -0.977767\"                                                              \n [51] \"iter  51 value -0.978123\"                                                              \n [52] \"iter  52 value -0.978291\"                                                              \n [53] \"iter  53 value -0.978642\"                                                              \n [54] \"iter  54 value -0.978814\"                                                              \n [55] \"iter  55 value -0.979161\"                                                              \n [56] \"iter  56 value -0.979337\"                                                              \n [57] \"iter  57 value -0.979678\"                                                              \n [58] \"iter  58 value -0.979859\"                                                              \n [59] \"iter  59 value -0.980194\"                                                              \n [60] \"iter  60 value -0.980380\"                                                              \n [61] \"iter  61 value -0.980710\"                                                              \n [62] \"iter  62 value -0.980901\"                                                              \n [63] \"iter  63 value -0.981225\"                                                              \n [64] \"iter  64 value -0.981421\"                                                              \n [65] \"iter  65 value -0.981739\"                                                              \n [66] \"iter  66 value -0.981940\"                                                              \n [67] \"iter  67 value -0.982252\"                                                              \n [68] \"iter  68 value -0.982458\"                                                              \n [69] \"iter  69 value -0.982765\"                                                              \n [70] \"iter  70 value -0.982975\"                                                              \n [71] \"iter  71 value -0.983276\"                                                              \n [72] \"iter  72 value -0.983492\"                                                              \n [73] \"iter  73 value -0.983787\"                                                              \n [74] \"iter  74 value -0.984008\"                                                              \n [75] \"iter  75 value -0.984296\"                                                              \n [76] \"iter  76 value -0.984522\"                                                              \n [77] \"iter  77 value -0.984805\"                                                              \n [78] \"iter  78 value -0.985036\"                                                              \n [79] \"iter  79 value -0.985312\"                                                              \n [80] \"iter  80 value -0.985549\"                                                              \n [81] \"iter  81 value -0.985819\"                                                              \n [82] \"iter  82 value -0.986061\"                                                              \n [83] \"iter  83 value -0.986324\"                                                              \n [84] \"iter  84 value -0.986572\"                                                              \n [85] \"iter  85 value -0.986829\"                                                              \n [86] \"iter  86 value -0.987082\"                                                              \n [87] \"iter  87 value -0.987332\"                                                              \n [88] \"iter  88 value -0.987590\"                                                              \n [89] \"iter  89 value -0.987834\"                                                              \n [90] \"iter  90 value -0.988098\"                                                              \n [91] \"iter  91 value -0.988335\"                                                              \n [92] \"iter  92 value -0.988604\"                                                              \n [93] \"iter  93 value -0.988835\"                                                              \n [94] \"iter  94 value -0.989110\"                                                              \n [95] \"iter  95 value -0.989334\"                                                              \n [96] \"iter  96 value -0.989614\"                                                              \n [97] \"iter  97 value -0.989831\"                                                              \n [98] \"iter  98 value -0.990116\"                                                              \n [99] \"iter  99 value -0.990327\"                                                              \n[100] \"iter 100 value -0.990618\"                                                              \n[101] \"final  value -0.990618 \"                                                               \n[102] \"stopped after 100 iterations\"                                                          \n[103] \"initial  value -0.444053 \"                                                             \n[104] \"iter   2 value -0.580107\"                                                              \n[105] \"iter   3 value -0.616594\"                                                              \n[106] \"iter   4 value -0.653531\"                                                              \n[107] \"iter   5 value -0.675354\"                                                              \n[108] \"iter   6 value -0.700842\"                                                              \n[109] \"iter   7 value -0.718002\"                                                              \n[110] \"iter   8 value -0.729862\"                                                              \n[111] \"iter   9 value -0.735738\"                                                              \n[112] \"iter  10 value -0.736471\"                                                              \n[113] \"iter  11 value -0.736813\"                                                              \n[114] \"iter  12 value -0.736814\"                                                              \n[115] \"iter  13 value -0.736818\"                                                              \n[116] \"iter  14 value -0.736819\"                                                              \n[117] \"iter  14 value -0.736819\"                                                              \n[118] \"iter  14 value -0.736819\"                                                              \n[119] \"final  value -0.736819 \"                                                               \n[120] \"converged\"                                                                             \n[121] \"$fit\"                                                                                  \n[122] \"\"                                                                                      \n[123] \"Call:\"                                                                                 \n[124] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[125] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[126] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[127] \"\"                                                                                      \n[128] \"Coefficients:\"                                                                         \n[129] \"          ma1   xmean\"                                                                 \n[130] \"      -1.0000  0.0079\"                                                                 \n[131] \"s.e.   0.1608  0.0172\"                                                                 \n[132] \"\"                                                                                      \n[133] \"sigma^2 estimated as 0.1957:  log likelihood = -12.96,  aic = 31.92\"                   \n[134] \"\"                                                                                      \n[135] \"$degrees_of_freedom\"                                                                   \n[136] \"[1] 17\"                                                                                \n[137] \"\"                                                                                      \n[138] \"$ttable\"                                                                               \n[139] \"      Estimate     SE t.value p.value\"                                                 \n[140] \"ma1    -1.0000 0.1608 -6.2187  0.0000\"                                                 \n[141] \"xmean   0.0079 0.0172  0.4582  0.6526\"                                                 \n[142] \"\"                                                                                      \n[143] \"$AIC\"                                                                                  \n[144] \"[1] 1.680028\"                                                                          \n[145] \"\"                                                                                      \n[146] \"$AICc\"                                                                                 \n[147] \"[1] 1.719502\"                                                                          \n[148] \"\"                                                                                      \n[149] \"$BIC\"                                                                                  \n[150] \"[1] 1.82915\"                                                                           \n[151] \"\"                                                                                      \n\n\nCode\ncapture.output(sarima(res.fit, 0,0,0)) \n\n\n\n\n\n [1] \"initial  value -0.444053 \"                                                             \n [2] \"iter   1 value -0.444053\"                                                              \n [3] \"final  value -0.444053 \"                                                               \n [4] \"converged\"                                                                             \n [5] \"initial  value -0.444053 \"                                                             \n [6] \"iter   1 value -0.444053\"                                                              \n [7] \"final  value -0.444053 \"                                                               \n [8] \"converged\"                                                                             \n [9] \"$fit\"                                                                                  \n[10] \"\"                                                                                      \n[11] \"Call:\"                                                                                 \n[12] \"arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \"\n[13] \"    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \"       \n[14] \"    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\"                      \n[15] \"\"                                                                                      \n[16] \"Coefficients:\"                                                                         \n[17] \"       xmean\"                                                                          \n[18] \"      0.0000\"                                                                          \n[19] \"s.e.  0.1472\"                                                                          \n[20] \"\"                                                                                      \n[21] \"sigma^2 estimated as 0.4114:  log likelihood = -18.52,  aic = 41.05\"                   \n[22] \"\"                                                                                      \n[23] \"$degrees_of_freedom\"                                                                   \n[24] \"[1] 18\"                                                                                \n[25] \"\"                                                                                      \n[26] \"$ttable\"                                                                               \n[27] \"      Estimate     SE t.value p.value\"                                                 \n[28] \"xmean        0 0.1472       0       1\"                                                 \n[29] \"\"                                                                                      \n[30] \"$AIC\"                                                                                  \n[31] \"[1] 2.160296\"                                                                          \n[32] \"\"                                                                                      \n[33] \"$AICc\"                                                                                 \n[34] \"[1] 2.17268\"                                                                           \n[35] \"\"                                                                                      \n[36] \"$BIC\"                                                                                  \n[37] \"[1] 2.259711\"                                                                          \n[38] \"\"                                                                                      \n\n\n\n\nUsing corss validation, we can confirm that ARIMA(0,0,1), or fit2, is the better model since is stays at a lower RMSE for the majority of the time in the plot below.\n\n\nCode\nn &lt;- length(res.fit)\nk &lt;- 3  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 &lt;- matrix(NA, 8)\nrmse2 &lt;- matrix(NA, 8)\n\nst &lt;- tsp(res.fit)[1] + (k - 1)\n\nfor (i in 1:8) {\n  # Define the training set\n  train_end &lt;- st + i - 1\n  xtrain &lt;- window(res.fit, end = train_end)\n\n  # Define the testing set\n  test_start &lt;- train_end + 1\n  test_end &lt;- min(st + i, tsp(res.fit)[2])\n  xtest &lt;- window(res.fit, start = test_start, end = test_end)\n\n  fit &lt;- Arima(xtrain, order = c(0, 0, 0), include.drift = TRUE, method = \"ML\")\n  fcast &lt;- forecast(fit, h = 4)\n\n  fit2 &lt;- Arima(xtrain, order = c(0, 0, 1), include.drift = TRUE, method = \"ML\")\n  fcast2 &lt;- forecast(fit2, h = 4)\n\n  rmse1[i] &lt;- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] &lt;- sqrt((fcast2$mean - xtest)^2)\n}\n\nplot(1:8, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:8, rmse1, type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\"), col = 2:3, lty = 1)\n\n\n\n\n\n\n\nThe final fit is ARIMA(0,0,1) of the residuals of regression on popularity with predictors instrumentalness, danceability, energy, loudness, and speechiness.\n\n\nCode\nfit &lt;- Arima(western_ts[, \"popularity\"], order=c(0,0,1), xreg = western_ts[, c('instrumentalness', 'danceability','energy', 'loudness', 'speechiness')])\n\nsummary(fit)\n\n\nSeries: western_ts[, \"popularity\"] \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n          ma1  intercept  instrumentalness  danceability   energy  loudness\n      -1.0000    42.7338           30.6405       10.7786  21.7724   -3.1092\ns.e.   0.1336     3.9446            4.2529        2.6569   3.4500    0.1412\n      speechiness\n         -10.2244\ns.e.       2.2792\n\nsigma^2 = 0.271:  log likelihood = -11.69\nAIC=39.38   AICc=53.78   BIC=46.93\n\nTraining set error measures:\n                       ME      RMSE       MAE         MPE      MAPE       MASE\nTraining set -0.009193279 0.4137218 0.3465777 -0.01410969 0.4060604 0.09581328\n                   ACF1\nTraining set -0.2511563\n\n\n\n\n\n\nCode\nmusic_fit &lt;- auto.arima(western_ts[, \"instrumentalness\"]) \nft &lt;- forecast(music_fit)\n\nmusic_fit &lt;- auto.arima(western_ts[, \"danceability\"]) \nft2 &lt;- forecast(music_fit)\n\nmusic_fit &lt;- auto.arima(western_ts[, \"energy\"]) \nft3 &lt;- forecast(music_fit)\n\nmusic_fit &lt;- auto.arima(western_ts[, \"loudness\"]) \nft4 &lt;- forecast(music_fit)\n\nmusic_fit &lt;- auto.arima(western_ts[, \"speechiness\"]) \nft5 &lt;- forecast(music_fit)\n\nxreg = cbind(INSTRUMENTAL = ft$mean,\n            DANCE = ft2$mean,\n            ENERGY = ft3$mean,\n            LOUDNESS = ft4$mean,\n            SPEECH = ft5$mean)\n\nfcast &lt;- forecast(fit, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Popularity\")\n\n\n\n\n\n\n\n\nThe forecasting for Western Artists with musical charactericals as predictors did enhance the overall predition, we a approximately constant popularity going forward. From this, we could say that popularity for Western artists seems to do more with musical characteristics than KPOP artists. This may be because KPOP artists rise to fame for a variety of reasons, viral music videos, dancing, viral moments, social media presence, and more. Therefore, for KPOP, it may not be a good metric to only use musical characteristics to define popularity."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Before making any conclusions about the effect of KPOP of the Western Music Industry, let’s take a look at out big picture to summarize each sector of analysis."
  },
  {
    "objectID": "conclusions.html#charting",
    "href": "conclusions.html#charting",
    "title": "Conclusions",
    "section": "Charting",
    "text": "Charting\n\nQuestion 1: How do Western Artists compare to KPOP on the Billboard Hot 100 over time?\n\n\n\n\n\n\nFrom the Billboard Hot 100 Chart, we can see that Western Artists dominated the charts in comparison to KPOP artists, with BTS being the only KPOP artists to make it on the Hot 100 from 1958 - 2021. This is due to a number of factors, the main reason being that KPOP on recently entered in the Western music industry and radio play is little to none for KPOP songs in the United States. Radio play is important to note as that is one of the metrics Billboard uses for charting."
  },
  {
    "objectID": "conclusions.html#musical-characteristics",
    "href": "conclusions.html#musical-characteristics",
    "title": "Conclusions",
    "section": "Musical Characteristics",
    "text": "Musical Characteristics\n\nQuestion 2: What can be said about the musical characteristics between Western and KPOP artists?\nQuestion 7: Can we predict popularity of KPOP verses Western music in the coming years?\nQuestion 9: How do musical characterists effect forecasted popularity between Western and KPOP artists?\n\n\n\nThus far, we were able to conclude that KPOP artists, on average, tend to have a higher energy, loudness, and instrumentalness than Western artists. Additionally, KPOP artists also tended to have larger valence than Western artists, with the exception of Beyoncé and Drake. This is due to the fact that almost all KPOP groups have designated rappers for every song in comparison to Western pop singers that don’t.\nWhen predicting popularity, the data set at hand was very limited due to new regulations on Spotify’s API. Thus the predictions for both KPOP and Western artists didn’t sufice to the full potentional of predictive modeling. However, the conclusions we were able to see was that while KPOP artist’s popularity weren’t effected by any of the musical characteristics in Spotify’s API, Western artists had a better prediction when predited on musical attributes. Again, this could be due to the reach of popularity KPOP artists bring elsewhere like social media, concerts, and other live events. Thus we can say that while we are still unsure of the popularity in the future, based on the trends we’ve looked at elsewhere, KPOP is continuing to grow in its popularity globally."
  },
  {
    "objectID": "conclusions.html#globalization",
    "href": "conclusions.html#globalization",
    "title": "Conclusions",
    "section": "Globalization",
    "text": "Globalization\n\nQuestion 8: Has cultural globalization been impacted since the onset of KPOP in West?\nQuestion 3: What is the relationship between cultural globalization in the U.S. and South Korean international tourism?\n\n\n\nSince the onset of KPOP charting in the West, we saw through the AMIRA model that the cultural globalization index within the United States continued to increase, with the forecast also showing an increase in the index. However, pinpointing whether this was due to Korean cultural exchange was the second step, which is why I moved to using an AR + ARCH model with Korean tourism as an additional metric.\nWhen taking into account Korean international passengers on cultural globalization within the US, we saw that globalization trended downwards in the coming years. This could mean that when using international travel into South Korea as a predictor, cultural globalization decreases in the United States. Thus, this could mean that in the future, we may be seeing a slight decrease in Korean media being embraced within the United States."
  },
  {
    "objectID": "conclusions.html#record-label-stocks",
    "href": "conclusions.html#record-label-stocks",
    "title": "Conclusions",
    "section": "Record Label Stocks",
    "text": "Record Label Stocks\n\nQuestion 4: Is KPOP as a whole propelling into the West or is it a a specific artist/group?\nQuestion 5: Does HYBE seem to fall under the Western music industry, the KPOP industry, or are they in a new league?\nQuestion 6: Accounting for volatility, will HYBE continue its current success, specifically in comparison to the traditionally Western labels?\n\n\n\n\n\n\n\n\n\n\n\nAfter modeling the stock prices of HYBE against both Western and Korean record labels, we saw that HYBE seems to have a negative foecasting when predicted on the “Big 3”, while HYBE sees a positively trended forecast when predicted on Western labels Universal Music Group and SONY. Contrarily, the other Korean lables seem to have postively trended forecast when predicted with HYBE. Thus, rather than KPOP in general dominating the music industry, we can say that artists from HYBE, specifically their most popular artist, BTS, has had the biggest impact on the stock market.\n\n\nSimilarly, it can be noted that through the analysis of the VAR models created as well as the ARCH model, HYBE seems to be significantly related to Korean record labels in comparison to the Western records labels.\nLastly, we can see from the ARCH model that accounting for all the volatility in the data results in a constant prediction, for the next 50 days. This could mean a number of things, but one that we can speak about is that the predictors (SM, JYP, and YG) are not the best indicators of predicting HYBE stock prices. Thus, ARCH may not be the best model for predicting HYBE stock."
  },
  {
    "objectID": "conclusions.html#final-thoughts",
    "href": "conclusions.html#final-thoughts",
    "title": "Conclusions",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\n10. What combined conclusions can be drawn from the four analytical venues demonstrated in the big-ideas chart?\n\nThus, we can say that KPOP has had a significant effect on the music industry as of recent. We have see that KPOP artists like BTS has recently entered the Billboard Hot 100, taking the first steps for the genre. Additionally, we saw that musically, popular music in the US is gravitating towards an R&B and Hip Hop sound, which, many KPOP groups have garnered a sound that blends both genres with intense dancability. In terms of a cultural shift, we can only see a slight increase in korean globalization within the Western world, however, record labels like HYBE have carved their own path as one of the leading record labels in the entire industry. It’s appart that KPOP groups have always been inspired by Western artists, but it is also clear that many groups are now defining a genre that will be a stand out in the Western music industry."
  },
  {
    "objectID": "data-vizes.html",
    "href": "data-vizes.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Problem 1:\nThis plot depicts the stock prices for large fashion companies pre and post the COVID-19 pandemic. The fashion brands included are Adidas, Christian Dior, H&M, Nike, Puma, and Ross. Looking a combination of high end and low end brands, we can see that all companies faced a massive downtick in price evaluations in March of 2020, signaling the announcement of the global COVID-19 pandemic. Something to point out is that Christian Dior had a spike in price around mid-2021, resulting in a high uptick compared to the other brands. I was unable to find a reason as to why this uptick occured, but we can assume it was due to the highend nature of the brand.\n\n\n\n\n\n\n\nProblem 2:\nThe following graph depicts the precipication in Washington D.C. from January 2021 - January 2022. The graph shows periodic spikes throughout the year, however, the most precipitation occured in the third quarter of the year, with the highest amount occuring on August 20, 2021 at 3.76. The driest part of the year was the last quarter.\n\n\n\n\n\n\n\n\nProblem 3:\nBelow, is a graph depicting the unemployement rate for women in the United States of America throughout the 2000’s. This data was sourced from FRED. From this graph we can see 2 major historical moments that caused women’s unemployement. The 2008 market crash caused the first steep incline in unemployment. However, the market was able to recover such that the women who did work were able to in the 2010’s. The next major event was the COVID-19 global pandemic. Due to a global shutdown, there was a major spike in unemployement. This may be due to several women also being mothers and needing to stay home for childcare as a result of childcare services being unavailable at the time.\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "financial-ts.html",
    "href": "financial-ts.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "In order to conduct a financial time series analysis, we’ll continue to look at the stock prices between US majority music labels and KPOP public record labels. In particular, let’s take a look at HYBE, a music label that is the first to support artists both from the KPOP and the Western music industry. Similar to out analysis of ARIMAX and VAR models, we’ll use ARCH/GARCH models in order to answer the following questions:\nAnswering these two questions can help us better understand the strength of HYBE in both markets as well as which industry seems to be be the direction in which the most successful record label will delve into."
  },
  {
    "objectID": "financial-ts.html#hybe-and-the-kpop-record-labels",
    "href": "financial-ts.html#hybe-and-the-kpop-record-labels",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "HYBE and the KPOP record labels:",
    "text": "HYBE and the KPOP record labels:\n\nVisualizationRegressionUsing auto.arimaManual AR/ARMA/ARIMA ModelARCH/GARCHModel SelectionEquation:\n\n\nFrom the plot, we can see that all four stock prices are not stationary, as they all experience volatility to some extent. In terms of HYBE, we can see the largest magnitude and most volatility in the data, with sharp upward and downward trends in the data. Thus, we’ll see if using “The Big Three” (SM, JYP, YG) and accounting for volatility in an ARCH/GARCH model will provide better predictions.\n\n\nData Gathering\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-09-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- UMGP[which(rowSums(is.na(UMGP)) &gt; 0),]\ndf_na_cols &lt;- UMGP[, which(colSums(is.na(UMGP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nSONY &lt;- data.frame(SONY$SONY.Adjusted)\nSONY &lt;- SONY %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date &lt;- as.Date(min(SONY$Date))  \nend_date &lt;- as.Date(max(SONY$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSONY &lt;- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SONY[which(rowSums(is.na(SONY)) &gt; 0),]\ndf_na_cols &lt;- SONY[, which(colSums(is.na(SONY)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\n#Creating a subset of only Korean Record label stock data\ndf &lt;- HYBE %&gt;%\n  left_join(SM, by = 'Date') %&gt;%\n  left_join(YG, by = 'Date') %&gt;%\n  left_join(JYP, by = 'Date')\n\n#Converting to time series \nhybe &lt;- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsm &lt;- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)\nyg &lt;- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)\njyp &lt;- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf_ts &lt;- cbind(hybe, sm, yg, jyp)\ncolnames(df_ts) &lt;- c(\"hybe\", \"sm\", \"yg\", \"jyp\")\n\n#Visualize\nautoplot(df_ts)\n\n\n\n\n\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df)), size = 0.8 * nrow(df)) \ntrain &lt;- df[train_indices, ] \ntest &lt;- df[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-87.63 -24.10   0.01  20.72  74.25 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.108e+03  1.769e+02  11.916   &lt;2e-16 ***\nDate        -1.133e-01  9.432e-03 -12.015   &lt;2e-16 ***\nSM_Price     1.475e+00  1.332e-01  11.073   &lt;2e-16 ***\nYG_Price     5.063e+00  2.302e-01  21.990   &lt;2e-16 ***\nJYP_Price   -1.314e+00  1.327e-01  -9.906   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29 on 836 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:   0.63 \nF-statistic: 358.5 on 4 and 836 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.58667065622527\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 31.5268851064457\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -8.9644, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 836:  log likelihood = -4022.7\nAIC=8047.39   AICc=8047.4   BIC=8052.13\n\nTraining set error measures:\n                       ME     RMSE      MAE MPE MAPE      MASE       ACF1\nTraining set 1.360155e-15 28.91303 23.92612 100  100 0.6951107 -0.0701053\n\n\n\n\nThus, from the manually approach, the model with the lowest AIC value is AR(1). Thus, we’ll use this model within our approach to find the best ARCH/GARCH model.\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n8049.256\n8063.459\n8049.284\n\n\n0\n0\n1\n8047.069\n8066.007\n8047.116\n\n\n0\n0\n2\n8048.019\n8071.692\n8048.091\n\n\n0\n0\n3\n8049.359\n8077.767\n8049.460\n\n\n0\n0\n4\n8050.912\n8084.054\n8051.046\n\n\n1\n0\n0\n8046.786\n8065.725\n8046.834\n\n\n1\n0\n1\n8047.856\n8071.529\n8047.928\n\n\n1\n0\n2\n8049.746\n8078.154\n8049.847\n\n\n1\n0\n3\n8050.935\n8084.078\n8051.070\n\n\n1\n0\n4\n8052.780\n8090.657\n8052.953\n\n\n2\n0\n0\n8047.880\n8071.553\n8047.952\n\n\n2\n0\n1\n8049.757\n8078.164\n8049.858\n\n\n2\n0\n2\n8051.781\n8084.923\n8051.915\n\n\n2\n0\n3\n8050.634\n8088.511\n8050.807\n\n\n2\n0\n4\n8054.804\n8097.415\n8055.020\n\n\n3\n0\n0\n8049.542\n8077.949\n8049.642\n\n\n3\n0\n1\n8051.002\n8084.144\n8051.136\n\n\n3\n0\n2\n8050.141\n8088.017\n8050.314\n\n\n3\n0\n3\n8055.183\n8097.794\n8055.400\n\n\n3\n0\n4\n8054.533\n8101.879\n8054.798\n\n\n4\n0\n0\n8050.818\n8083.960\n8050.953\n\n\n4\n0\n1\n8052.676\n8090.552\n8052.849\n\n\n4\n0\n2\n8053.909\n8096.520\n8054.125\n\n\n4\n0\n3\n8055.875\n8103.221\n8056.140\n\n\n4\n0\n4\n8046.913\n8098.993\n8047.231\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 8049.256 8063.459 8049.284\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 0 0 8046.786 8065.725 8046.834\n\n\n\n\nSince the original data doesn’t show time-varying volatility within stock prices, we’ll go forward without testing the GARCH model. Thus, the two models we’ll look at is AR + ARCH and just ARCH in order to find the best one at predicting HYBE prices given its volatility.\nFrom the manually calculation, we can see that the best ARCH model is ARCH(1,4). The next step now is to check which whether the AR+ARCH model or the ARCH model itself is the best.\n\n\nARCH selection\nbest_ar_model &lt;- Arima(lm.residuals,order=c(1,0,0))\nar.res &lt;- best_ar_model$residuals\n\nacf(ar.res^2)\n\n\n\n\n\nARCH selection\npacf(ar.res^2)\n\n\n\n\n\nARCH selection\narch_model &lt;- list() ## set counter\ncc &lt;- 1\n\nfor (p in 1:4) {\n  for (q in 1:4) {\n    arch_model[[cc]] &lt;- garch(ar.res,order=c(q,p),trace=F)\n    cc &lt;- cc + 1\n  }\n} \n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(arch_model, AIC) ## model with lowest AIC is the best\narch_model[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = ar.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4  \n6.246e+02  9.202e-12  6.121e-02  6.027e-02  6.121e-02  6.023e-02  \n\n\n\n\nBased on the two models, looking at the Ljung-Box Test and the AIC values, we can say that ARCH(1,4) is the best model to predict HYBE stock prices.\n\n\nModel selection\nsummary(garchFit(~garch(1,4), ar.res, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 4), data = ar.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 4)\n&lt;environment: 0x7fdf194a2e58&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2        beta3  \n-0.00134274  21.05589755   0.00000001   0.97494048   0.00000001   0.00000001  \n      beta4  \n 0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.343e-03   9.945e-01   -0.001    0.999\nomega   2.106e+01   1.290e+01    1.632    0.103\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.749e-01   1.978e+01    0.049    0.961\nbeta2   1.000e-08   1.715e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.602    normalized:  -4.78074 \n\nDescription:\n Fri Dec  8 04:43:02 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  18.0638061 1.195348e-04\n Shapiro-Wilk Test  R    W       0.9847035 1.076037e-07\n Ljung-Box Test     R    Q(10)   5.0485163 8.879136e-01\n Ljung-Box Test     R    Q(15)  23.2427741 7.911959e-02\n Ljung-Box Test     R    Q(20)  27.8650892 1.126317e-01\n Ljung-Box Test     R^2  Q(10)   7.4176133 6.855185e-01\n Ljung-Box Test     R^2  Q(15)   8.7640540 8.895276e-01\n Ljung-Box Test     R^2  Q(20)  18.6867078 5.422662e-01\n LM Arch Test       R    TR^2    7.5249668 8.210648e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.578127 9.617535 9.577990 9.593230 \n\n\nModel selection\nsummary(garchFit(~arma(1, 0) + garch(1, 4), ar.res, trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + garch(1, 4), data = ar.res, \n    trace = F) \n\nMean and Variance Equation:\n data ~ arma(1, 0) + garch(1, 4)\n&lt;environment: 0x7fdefc568438&gt;\n [data = ar.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1        omega       alpha1        beta1        beta2  \n-0.00144214   0.00236649  21.18647953   0.00000001   0.97478384   0.00000001  \n      beta3        beta4  \n 0.00000001   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)\nmu     -1.442e-03   9.949e-01   -0.001    0.999\nar1     2.366e-03   3.450e-02    0.069    0.945\nomega   2.119e+01   1.295e+01    1.636    0.102\nalpha1  1.000e-08         NaN      NaN      NaN\nbeta1   9.748e-01   1.946e+01    0.050    0.960\nbeta2   1.000e-08   1.567e+01    0.000    1.000\nbeta3   1.000e-08         NaN      NaN      NaN\nbeta4   1.000e-08         NaN      NaN      NaN\n\nLog Likelihood:\n -4020.441    normalized:  -4.780548 \n\nDescription:\n Fri Dec  8 04:43:03 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  17.9142074 1.288188e-04\n Shapiro-Wilk Test  R    W       0.9847546 1.124557e-07\n Ljung-Box Test     R    Q(10)   5.0407836 8.884370e-01\n Ljung-Box Test     R    Q(15)  23.1202713 8.161946e-02\n Ljung-Box Test     R    Q(20)  27.8526110 1.129345e-01\n Ljung-Box Test     R^2  Q(10)   7.4106948 6.861867e-01\n Ljung-Box Test     R^2  Q(15)   8.7613151 8.896631e-01\n Ljung-Box Test     R^2  Q(20)  18.7545990 5.378250e-01\n LM Arch Test       R    TR^2    7.4989478 8.229593e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n9.580122 9.625159 9.579943 9.597382 \n\n\nModel selection\nfina_fit &lt;- garchFit(~garch(1,4), ar.res, trace = F)\n\npredict(fina_fit, n.ahead = 50, plot = TRUE)\n\n\n\n\n\n   meanForecast meanError standardDeviation lowerInterval upperInterval\n1  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n2  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n3  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n4  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n5  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n6  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n7  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n8  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n9  -0.001342735  28.98684          28.98684      -56.8145      56.81182\n10 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n11 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n12 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n13 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n14 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n15 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n16 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n17 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n18 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n19 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n20 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n21 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n22 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n23 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n24 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n25 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n26 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n27 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n28 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n29 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n30 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n31 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n32 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n33 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n34 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n35 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n36 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n37 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n38 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n39 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n40 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n41 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n42 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n43 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n44 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n45 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n46 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n47 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n48 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n49 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n50 -0.001342735  28.98684          28.98684      -56.8145      56.81182\n\n\n\n\nBased on the model above, we’ll say that equation for the model is as follows:\n\\(X_t = 2108 -0.1133z_1+ 1.475z_2 + 5.063z_3 - 1.314z_4\\)\n\\(y^*_t = y_t−0.00134274\\)\n\\(y_t = \\sigma_t \\epsilon_t\\)\n\\(\\sigma^2_t = 21.05589755 + 0.00000001y^2_{t-1} + 0.97494048\\sigma^2_{t-1} + 0.00000001\\sigma^2_{t-2} + 0.00000001\\sigma^2_{t-3} + 0.00000001\\sigma^2_{t-4}\\)"
  },
  {
    "objectID": "financial-ts.html#hybe-and-the-western-record-labels",
    "href": "financial-ts.html#hybe-and-the-western-record-labels",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "HYBE and the Western record labels:",
    "text": "HYBE and the Western record labels:\n\nVisualizationRegressionUsing auto.arimaManual AR/ARMA/ARIMA Model\n\n\nAgain, from an initial visualization, it doesn’t appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, and Universal Music Group stock prices has a stationary trend.\n\n\nData Visualization\ndf2 &lt;- HYBE %&gt;%\n  left_join(UMGP, by = 'Date') %&gt;%\n  left_join(SONY, by = 'Date') %&gt;%\n  drop_na()\n\nhybe &lt;- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)\numgp &lt;- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)\nsony &lt;- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)\n\ndf2_ts &lt;- cbind(hybe, umgp, sony)\ncolnames(df2_ts) &lt;- c(\"hybe\", \"umgp\", \"sony\")\n\nautoplot(df2_ts)\n\n\n\n\n\n\n\nWe can see that the residuals are stationary in accordance to the Dickey-Fuller test. Additionally, both the ACF and the PACF plots show that the residuals are not auto correlated since the plot is approximately stationary. Thus, we can proceed with these residuals.\nNext, using these residuals, we’ll first find the best ARMA/AR/ARIMA model. We’ll do this is two ways, first using auto.arima and then manually by running through multiple models.\n\n\nAuto.arima() code\nset.seed(5600)\n\n#Doing an 80/20 split\ntrain_indices &lt;- sample(seq_len(nrow(df2)), size = 0.8 * nrow(df2)) \ntrain &lt;- df2[train_indices, ] \ntest &lt;- df2[-train_indices, ]\n\n#First, fitting the model: \nmodel &lt;- lm(HYBE_Price ~ ., data = train)\n\n#Checking residuals\nsummary(model)\n\n\n\nCall:\nlm(formula = HYBE_Price ~ ., data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.272 -21.250  -1.858  20.612 106.626 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.644e+02  7.338e+01 -13.142  &lt; 2e-16 ***\nDate         4.490e-02  3.611e-03  12.432  &lt; 2e-16 ***\nUMGP_Price   7.328e+00  1.900e+00   3.856 0.000124 ***\nSONY_Price   2.898e+00  9.427e-02  30.737  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.64 on 836 degrees of freedom\nMultiple R-squared:  0.6664,    Adjusted R-squared:  0.6652 \nF-statistic: 556.7 on 3 and 836 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAuto.arima() code\nlm_predictions &lt;- predict(model, newdata = test)\nr_squared &lt;- cor(test$HYBE_Price, lm_predictions)^2\nrmse &lt;- sqrt(mean((test$HYBE_Price - lm_predictions)^2))\nprint(paste(\"R-squared:\", r_squared))\n\n\n[1] \"R-squared: 0.658219040875386\"\n\n\nAuto.arima() code\nprint(paste(\"RMSE:\", rmse))\n\n\n[1] \"RMSE: 28.437046388039\"\n\n\nAuto.arima() code\nlm.residuals &lt;- residuals(model)\nacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\npacf(lm.residuals)\n\n\n\n\n\nAuto.arima() code\nadf.test(lm.residuals)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  lm.residuals\nDickey-Fuller = -10.175, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nAuto arima provided the model where p,q,d = 0. This model would not correctly predict, therefore we turn to the manual approach for better output.\n\n\nAuto.arima() code\narima_model &lt;- auto.arima(lm.residuals)\nsummary(arima_model)\n\n\nSeries: lm.residuals \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 760.3:  log likelihood = -3978.08\nAIC=7958.17   AICc=7958.17   BIC=7962.9\n\nTraining set error measures:\n                      ME     RMSE     MAE MPE MAPE      MASE        ACF1\nTraining set 8.50221e-16 27.57402 22.5766 100  100 0.7008286 -0.04678467\n\n\n\n\nFrom both auto.arima() and the manual approach, we can see that no AR/ARMA/ARIMA model was created from the residuals of the linear regression model. Thus, we can say that there is no autocorrelation in the data that would call for an AR/ARMA/ARIMA model.\n\n\nManual AR/ARMA/ARIMA code\ni=1\nd=0\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) \n\n\nfor (p in 1:5)# p=0, 1,2,3, 4\n{\n  for(q in 1:5)# q=0, 1,2,3,4\n  {\n    if(p-1+d+q-1&lt;=10) #usual threshold\n    {\n      model&lt;- Arima(lm.residuals,order=c(p-1,d,q-1),include.drift=TRUE) \n      ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n      i=i+1\n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n7959.626\n7973.826\n7959.655\n\n\n0\n0\n1\n7959.481\n7978.415\n7959.529\n\n\n0\n0\n2\n7961.453\n7985.120\n7961.525\n\n\n0\n0\n3\n7963.207\n7991.607\n7963.308\n\n\n0\n0\n4\n7963.960\n7997.094\n7964.095\n\n\n1\n0\n0\n7959.516\n7978.450\n7959.564\n\n\n1\n0\n1\n7961.465\n7985.132\n7961.537\n\n\n1\n0\n2\n7963.449\n7991.849\n7963.550\n\n\n1\n0\n3\n7964.976\n7998.110\n7965.110\n\n\n1\n0\n4\n7965.596\n8003.463\n7965.769\n\n\n2\n0\n0\n7961.429\n7985.096\n7961.501\n\n\n2\n0\n1\n7963.435\n7991.835\n7963.535\n\n\n2\n0\n2\n7965.355\n7998.489\n7965.489\n\n\n2\n0\n3\n7955.273\n7993.140\n7955.446\n\n\n2\n0\n4\n7967.593\n8010.193\n7967.809\n\n\n3\n0\n0\n7963.309\n7991.709\n7963.409\n\n\n3\n0\n1\n7965.066\n7998.199\n7965.200\n\n\n3\n0\n2\n7967.505\n8005.372\n7967.678\n\n\n3\n0\n3\n7960.460\n8003.061\n7960.677\n\n\n3\n0\n4\n7967.138\n8014.472\n7967.403\n\n\n4\n0\n0\n7963.706\n7996.840\n7963.841\n\n\n4\n0\n1\n7965.446\n8003.313\n7965.619\n\n\n4\n0\n2\n7965.224\n8007.825\n7965.441\n\n\n4\n0\n3\n7967.208\n8014.542\n7967.473\n\n\n4\n0\n4\n7969.184\n8021.251\n7969.503\n\n\n\n\n\nManual AR/ARMA/ARIMA code\n#Best model: \ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC     BIC     AICc\n14 2 0 3 7955.273 7993.14 7955.446\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 0 0 7959.626 7973.826 7959.655\n\n\nManual AR/ARMA/ARIMA code\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC     BIC     AICc\n14 2 0 3 7955.273 7993.14 7955.446\n\n\n\n\n\nFrom this information, we can deduce the fact that Western record labels (Warner Group and Universal Music Group) do not have an affect on HYBE stock prices. Thus, we can say that HYBE’s success and future would best be predicted by the performance of other KPOP record labels (SM, JYP, and YG). Thus, we can say that for the best financial outcomes, music companies may be looking to KPOP groups and their marketing and music strategies in the coming future."
  },
  {
    "objectID": "spotify_data_gathering.html",
    "href": "spotify_data_gathering.html",
    "title": "Time Series - Shriya Chinthak",
    "section": "",
    "text": "library(ggplot2)\nlibrary(knitr)\nlibrary(spotifyr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(plotly)\n\nSys.setenv(SPOTIFY_CLIENT_ID = \"48875e31f589401f83c6bd43005d94f7\")\nSys.setenv(SPOTIFY_CLIENT_SECRET = \"d215e4ea690d4b9b9c1c5e0afbb113a5\")\n\n\naccess_token &lt;- get_spotify_access_token()\nartists &lt;- c(\"Drake\", \"Beyonce\", \"Harry Styles\", \"Taylor Swift\", \"Twice\", \"BTS\", \"EXO\", \"BlackPink\")\n\nget_combined_artists_data &lt;- function(artist_names) {\n  combined_data &lt;- data.frame()\n  \n  for (artist_name in artist_names) {\n    artist_data &lt;- get_artist_audio_features(artist_name)\n    \n    selected_columns &lt;- c(\"artist_name\", \"instrumentalness\", \"valence\", \"danceability\",\n                           \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n                           \"liveness\", \"tempo\", \"track_name\", \"track_id\", \"album_name\",\n                           \"album_release_year\", \"album_release_date\")\n    \n    artist_df &lt;- data.frame(artist_data[selected_columns])\n    \n    colnames(artist_df) &lt;- c(\"artist_name\", \"instrumentalness\", \"valence\", \"danceability\",\n                             \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n                             \"liveness\", \"tempo\", \"track_name\", \"track_id\", \"album_name\",\n                             \"album_release_year\", \"album_release_date\")\n    \n    # Combine data for each artist\n    combined_data &lt;- bind_rows(combined_data, artist_df)\n  }\n  \n  return(combined_data)\n}\n\nspotify_data &lt;- get_combined_artists_data(artists)\n\n\npopularity_data &lt;- read.csv(\"artists_top_song_popularity.csv\")\n\n\npopularity_data &lt;- popularity_data %&gt;%\nselect(\"track_id\", \"popularity\")\n\n\nspotify_data &lt;- spotify_data %&gt;%\nleft_join(popularity_data, by = 'track_id')\n\n\nmissing_rows &lt;- is.na(spotify_data$popularity)\n\n# Separate the data into two sets: one with NAs in 'popularity' and one without NAs\ndata_with_na &lt;- spotify_data[missing_rows, ]\ndata_without_na &lt;- spotify_data[!missing_rows, ]\n\n# Build a linear regression model using rows without NAs\nmodel &lt;- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = data_without_na)\n\n# Predict popularity for rows with NAs\npredicted_popularity &lt;- predict(model, newdata = data_with_na)\n\n# Replace NAs with predicted values\nspotify_data$popularity[missing_rows] &lt;- predicted_popularity\n\nspotify_data$popularity &lt;- as.integer(spotify_data$popularity)\n\nspotify_data &lt;- spotify_data[!(abs(spotify_data$popularity - mean(spotify_data$popularity)) &gt; (3 * sd(spotify_data$popularity))), ]\n\n\nspotify_data\n\n\nA data.frame: 1978 × 16\n\n\n\nartist_name\ninstrumentalness\nvalence\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\ntempo\ntrack_name\ntrack_id\nalbum_name\nalbum_release_year\nalbum_release_date\npopularity\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nDrake\n0.00e+00\n0.2000\n0.402\n0.514\n-7.322\n0.0471\n0.1060\n0.1810\n120.009\nVirginia Beach\n6YV2AI87l1n2fzqU8Dyo05\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n83\n\n\n2\nDrake\n0.00e+00\n0.3090\n0.646\n0.605\n-7.590\n0.1880\n0.0636\n0.2530\n90.156\nAmen (feat. Teezo Touchdown)\n59ZmQR4pwCaa5iM3veM7Cs\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n83\n\n\n3\nDrake\n0.00e+00\n0.1040\n0.571\n0.550\n-6.567\n0.1060\n0.0548\n0.1790\n120.947\nCalling For You (feat. 21 Savage)\n2R30S0W4JCM9JaQWlpmeWn\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n81\n\n\n4\nDrake\n0.00e+00\n0.0658\n0.697\n0.320\n-9.347\n0.2140\n0.0127\n0.1800\n136.976\nFear Of Heights\n6LFW4dEsLeiGluniXRgVYr\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n86\n\n\n5\nDrake\n0.00e+00\n0.0667\n0.566\n0.564\n-7.880\n0.2770\n0.0149\n0.1040\n139.920\nDaylight\n1us5wNgZc0YLT8RQQs2Q7L\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n85\n\n\n6\nDrake\n0.00e+00\n0.2450\n0.475\n0.655\n-7.342\n0.3300\n0.0306\n0.3750\n163.997\nFirst Person Shooter (feat. J. Cole)\n6xIsHPRHdbzU6UMVFn4wh8\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n84\n\n\n7\nDrake\n2.84e-04\n0.1400\n0.673\n0.672\n-8.577\n0.2240\n0.0466\n0.1930\n136.902\nIDGAF (feat. Yeat)\n2uvBprdlMpzeN5Bq0PzMBI\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n88\n\n\n8\nDrake\n0.00e+00\n0.3250\n0.487\n0.490\n-9.373\n0.0649\n0.5540\n0.1260\n141.389\n7969 Santa\n0pdMOh52apEWAS1xELJY7Q\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n88\n\n\n9\nDrake\n0.00e+00\n0.1050\n0.483\n0.408\n-9.243\n0.0502\n0.5080\n0.2590\n88.880\nSlime You Out (feat. SZA)\n4gQBXN2GBRpemMuxg5y3h9\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n86\n\n\n10\nDrake\n1.31e-05\n0.3690\n0.626\n0.309\n-10.397\n0.0433\n0.8500\n0.1170\n117.940\nBahamas Promises\n3nHat22UwPywIevUrXIhy1\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n87\n\n\n11\nDrake\n0.00e+00\n0.2310\n0.570\n0.295\n-13.693\n0.0445\n0.8370\n0.1670\n84.139\nTried Our Best\n5BTu8L170anjdgSohdBkkv\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n95\n\n\n13\nDrake\n0.00e+00\n0.1470\n0.709\n0.377\n-11.327\n0.2190\n0.3340\n0.1760\n127.921\nDrew A Picasso\n7jKNxYzPFGDWEemcRVebLb\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n91\n\n\n14\nDrake\n0.00e+00\n0.1760\n0.647\n0.340\n-8.795\n0.0496\n0.2450\n0.1070\n91.998\nMembers Only (feat. PARTYNEXTDOOR)\n7uHF03xE84sQ5PicRNH3yu\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n84\n\n\n15\nDrake\n0.00e+00\n0.4730\n0.832\n0.555\n-7.766\n0.1160\n0.0506\n0.2130\n95.618\nWhat Would Pluto Do\n4e0uZkMYa8e8HJ5TEUN417\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n82\n\n\n16\nDrake\n0.00e+00\n0.2260\n0.572\n0.406\n-10.081\n0.3080\n0.5430\n0.0982\n125.019\nAll The Parties (feat. Chief Keef)\n04BF53Rb6LOpDUJG518IqS\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n88\n\n\n17\nDrake\n0.00e+00\n0.6680\n0.567\n0.682\n-6.423\n0.2740\n0.6430\n0.5120\n79.043\n8am in Charlotte\n1dKXBUKx7oCB2UXlkihNn8\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n78\n\n\n18\nDrake\n0.00e+00\n0.5740\n0.502\n0.391\n-9.013\n0.0428\n0.8450\n0.1120\n132.819\nBBL Love - Interlude\n4bF4MxTGnNj8dTW0LZmAx5\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n84\n\n\n19\nDrake\n1.82e-04\n0.2410\n0.868\n0.452\n-6.825\n0.3040\n0.0481\n0.1330\n107.538\nGently (feat. Bad Bunny)\n3BE3IYsAhqkTiRIR7T7x7M\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n80\n\n\n20\nDrake\n0.00e+00\n0.1410\n0.636\n0.729\n-4.495\n0.0550\n0.0458\n0.3840\n146.009\nRich Baby Daddy (feat. Sexyy Red & SZA)\n2DSy4HfDQoBsPPcRSoeM16\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n77\n\n\n21\nDrake\n0.00e+00\n0.2820\n0.702\n0.710\n-5.003\n0.0459\n0.0144\n0.1440\n158.129\nAnother Late Night (feat. Lil Yachty)\n6N7CsPNdryWPtG5CTmCHpx\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n79\n\n\n22\nDrake\n0.00e+00\n0.2260\n0.848\n0.625\n-7.640\n0.0587\n0.0195\n0.1090\n93.019\nAway From Home\n2QThr0NmKP43AbPOrW2cDl\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n83\n\n\n23\nDrake\n3.37e-06\n0.3430\n0.440\n0.274\n-14.464\n0.0809\n0.0924\n0.1280\n76.513\nPolar Opposites\n1GZJGEId948OqKmf24ubj5\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n99\n\n\n24\nDrake\n0.00e+00\n0.3930\n0.455\n0.707\n-5.765\n0.1790\n0.1550\n0.1320\n114.470\nRed Button\n5Mc8XDfGQCMkOEZm9rS1cE\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n80\n\n\n25\nDrake\n7.12e-05\n0.8460\n0.650\n0.839\n-4.428\n0.1800\n0.5720\n0.2190\n87.390\nStories About My Brother\n0ZB5pdrM1leH58IgokDPZM\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n75\n\n\n26\nDrake\n0.00e+00\n0.6520\n0.354\n0.790\n-6.758\n0.3290\n0.2910\n0.3090\n74.179\nThe Shoe Fits\n2Mph2Rvl5kVg0MYtfV60wZ\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n82\n\n\n27\nDrake\n0.00e+00\n0.5200\n0.476\n0.926\n-6.623\n0.4800\n0.3530\n0.2240\n86.252\nWick Man\n4ASBfwBwwo7DTT2zq9YSLP\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n83\n\n\n28\nDrake\n0.00e+00\n0.2500\n0.560\n0.784\n-5.304\n0.3010\n0.0508\n0.2210\n78.161\nEvil Ways (feat. J. Cole)\n2ut4BOQSqxLpcX5MtPjzYa\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n79\n\n\n29\nDrake\n0.00e+00\n0.2760\n0.465\n0.700\n-5.163\n0.0893\n0.2030\n0.2990\n120.854\nYou Broke My Heart\n5Se32hEA9raeboZerywxka\nFor All The Dogs Scary Hours Edition\n2023\n2023-11-17\n78\n\n\n30\nDrake\n0.00e+00\n0.2000\n0.402\n0.514\n-7.322\n0.0471\n0.1060\n0.1810\n120.009\nVirginia Beach\n3eP13S8D5m2cweMEg3ZDed\nFor All The Dogs\n2023\n2023-10-06\n85\n\n\n31\nDrake\n0.00e+00\n0.3090\n0.646\n0.605\n-7.590\n0.1880\n0.0636\n0.2530\n90.156\nAmen (feat. Teezo Touchdown)\n0Mrnt1YqVuW2bqmwu4VxDt\nFor All The Dogs\n2023\n2023-10-06\n83\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1984\nBLACKPINK\n0.00e+00\n0.462\n0.607\n0.770\n-4.842\n0.0463\n0.04760\n0.6900\n97.083\nPLAYING WITH FIRE - Live\n3fo1Z8nJX9gRHYPjb4mgJs\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n76\n\n\n1985\nBLACKPINK\n0.00e+00\n0.409\n0.613\n0.863\n-4.388\n0.0826\n0.07860\n0.3420\n124.922\nBOOMBAYAH - Live\n5SK8ZoIj62LTcRW7OQ8vtZ\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n77\n\n\n1986\nBLACKPINK\n0.00e+00\n0.428\n0.643\n0.918\n-3.866\n0.1070\n0.19000\n0.5920\n125.000\nAs If It's Your Last - Live\n3KcrCvutXhEEinowTrLfN4\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n75\n\n\n1987\nBLACKPINK\n0.00e+00\n0.532\n0.681\n0.923\n-3.781\n0.0935\n0.14000\n0.4090\n112.952\nWHISTLE (Remix Version) - Live\n6w5egqwHmhrBJsw8soFcuU\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n75\n\n\n1988\nBLACKPINK\n0.00e+00\n0.379\n0.580\n0.896\n-3.411\n0.1310\n0.04130\n0.4490\n139.973\nDDU DDU-DU (Remix Version) - Live\n1b7PAugOHr8ZjD1Dbj8fON\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n75\n\n\n1989\nBLACKPINK\n0.00e+00\n0.489\n0.342\n0.390\n-8.777\n0.0477\n0.59400\n0.9660\n175.027\nSTAY - Live\n7ov2yk9ZbtJoOpcd7QoRwd\nBLACKPINK 2018 TOUR 'IN YOUR AREA' SEOUL (Live)\n2019\n2019-08-30\n83\n\n\n1990\nBLACKPINK\n4.46e-06\n0.390\n0.571\n0.849\n-8.306\n0.1430\n0.09180\n0.9360\n139.937\nDDU-DU DDU-DU - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n6PDvHZDWtGSVr8LXN3stsH\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n86\n\n\n1991\nBLACKPINK\n0.00e+00\n0.379\n0.588\n0.897\n-6.947\n0.0821\n0.04120\n0.7080\n109.838\nFOREVER YOUNG - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n32ssBfmhrG6qbEapVNwAOm\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n83\n\n\n1992\nBLACKPINK\n0.00e+00\n0.663\n0.729\n0.512\n-9.975\n0.0527\n0.65500\n0.6950\n102.935\nWHISTLE - Acoustic Ver. BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n3QuzFJUtG9H4RoEv5J9mCP\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n85\n\n\n1993\nBLACKPINK\n0.00e+00\n0.507\n0.551\n0.407\n-8.613\n0.0306\n0.24500\n0.2980\n88.099\nSTAY - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n5x9VzjchhXQfDBZcnO5xPM\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n83\n\n\n1994\nBLACKPINK\n0.00e+00\n0.226\n0.359\n0.290\n-11.105\n0.0380\n0.83700\n0.5070\n137.304\nLET IT BE&lt;U+301C&gt; YOU&I&lt;U+301C&gt; ONLY LOOK AT ME / ROS'E - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\" \n7Iv6YcXoLNhTPYhHqtyNUy\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n89\n\n\n1995\nBLACKPINK\n0.00e+00\n0.327\n0.267\n0.312\n-11.951\n0.0314\n0.45600\n0.3100\n142.572\n&lt;U+96EA&gt;&lt;U+306E&gt;&lt;U+83EF&gt; / JISOO - LIVE BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\" \n1sIKkwYIffUw0vLU8RsWIR\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n93\n\n\n1996\nBLACKPINK\n5.30e-06\n0.365\n0.649\n0.637\n-9.747\n0.2630\n0.05350\n0.9500\n95.009\nSOLO - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n5S7by1wwGii36WAs4sBjzc\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n86\n\n\n1997\nBLACKPINK\n0.00e+00\n0.622\n0.497\n0.580\n-10.129\n0.0648\n0.12700\n0.6710\n106.953\n&lt;U+30E9&gt;&lt;U+30B9&gt;&lt;U+30C8&gt;&lt;U+30FB&gt;&lt;U+30AF&gt;&lt;U+30EA&gt;&lt;U+30B9&gt;&lt;U+30DE&gt;&lt;U+30B9&gt; &lt;U+301C&gt; &lt;U+8D64&gt;&lt;U+9F3B&gt;&lt;U+306E&gt;&lt;U+30C8&gt;&lt;U+30CA&gt;&lt;U+30AB&gt;&lt;U+30A4&gt; - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n6149wOhItvVX0zoa1KK5hw\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n88\n\n\n1998\nBLACKPINK\n0.00e+00\n0.551\n0.650\n0.697\n-9.582\n0.2720\n0.13000\n0.8620\n99.771\nKiss and Make Up - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n6X9DNG2WR3IclfneGurU0T\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n86\n\n\n1999\nBLACKPINK\n1.00e-05\n0.814\n0.783\n0.809\n-10.000\n0.2210\n0.05880\n0.5150\n140.025\nSO HOT - THEBLACKLABEL REMIX BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n1fMXWEkJpIH483OrKn1zFV\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n89\n\n\n2000\nBLACKPINK\n0.00e+00\n0.804\n0.711\n0.815\n-7.164\n0.1180\n0.02290\n0.8030\n141.973\nREALLY - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n7nWREW5AWOgLMmW6jKJUEz\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n82\n\n\n2001\nBLACKPINK\n0.00e+00\n0.685\n0.737\n0.871\n-7.936\n0.1250\n0.03760\n0.8400\n106.978\nSEE U LATER - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n5dYjvCegNq6LbwWTzy1xIC\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n83\n\n\n2002\nBLACKPINK\n0.00e+00\n0.421\n0.682\n0.835\n-6.269\n0.0928\n0.06770\n0.7970\n125.056\nBOOMBAYAH - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n1uK64bAOSKhKFTUwrF2r2p\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n80\n\n\n2003\nBLACKPINK\n0.00e+00\n0.593\n0.619\n0.756\n-6.859\n0.1220\n0.02310\n0.8420\n96.970\nPLAYING WITH FIRE - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n6b5xktkZLF96lv5HrFucu4\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n80\n\n\n2004\nBLACKPINK\n0.00e+00\n0.516\n0.626\n0.833\n-6.327\n0.0778\n0.04750\n0.5740\n125.027\nAS IF IT'S YOUR LAST - BLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n4sB4UCnEu6UVC0dWtCqUAT\nBLACKPINK ARENA TOUR 2018 \"SPECIAL FINAL IN KYOCERA DOME OSAKA\"\n2019\n2019-03-13\n81\n\n\n2005\nBLACKPINK\n4.85e-06\n0.513\n0.748\n0.868\n-2.296\n0.0533\n0.02250\n0.3670\n124.952\nBOOMBAYAH - Japanese Version\n5nIjOnMbC0QDMrYFLGx0yV\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n71\n\n\n2006\nBLACKPINK\n0.00e+00\n0.588\n0.823\n0.718\n-5.474\n0.0511\n0.00235\n0.0976\n102.956\nWHISTLE - Japanese Version\n3sgrwjWNJy753U30irIdEN\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n78\n\n\n2007\nBLACKPINK\n0.00e+00\n0.656\n0.664\n0.764\n-3.915\n0.0661\n0.02430\n0.1630\n97.029\nPLAYING WITH FIRE - Japanese Version\n29x3S9kmzTGHswtjSVeUPr\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n75\n\n\n2008\nBLACKPINK\n0.00e+00\n0.399\n0.384\n0.391\n-7.526\n0.0385\n0.57800\n0.1960\n175.818\nSTAY - Japanese Version\n4AESxPBujvuBbCAfBuA2sq\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n82\n\n\n2009\nBLACKPINK\n0.00e+00\n0.527\n0.766\n0.869\n-2.261\n0.0716\n0.12700\n0.0680\n125.067\nAS IF IT'S YOUR LAST - Japanese Version\n5DVgfulxeJZJYc8FseyfUf\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n72\n\n\n2010\nBLACKPINK\n0.00e+00\n0.428\n0.703\n0.845\n-3.479\n0.0682\n0.02960\n0.2130\n139.936\nDDU-DU DDU-DU - Japanese Version\n4jUEHIrc443f743JbyLN0y\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n75\n\n\n2011\nBLACKPINK\n1.12e-06\n0.463\n0.628\n0.967\n-1.134\n0.0569\n0.08680\n0.0589\n110.072\nFOREVER YOUNG - Japanese Version\n02h4inVwwNX1cMuGCDtsgV\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n70\n\n\n2012\nBLACKPINK\n0.00e+00\n0.948\n0.715\n0.876\n-2.361\n0.1010\n0.02550\n0.2810\n142.034\nREALLY - Japanese Version\n6OKXcx7tClGAS0o2cOTl2v\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n71\n\n\n2013\nBLACKPINK\n0.00e+00\n0.659\n0.774\n0.907\n-3.799\n0.1140\n0.03220\n0.5080\n107.005\nSEE U LATER - Japanese Version\n7IS4NciwYPs1wMywOKx69z\nBLACKPINK IN YOUR AREA (Japanese Version)\n2018\n2018-12-05\n74\n\n\n\n\n\n\nwrite.csv(spotify_data, \"spotify_data_cleaned.csv\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "In order to gain a greater understanding of the larger effect KPOP has on the Western music industry, we will need to analyze a variety of sources. The sources we’ll take a look at are a quantitative measure of musical characteristics from the Spotify API, music charting data from the Billboard Hot 100, stock data of the record lables attributed to each of the 8 artists we’ll be analyzing, the KOF Swiss Economic Institute’s globalization index, and inboud passenger statistics of international flights into South Korea from the Korean Tourism Organization.\n\n\n\n\n\n\nData Files\n\n\n\nTo view the raw data and cleaned data, please navigate to my Github linked under to tab titled Code.\nPlease note that data pulled from APIs like Spotify data will be archived to Github based on the last update of this sight. For the most up to date data, we recommend using the API code below\n\n\n\n\nStreaming\n\n\n\n\n\n\n\n\n\n\nLeveraging the capabilities of the Spotify API, I intend to conduct a comprehensive analysis of eight artists, spanning both KPOP and Western genres, by tapping into various streaming data, including discography details and an in-depth examination of musical components. This exploration involves scrutinizing factors such as tempo, key signatures, danceability, energy, and acousticness to discern similarities and differences in the musical characteristics of KPOP and Western artists.\nAdditionally, the incorporation of Spotify’s popularity score offers a holistic view of an artist’s overall impact, allowing for the identification of their most popular tracks and facilitating a comparative assessment between the two genres on a global scale. The popularity score becomes a crucial metric for gauging each artist’s reach and influence, helping unveil trends and preferences in the dynamic landscape of global music consumption. I intend to use the popularity score to determine which songs between both KPOP and the Western artists contribute to the global popularity and the future of popular music.\n\n\nNOTE: The code used for the remainder of anaylsis uses a popularity index created by Spotify. However, due to Spotify’s new regulations on the public information provided through their API, popularity is no longer accessible for all songs, just an artist’s top 10 songs. Thus, in order to continue with analysis, I needed to exprapolate the blank poplarity values through regression. I trained the model on the existing popularity scores and then predicted the remaining values. In the future, I would like the find an alternatibve metric to measure popularity of every song.\n\n\nCode to gather Spotify Data\n#The following commented lines are the popularity index gathered through Python. The remainder of the code is in R:\n#______________________________________________________\n# import spotipy\n# from spotipy.oauth2 import SpotifyClientCredentials\n# import pandas as pd\n\n# client_id = \"48875e31f589401f83c6bd43005d94f7\"\n# client_secret = \"d215e4ea690d4b9b9c1c5e0afbb113a5\"\n\n# # Authenticate with Spotify API\n# sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=client_id, client_secret=client_secret))\n\n# artists = [\"Drake\", \"Beyonce\", \"Harry Styles\", \"Taylor Swift\", \"Twice\", \"BTS\", \"EXO\", \"BlackPink\"]\n\n# # Function to get combined data for multiple artists\n# def get_combined_artists_data(artist_names):\n#     combined_data = pd.DataFrame()\n\n#     for artist_name in artist_names:\n        \n#         top_tracks = sp.artist_top_tracks(sp.search(q=artist_name, type=\"artist\")[\"artists\"][\"items\"][0][\"id\"])[\"tracks\"]\n        \n#         # Extract relevant track information\n#         track_data = pd.DataFrame({\n#             \"artist_name\": [artist_name] * len(top_tracks),\n#             \"track_name\": [track[\"name\"] for track in top_tracks],\n#             \"track_id\": [track[\"id\"] for track in top_tracks],\n#             \"popularity\": [track[\"popularity\"] for track in top_tracks],\n#         })\n        \n#         # Combine data for each artist\n#         combined_data = pd.concat([combined_data, track_data], ignore_index=True)\n\n#     return combined_data\n\n# # Example usage\n# combined_artists_df = get_combined_artists_data(artists)\n# print(combined_artists_df)\n\n# combined_artists_df.to_csv(\"raw_data/artists_top_song_popularity.csv\")\n\naccess_token &lt;- get_spotify_access_token()\nartists &lt;- c(\"Drake\", \"Beyonce\", \"Harry Styles\", \"Taylor Swift\", \"Twice\", \"BTS\", \"EXO\", \"BlackPink\")\n\nget_combined_artists_data &lt;- function(artist_names) {\n  combined_data &lt;- data.frame()\n  \n  for (artist_name in artist_names) {\n    artist_data &lt;- get_artist_audio_features(artist_name)\n    \n    selected_columns &lt;- c(\"artist_name\", \"instrumentalness\", \"valence\", \"danceability\",\n                           \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n                           \"liveness\", \"tempo\", \"track_name\", \"track_id\", \"album_name\",\n                           \"album_release_year\", \"album_release_date\")\n    \n    artist_df &lt;- data.frame(artist_data[selected_columns])\n    \n    colnames(artist_df) &lt;- c(\"artist_name\", \"instrumentalness\", \"valence\", \"danceability\",\n                             \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n                             \"liveness\", \"tempo\", \"track_name\", \"track_id\", \"album_name\",\n                             \"album_release_year\", \"album_release_date\")\n    \n    # Combine data for each artist\n    combined_data &lt;- bind_rows(combined_data, artist_df)\n  }\n  \n  return(combined_data)\n}\n\nspotify_data &lt;- get_combined_artists_data(artists)\n\nwrite.csv(spotify_data, \"raw_data/spotify_data_raw.csv\")\n\npopularity_data &lt;- read.csv(\"raw_data/artists_top_song_popularity.csv\")\n\npopularity_data &lt;- popularity_data %&gt;%\nselect(\"track_id\", \"popularity\")\n\nspotify_data &lt;- spotify_data %&gt;%\nleft_join(popularity_data, by = 'track_id')\n\nmissing_rows &lt;- is.na(spotify_data$popularity)\n\n# Separate the data into two sets: one with NAs in 'popularity' and one without NAs\ndata_with_na &lt;- spotify_data[missing_rows, ]\ndata_without_na &lt;- spotify_data[!missing_rows, ]\n\n# Build a linear regression model using rows without NAs\nmodel &lt;- lm(popularity ~ instrumentalness + valence + danceability + energy + loudness + speechiness + acousticness + liveness + tempo, data = data_without_na)\n\n# Predict popularity for rows with NAs\npredicted_popularity &lt;- predict(model, newdata = data_with_na)\n\npredicted_popularity &lt;- pmax(1, pmin(predicted_popularity, 100))\n\n# Replace NAs with predicted values\nspotify_data$popularity[missing_rows] &lt;- predicted_popularity\n\nspotify_data$popularity &lt;- as.integer(spotify_data$popularity)\n\nspotify_data &lt;- spotify_data[!(abs(spotify_data$popularity - mean(spotify_data$popularity)) &gt; (3 * sd(spotify_data$popularity))), ]\n\nwrite.csv(spotify_data, \"cleaned_data/spotify_data_cleaned.csv\")\n\n\n\n\n\nMusic Charts\n\n\n\n\n\nOne of the longest-running and most influential music charting services is the Billboard Chart. Since 1958, the Billboard charts have documented the top 100 songs in the United States every week. This extensive and rich dataset spans over six decades, offering a unique lens through which to analyze the trajectory of popular music. By delving into this historical data, we gain valuable insights into the evolution of musical preferences, genre dynamics, and the global impact of the music industry.\nThe Billboard Chart serves as a time capsule, allowing us to trace the rise and fall of genres, observe the ebb and flow of musical trends, and investigate the longevity of artists and songs, unveiling the staying power of certain genres and the factors contributing to enduring success.\n\n\n\nMusic Stock\n\n\nIn order to analyze artists in the West and in South Korea, we will also need to understand how they are a part of the financial side of the music industry. Thus, we will analyze the stock data of record label companies these artists fall under. For the West, Taylor Swift and Drake are signed to subsidiaries under University Music Group and Harry Styles and Beyoncé are signed under subsidiaries of SONY. For the four chosen KPOP artists, BTS is signed under HYBE Co. labels, Blackpink is under YG Entertainment, EXO is under SM Entertainment, and Twice is under JYP Entertainment.\nUsing the Quantmod package, we can analyze the stock prices of several music record companies over a course of multiple years. This will allow us to identify specific trends within the music industry as well as interpret shareholder’s opinions of globalization news as well as musical news and other world events throughout history.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Globalization\n\n\n\n\n\n\n\n\n\n\nThe Swiss Economic Institute’s KOF Globalization index is a way to quantify globalization of a country. This metric was started in the 1970’s as countries increasingly began to embrace globalization, which is defined as the process by which businesses or other organizations develop international influence or start operating on an international scale. However, this metric not only quanitfies globalization per country, but it also has mutliple indices for cultural, technological, and social globalization. This metric can be used in tandum with music trends in order to decipher if musical globalization is happening concurrently with general globalization.\n\n\n\n\n\nKorean Tourism\n\n\n\n\n\nIn order to expand upon globalization, we’ll also use tourism statistics regarding inbound passengers to South Korea from international travel as a ways to measure interest in the country over time. I’ve gathered both yearly data of a count of those who visited South Korea purely for tourism as well as monthly data of international arrivals (as a number of passengers) into Incheon Airport, the largest international airport in the country, on account of tourism. This tourist metric into South Korea can be used to explore globalization, but also understand the extent in which Korean culture is reaching people globally.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "deep-learning.html",
    "href": "deep-learning.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "In order to compare the tradition time series models, we’ll next take a look at deep learning models for comparison. The data we’ll take a look at is the Air passangers into South Korea which we modeled through ARIMA. In order to compare the predictions of that model with deep learning, we will create 3 different neural network models (RNNs, GRU, LSTM) using the tensorflow package found in python. After we gather the predictions for these models, we’ll compare those predictions with ARIMA’s forecast via RMSE as well as test the forecasting reach of the models."
  },
  {
    "objectID": "deep-learning.html#obtaining-the-models",
    "href": "deep-learning.html#obtaining-the-models",
    "title": "Deep Learning for Time Series",
    "section": "Obtaining the Models",
    "text": "Obtaining the Models\nFirst, we’ll begin by gathering the data and transforming it such that it is suitable for deep learning. Below is confirmation that the data was transformed and no NA values were found.\n\n\nData Transformation\nsk_passengers = pd.read_excel('raw_data/sk_passenger_arrivals.xlsx')\n\n# Manipulate data\nsk_passengers['date'] = sk_passengers['year'].astype(str) + '-' + sk_passengers['month'].astype(str)\nsk_passengers['date'] = pd.to_datetime(sk_passengers['date'] + '-01')\nsk_passengers = sk_passengers[sk_passengers['date'].dt.year &lt; 2020]\n\ndf = sk_passengers\n\ndf = df.rename(columns={\"date\": \"t\", \"Passengers\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\n\nCHECK NA:\n t    0\ny    0\ndtype: int64\n\n\nData Transformation\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n\n\nNext, we’ll go ahead and define mutliple functions used for each deep learning algorithm. The functions form_arrays, regression_report, and history_plots are used for the pre-prosessing and post anylsis of the model’s results.\n\n\nFunction Definitions\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --&gt; report and plot for debugging\n    # unique=True --&gt; don't re-sample: \n    # x1,x2,x3 --&gt; x4 then x4,x5,x6 --&gt; x7 instead of x2,x3,x4 --&gt; x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay&lt; x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count&lt;2: print(\"indice range:\",i_start,i_stop,\"--&gt;\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --&gt; start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j&gt;=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count&lt;2: print(xtmp, \"--&gt;\",ytmp)\n        if verbose and count&lt;2: print(\"shape:\",xtmp.shape, \"--&gt;\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count&lt;2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" RMSE:\",(mean_squared_error(yt,ytp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" RMSE:\",(mean_squared_error(yv,yvp))**(1/2))\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nNow that we have our functions defined, let’s visualize the raw, normalized, and train-validation split.\n\n\nVisualization of Raw Data\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Raw Data\")\nplt.show()\n\n\n\n\n\nVisualization of Raw Data\nprint(np.mean(x,axis=0).shape,np.std(x,axis=0).shape)\n\n\n(1,) (1,)\n\n\nVisualization of Raw Data\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\nprint(x.shape)\n\n\n(111, 1)\n\n\nVisualization of Raw Data\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.title(\"Normalized Data\")\nplt.show()\n\n\n\n\n\nVisualization of Raw Data\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; xt=x[0:cut]\ntv=t[cut:]; xv=x[cut:]\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.title(\"Train/Validation Split\")\nplt.show()\n\n\n\n\n\n\n\nTraining/Validation Shape\n# training\nL=5; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\nprint(\"training:\",Xt.shape,Yt.shape)\n\n\ntraining: (82, 6, 1) (82, 1)\n\n\nTraining/Validation Shape\nprint(\"validation:\",Xv.shape,Yv.shape)\n\n\nvalidation: (17, 6, 1) (17, 1)\n\n\nFinally, we can begin running our machine learning models. The models and their results are as follows: RNN, GRU, and LSTM.\n\nRNN\n\nResultsError\n\n\n\n\nRNN\nprint(Xt.shape,\"--&gt;\",Yt.shape)\n\n\n(82, 6, 1) --&gt; (82, 1)\n\n\nRNN\nprint(Xv.shape,\"--&gt;\",Yv.shape)\n\n\n(17, 6, 1) --&gt; (17, 1)\n\n\nRNN\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\nRNN\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n\n\n\n\nRNN\n# Predictions \nYtp=model.predict(Xt)\n\n\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 3ms/step\n\n\nRNN\nYvp=model.predict(Xv) \n\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 32ms/step\n\n\nRNN\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n\n\n---------- Regression report ----------\nTRAINING:\n RMSE: 0.073998754678601\n MSE: 0.0054758156939837726\n MAE: 0.05264254589655415\nVALIDATION:\n RMSE: 0.5089183272964894\n MSE: 0.2589978638582568\n MAE: 0.38113220006035203\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n\n\ntraining: (85, 3, 1) (85, 1)\nvalidation: (20, 3, 1) (20, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 26ms/step\n0.5876454145330239\n0.5429957338834613\ntraining: (84, 4, 1) (84, 1)\nvalidation: (19, 4, 1) (19, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 25ms/step\n0.4348442018121856\n0.816793709809624\ntraining: (83, 5, 1) (83, 1)\nvalidation: (18, 5, 1) (18, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 27ms/step\n0.6430213156032795\n0.288404124487964\ntraining: (82, 6, 1) (82, 1)\nvalidation: (17, 6, 1) (17, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 32ms/step\n0.073998754678601\n0.5089183272964894\ntraining: (81, 7, 1) (81, 1)\nvalidation: (16, 7, 1) (16, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 31ms/step\n0.4741316550681732\n0.7969464264549359\n\n\n\n\n\n\n\n\nGRU\n\nResultsError\n\n\n\n\nGRU\nprint(Xt.shape,\"--&gt;\",Yt.shape)\n\n\n(81, 7, 1) --&gt; (81, 1)\n\n\nGRU\nprint(Xv.shape,\"--&gt;\",Yv.shape)\n\n\n(16, 7, 1) --&gt; (16, 1)\n\n\nGRU\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\nGRU\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n\n\n\n\nGRU\n# Predictions \nYtp=model.predict(Xt)\n\n\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 16ms/step\n\n\nGRU\nYvp=model.predict(Xv) \n\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 31ms/step\n\n\nGRU\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n\n\n---------- Regression report ----------\nTRAINING:\n RMSE: 0.20005800818202552\n MSE: 0.04002320663775939\n MAE: 0.14640346866741233\nVALIDATION:\n RMSE: 0.4096318623542952\n MSE: 0.16779826265584827\n MAE: 0.3527306684633301\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n\n\ntraining: (85, 3, 1) (85, 1)\nvalidation: (20, 3, 1) (20, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 24ms/step\n0.6768015394872124\n0.8083459565830079\ntraining: (84, 4, 1) (84, 1)\nvalidation: (19, 4, 1) (19, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 25ms/step\n0.47466436603899287\n1.0162932262307682\ntraining: (83, 5, 1) (83, 1)\nvalidation: (18, 5, 1) (18, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 24ms/step\n0.3575766200434074\n0.7627918041508658\ntraining: (82, 6, 1) (82, 1)\nvalidation: (17, 6, 1) (17, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 29ms/step\n0.26278402428411046\n0.5198435474759417\ntraining: (81, 7, 1) (81, 1)\nvalidation: (16, 7, 1) (16, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 3ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 27ms/step\n0.20005800818202552\n0.4096318623542952\n\n\n\n\n\n\n\n\nLSTM\n\nResultsError\n\n\n\n\nLSTM\nprint(Xt.shape,\"--&gt;\",Yt.shape)\n\n\n(81, 7, 1) --&gt; (81, 1)\n\n\nLSTM\nprint(Xv.shape,\"--&gt;\",Yv.shape)\n\n\n(16, 7, 1) --&gt; (16, 1)\n\n\nLSTM\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\nLSTM\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n\n\n\n\nLSTM\n# Predictions \nYtp=model.predict(Xt)\n\n\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n\nLSTM\nYvp=model.predict(Xv) \n\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 25ms/step\n\n\nLSTM\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n\n\n---------- Regression report ----------\nTRAINING:\n RMSE: 0.23777248573300097\n MSE: 0.05653575497165015\n MAE: 0.1706376927855152\nVALIDATION:\n RMSE: 1.0626047755606447\n MSE: 1.129128909044288\n MAE: 0.7659915126549615\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n\n\ntraining: (85, 3, 1) (85, 1)\nvalidation: (20, 3, 1) (20, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 28ms/step\n0.6331292662118136\n0.8435635965847128\ntraining: (84, 4, 1) (84, 1)\nvalidation: (19, 4, 1) (19, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 29ms/step\n0.5021301108064852\n1.9434281245417948\ntraining: (83, 5, 1) (83, 1)\nvalidation: (18, 5, 1) (18, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 26ms/step\n0.44361258296742223\n2.9168342403315357\ntraining: (82, 6, 1) (82, 1)\nvalidation: (17, 6, 1) (17, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 2ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 27ms/step\n0.34541560230483587\n2.800158939030564\ntraining: (81, 7, 1) (81, 1)\nvalidation: (16, 7, 1) (16, 1)\n\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n3/3 [==============================] - 0s 3ms/step\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 30ms/step\n0.23777248573300097\n1.0626047755606447"
  },
  {
    "objectID": "deep-learning.html#comparing-the-models",
    "href": "deep-learning.html#comparing-the-models",
    "title": "Deep Learning for Time Series",
    "section": "Comparing the Models",
    "text": "Comparing the Models\nFrom the models given, we can see the following RMSE values:\n\nRNN\n\nTraining: 0.11260836081499234, Validation: 0.48808609621144566\n\nGRU\n\nTraining: 0.1748385330640682; Validation: 0.27231605760159167\n\nLSTM\n\nTraining: 0.24832560941644433; Validation: 0.6533320413106263\n\n\nThus, we can see that based on the loss metric (RMSE), the GRU model seems to be be the best for predicting air passengers from international flights into South Korea."
  },
  {
    "objectID": "deep-learning.html#the-problem-with-forecasting",
    "href": "deep-learning.html#the-problem-with-forecasting",
    "title": "Deep Learning for Time Series",
    "section": "The Problem with Forecasting:",
    "text": "The Problem with Forecasting:\nThe 3 deep learning models, while highly effective, cannot forecast with “new” x-values. Unfortunelty, due to that way we’ll structured these models, those “x-values”, rather than time, is the value being measured, in our case, air passengers. Thus, we shall focus on the forecasting ability of the model through validation results. Taking a look at GRU, when checking the RMSE value after multiple lengths of train/validation, we can see that as the validation size increases, the RMSE actually did decrease. However, the other models failed to reduce the loss function as the validation size increased. However, we can note that all three models benefited from regularizing around 0 since the predictions for regularized data above 0 had a great loss return."
  },
  {
    "objectID": "deep-learning.html#comparing-to-arimasarima",
    "href": "deep-learning.html#comparing-to-arimasarima",
    "title": "Deep Learning for Time Series",
    "section": "Comparing to ARIMA/SARIMA",
    "text": "Comparing to ARIMA/SARIMA\nPreviously, the air passenger dataset was used to model a SARIMA model, which produced a positivly trended forecast. That specific fit produced an RMSE of 86406.29, which is much much higher that any of the deep learning models. However, the visualization of the forecast from the SARIMA model fit much better than that of any of the deep learning models.\nThus, as of now, we can say that while the deep learning models effectively reduced the RMSE and other loss functions, these models lack the ability to forecast far in the future or accurately account for seasonality and volitility in the data. Therefore, we can say that more research and deep learning implementations would need to be performed in order to compare to the forecasting abilities of SARIMA and other time series models."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for time series involves a systematic process to uncover patterns, trends, and underlying characteristics within the data. The initial step often includes a fundamental Time Series Plot, providing a visual representation of data points over time. Next, plotting the Simple Moving Average (SMA) aids in smoothing fluctuations and revealing long-term trends. Lag plots help identify localized patterns and autocorrelations within the time series, providing insights into potential cyclical behavior.\nThe examination of Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots further refines our understanding of temporal dependencies and the need for differencing. This step is crucial in selecting appropriate parameters for time series models.\nIn addition to the ACF & PACF plots, the Dickey-Fuller Test is employed to identify stationarity. This statistical test evaluates whether a time series possesses a unit root, indicative of non-stationarity. A stationary time series exhibits consistent statistical properties over time, making it useful for modeling.\nThe purpose of EDA for our analysis is to confirm our univariate time series data is stationary prior to modeling. In our case, we will be performing EDA on the KOF globalization index, the record label of greatest interest, HYB Co., and the inbound passengers to South Korea.\nImporting Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\npy_install(\"tensorflow\")"
  },
  {
    "objectID": "eda.html#globalization",
    "href": "eda.html#globalization",
    "title": "Exploratory Data Analysis",
    "section": "Globalization",
    "text": "Globalization\n\nTime Series plotMoving Average SmoothingLag-plotACF & PACFDickey-Fuller TestStationary\n\n\n\n\nCode\n# Import dataset\nglobal &lt;- read_csv('cleaned_data/globalization.csv')\n\n# Filter information\nglobal &lt;- global %&gt;%\n  filter(country == 'United States') %&gt;%\n  select(year, KOFGI) %&gt;%\n  mutate(year = as.Date(year))\n\n# Create time series\nglobal_ts &lt;-ts(global$KOFGI, star=decimal_date(as.Date(\"1970-01-01\", format = \"%Y-%m-%d\")), frequency = 1)\n\n# Create time series plot\nglobal_plot &lt;- plot(as.ts(global_ts), main = \"Time Series of KOF Globalization Index within the United States\",\n                  xlab = 'Time', ylab = 'KOF Index')\n\n\n\n\n\nCode\n# Show plot\nggplotly(global_plot)\n\n\n\n\n\n\n\n\n\nSMA Code\nimport pandas as pd\nimport datetime\nimport plotly \nfrom datetime import datetime \nfrom pandas_datareader import data as pdr\nimport plotly.offline as pyo\nimport plotly.graph_objects as go \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nglobal_data = pd.read_csv('cleaned_data/globalization.csv')\nglobal_data = global_data[global_data['country'] == 'United States']\nglobal_data['year'] = pd.to_datetime(global_data['year'], format='%Y')\n\nglobal_data['5yma'] = global_data['KOFGI'].rolling(window = 5).mean()\nglobal_data['10yma'] = global_data['KOFGI'].rolling(window = 10).mean()\nglobal_data['20yma'] = global_data['KOFGI'].rolling(window = 20).mean()\n\nfig = px.line(global_data, x='year', y=['KOFGI', '5yma', '10yma', '20yma'],\n              labels={'value': 'Yearly Data', 'variable': 'Moving Average'},\n              title='Yearly Data and Moving Averages for the Globalization Index')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\ngglagplot(global_ts, do.lines=FALSE)+ggtitle(\"Lag Plot for the KOF Globalization Index\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(global_ts)+ggtitle(\"ACF Plot for Globalization Index\")\n\n\n\n\n\nCode\nggPacf(global_ts)+ggtitle(\"PACF Plot for Globalization Index\")\n\n\n\n\n\n\n\n\n\nCode\nglobal_test &lt;- adf.test(global_ts)\nprint(global_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  global_ts\nDickey-Fuller = 0.59021, Lag order = 3, p-value = 0.99\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nfit = lm(global_ts~time(global_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(global_ts, 50, main=\"Original Data: Globalization Index\")\nplot2 &lt;- ggAcf(resid(fit), 50, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(global_ts), 50, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\nLet’s begin by anaylzing globalization of the United States from 1970 to 2022. I will be using the general globalization index for this time series anaylsis. First I will begin by filtering the data and creating a time series object in R. This will allow us to plot the time series data for initial analysis.\nFrom the plot of the globalization index in the United States, we can see a strong positive upward trend. In terms of seasonality and cyclic patterns, we are unable to see such patterns in the data. Additionally, we can see very slight peaks in the data in 1986 and 2009, however, they are not enough to conclude any patterns of interest. Thus,we can say this plot is neither additive nor multiplicative.\nNext, we’ll take a look at a moving average smoothing plot to obtain some information on potential crossings.\nThe plot shows us the smoothing moving average for the yearly KOF globalization index data. The three types of smoothing I chose was 5-year, 10-year, and 20-year moving averages. As we can see in the graph, all smoothing lines show a positive upward trend across the time interval (the past 50 years). There is also no crossing between the smoothing lines, possibly indicating that the data had a constant upward trend with no seasonality or cyclical trends throughout.\nNext, let’s take a look at a few lag plots of data on itself as well ACF and PACF in order to identify possible signs of stationarity.\nWe can see in lags 1,2, and 3 a very strong positive linear relationship, meaning a positive autocorrelation in the lags. From lag 4 and onward, the trend is still strongly positive, but less linear, suggesting a weaker autocorrelation. We also don’t see any groupings in the lags, suggesting that there is no seasonality in the data.\nLooking at the ACF and PACF plots, we get a better understanding this time series. The ACF plot shows the present lag is significantly correlated with the first 12 years, after which it become significantly uncorrelated. Additionally, the PACF shows a stationary plot, due to the PACF values being contained in the significance bands. Thus, we can say that there is in fact strong autocorrelation in this time series data, however correlation is not present within the residuals.\nThe Dickey-Fuller Test, which tests the alternative hypothesis that the time series is stationary, returned a p-value of 0.99. Since 0.99 &gt; 0.05, we do not have enough evidence and thus, fail to reject the null hypothesis, meaning that the time series object is not stationary. However, since we got different results from the ACF and PACF, we’ll proceed with the ACF results and difference/detrend the data.\nTherefore, in order to obtain stationary data to runs an ARMA and AMRIMA model on, we will need to compare differenced and detrended data to find which approach produces stationary data.\nFrom this plot, we can clearly see that the first differenced data results in a stationary plot, with the ACF values inside the significance bands. Since the first difference was able to coerce the data to be stationary, we can also say that the original data was linearly trended. Thus, moving forward, we will use first differencing on the globalization index in order to model this value."
  },
  {
    "objectID": "eda.html#stock-prices-looking-at-hybe-entertainment",
    "href": "eda.html#stock-prices-looking-at-hybe-entertainment",
    "title": "Exploratory Data Analysis",
    "section": "Stock Prices: Looking at HYBE Entertainment",
    "text": "Stock Prices: Looking at HYBE Entertainment\n\nTime Series plotMoving Average SmoothingLag-plotACF & PACFDickey-Fuller TestStationary\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\nname &lt;- getSymbols('352820.KS', from = \"2019-01-01\", to = \"2023-09-01\")\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X352820.KS.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\n\nstart_date &lt;- as.Date(\"2020-10-15\")\nend_date &lt;- as.Date(\"2023-09-01\")\n\nall_dates &lt;- data.frame(Date = seq(start_date, end_date, by = \"days\"))\n\nmerged_data &lt;- all_dates %&gt;%\n  left_join(HYBE, by = \"Date\")\n\nimputed_time_series &lt;- na_ma(merged_data, k = 4, weighting = \"exponential\")\ndf_HYBE &lt;-data.frame(imputed_time_series)\ndf_HYBE$Date &lt;-as.Date(df_HYBE$Date,format = \"%Y-%m-%d\")\n\nwrite.csv(df_HYBE, 'cleaned_data/HYBE_cleaned_data.csv')\n\n# Create time series\nHYBE_ts &lt;-ts(df_HYBE$Price, star=decimal_date(as.Date(\"2020-10-15\", format = \"%Y-%m-%d\")), frequency = 365.25)\n\n# Create time series plot\nHYBE_plot &lt;- plot(as.ts(HYBE_ts), main = \"Time Series of HYBE Stock Prices\",\n                  xlab = 'Time', ylab = 'Price (USD)')\n\n\n\n\n\n\n\n\n\nSMA Code\nHYBE_data = pd.read_csv('cleaned_data/HYBE_cleaned_data.csv')\n\nHYBE_data.drop(HYBE_data.columns[0], axis=1, inplace = True)\nHYBE_data['Date'] = pd.to_datetime(HYBE_data['Date'])\nHYBE_data.dropna(axis  = 0, inplace = True)\n\nHYBE_data['3wma'] = HYBE_data['Price'].rolling(window = 15).mean()\nHYBE_data['20wma'] = HYBE_data['Price'].rolling(window = 100).mean()\nHYBE_data['50wma'] = HYBE_data['Price'].rolling(window = 250).mean()\n\nfig = px.line(HYBE_data, x='Date', y=['Price', '3wma', '20wma', '50wma'],\n              labels={'value': 'Daily Data', 'variable': 'Weekly Moving Average'},\n              title='Smoothing Moving Averages for HYBE Stock')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\ngglagplot(HYBE_ts, do.lines=FALSE)+\n  ggtitle(\"Lag Plot for HYBE Stock Prices\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nggAcf(HYBE_ts, lag.max = 30)+ggtitle(\"ACF Plot for HYBE Stock Prices\")\n\n\n\n\n\nCode\nggPacf(HYBE_ts, lag.max = 30)+ggtitle(\"PACF Plot for HYBE Stock Prices\")\n\n\n\n\n\n\n\n\n\nCode\nHYBE_test &lt;- adf.test(HYBE_ts)\nprint(HYBE_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  HYBE_ts\nDickey-Fuller = -2.0109, Lag order = 10, p-value = 0.5737\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nfit = lm(HYBE_ts~time(HYBE_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(HYBE_ts, lag.max = 30, main=\"Original Data: HYBE Stock Prices\")\nplot2 &lt;- ggAcf(resid(fit), lag.max = 30, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(HYBE_ts), lag.max = 30, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3, ncol=3)\n\n\n\n\n\n\n\nCode\nplot4 &lt;- ggAcf(diff(diff(HYBE_ts)), lag.max = 30, main=\"Second Differenced Data\")\nplot4\n\n\n\n\n\n\n\n\nNext, since we saw, through the initial data visualization, the prevelance of KPOP, and specifically BTS, on the western music industry, we will take a look at HYBE stock prices through further time series EDA.\nPrimarily, we will clean our data such that missing dates corresponding to weekends and holidays where the stock market is closed will be estimated through exponential prediction. After which we will take the data and transform it into a time series object to plot.\nUnlike the globalization index, the HYBE stock price fluctuates quite frequently in the smaller range of time. From 2021 to 2022, we can see a strong positive trend with slight seasonality. However, from 2022 onwards we see a sharp downward trend and with varying degrees of peaks. Thus, the uneven nature of the peaks and troughs results in data that is neither additive or multiplicative. Additionally, since the dataset is smaller, we cannot say anything of certain regarding cyclical patterns.\nNow, let’s look at the SMA graph for HYBE stock to identify potential crossings.\nThis plot depicts the smoothing moving average of HYBE stock prices. The smoothing windows I chose was 3-week, 20-week, and 50-week on the daily data. As we can see, the 3-week smoothing line is closely correlated with the actual prices, as expected. We can also see some crossings between the 3 smoothing moving averages. Primarily, we can see a crossing in September of 2021, where the 3-week SMA briefly crosses under and over the 20-week line. The crossing over of a shorter SMA, also called a golden cross, indicates that a postive trend in prices was to be expected, which is what occured. This is most likely due to the announcement of BTS performing at the 2021 Grammy Music Awards. Additionally, we can see a significant crossing again in April of 2022. This is when the 3-week and 20-week line cross under the 50-week. When a shorter term SMA cross under the longer SMA, we can infer a drop in stock prices, also called a Death Cross. This inevitably did happen, most likely due to talk of BTS’s enlistment into the Korean military as a part of mandatory service, which was officially announced June of that year.\nNext, let’s take a look at a few lag plots of data on itself as well ACF and PACF in order to identify possible signs of stationarity.\nThese lag plots show similar results to that of the globalization index. The forst four lags have a very strong positive linear correlation, suggesting autocorrelation amongst those lags. From lag 5 onwards, we still see a string linear correlation, however we can also see a small circular pattern forming in the lag plots, suggesting a possibility of single-cycle sinusodial data.\nLooking at the ACF & PACF plots, the ACF plot shows the present lag is significantly correlated with all other present lags in the plot, since all values are well above the siginificance bands. Additionally, the PACF shows a stationary plot, due to the PACF values being contained in the significance bands. Thus, we can say that there is in fact strong autocorrelation in this time series data, however correlation is not present within the residuals.\nThe Dickey-Fuller Test resulted in a p-value of 0.5737. Since 0.5737 &gt; 0.05, we can fail to reject the null hypothesis and say that the time series object is not stationary. However, since we got different results from the ACF and PACF, we’ll proceed with the ACF results and difference/detrend the data.\nHowever, after trying both detrending and first difference methods, both result in ACF plots showing autocorrelation and non-stationary tendencies. Thus, we will try the second differencing approach.\nWith the second difference, we were able to get the HYBE stock prices to become stationary. Therefore, we could also suggest the original data has quadratic trending behavior.\nThus, going forward, we can use the second difference of the HYBE stock prices for modeling."
  },
  {
    "objectID": "eda.html#korean-tourism",
    "href": "eda.html#korean-tourism",
    "title": "Exploratory Data Analysis",
    "section": "Korean Tourism",
    "text": "Korean Tourism\n\nTime Series PlotMoving Average SmoothingLag-plotACF & PACFDickey-Fuller TestStationary\n\n\n\n\nCode\nsk_passengers &lt;- read_xlsx('raw_data/sk_passenger_arrivals.xlsx')\n\nsk_passengers &lt;- sk_passengers %&gt;%\n  unite(date, year, month, sep = '-') %&gt;%\n  mutate(date = as.Date(paste(date, '01', sep = '-'))) %&gt;%\n  filter(year(date) &lt; 2020) #In order to avoid the anomaly of the 2020 pandemic\n\nwrite.csv(sk_passengers, \"cleaned_data/air_passengers_cleaned.csv\")\n\nair_travel_ts &lt;- ts(sk_passengers$Passengers, start = c(2010, 10), \n                    frequency = 12)\n\nautoplot(air_travel_ts)+ggtitle(\"Air passenger arrivals to Incheon Airport (SK)\") \n\n\n\n\n\n\n\n\n\nSMA Code\nimport pandas as pd\nimport datetime\nimport plotly \nfrom datetime import datetime \nfrom pandas_datareader import data as pdr\nimport plotly.offline as pyo\nimport plotly.graph_objects as go \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nglobal_data = pd.read_csv('cleaned_data/air_passengers_cleaned.csv')\nglobal_data['date'] = pd.to_datetime(global_data['date'], format='%Y-%m')\n\nglobal_data['6mma'] = global_data['Passengers'].rolling(window = 6).mean()\nglobal_data['12mma'] = global_data['Passengers'].rolling(window = 12).mean()\nglobal_data['36mma'] = global_data['Passengers'].rolling(window = 36).mean()\n\nfig = px.line(global_data, x='date', y=['Passengers', '6mma', '12mma', '36mma'],\n              labels={'value': 'Monthly Data', 'variable': 'Moving Average'},\n              title='Monthly Data and Moving Averages for the Inbound Passengers to SK')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\ngglagplot(air_travel_ts, do.lines=FALSE)+\n  ggtitle(\"Lag Plot for Inbound Passengers to SK\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nCode\ngglagplot(air_travel_ts, do.lines=FALSE, set.lags = c(12, 24, 36, 48))+\n  ggtitle(\"Air passenger arrivals to Incheon Airport (SK)\") \n\n\n\n\n\n\n\n\n\nCode\nggAcf(air_travel_ts, lag.max = 30)+ggtitle(\"ACF Plot for Inbound Passengers to SK\")\n\n\n\n\n\nCode\nggPacf(air_travel_ts, lag.max = 30)+ggtitle(\"PACF Plot for Inbound Passengers to SK\")\n\n\n\n\n\n\n\n\n\nCode\npassengers_test &lt;- adf.test(air_travel_ts)\nprint(passengers_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  air_travel_ts\nDickey-Fuller = -3.1634, Lag order = 4, p-value = 0.09754\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nfit = lm(air_travel_ts~time(air_travel_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(air_travel_ts, lag.max = 30, main=\"Original Data: Inbound Passengers to SK\")\nplot2 &lt;- ggAcf(resid(fit), lag.max = 30, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(air_travel_ts), lag.max = 30, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3, ncol=3)\n\n\n\n\n\n\n\nCode\nplot4 &lt;- ggAcf(air_travel_ts %&gt;% diff(12), lag.max = 30, main=\"Seasonal Differenced Data\")\nplot4\n\n\n\n\n\nCode\nplot5 &lt;- ggAcf(air_travel_ts %&gt;% diff() %&gt;% diff(12), lag.max = 30, main=\"Seasonal Differenced + First Differenced Data\")\nplot5\n\n\n\n\n\n\n\n\nIn order to understand globalization in terms of South Korean culture globally, we’ll also be looking at the number of inbound passengers into South Korea’s Incheon airport from international flights. This data is monthly in nature, thus we’ll clean the data in order to represent the date column as a datetime type in R.\nFirst, let’s take a look at the time series plot. In comparison to HYBE and the globalization index, we can see that this data is seasonal, with peaks at approxiamtely the summer months of every year. This is of course due to travelers entering the country during the summer months on holiday. Additionally, we can also see a positive trend and cyclical patterns. One thing to point out is the dip in passengers during the summer of 2015. This is most likely due to the MERS outbreak in South Korea in May of 2015. This outbreak continued to that summer, and as a result, a siginifcantly fewer number of inbound tourists.\nThis plot depicts the smoothing moving average of our inbound passengers to South Korea. The smoothing windows I chose was 6 month, 12 month, and 36 months (or 3 years) on the monthly data. In all moving averages, we can confirm the existence of a smooth string positive upward trend in inbound passengers. Please note that because we are analyzing the nature of flights into country, we have removed the data from the year 2020 since the country of South Korea banned all incoming and outbound flights due to the COVID-19 global pandemic.\nNext, we’ll take a look a monthly and yearly lag plots for inbound passengers. As we can see in both plots, there a strong positive correlation betwen months as well as between years in the data. This means that the data is most likely not stationary. To confirm this hypothesis, we’ll analyze the ACF and PACF plots next.\nIn our ACF we see that there is a strong autocorrelation between all lags in the data as they are above the significance bands. While PACF for the most is not autocorrelated, we can say for now that differencing is nessesary. Using the augmented dickey fuller test, we can confirm our assumptions since we are rejecting the null hypothesis with 90% cofidence (0.09754 &lt; 0.1).\nThus, in order to model this data, we will need to make it stationary. From the plotting output, we see that first difference and detrended data still produce a siginifcant amount of autocorrelation. Thus, after trying both seasonal differencing and seasonal + first differencing, we can we that seasonal + first differencing produces the best ACF plot and approximately stationary data."
  },
  {
    "objectID": "eda.html#popularity-kpop",
    "href": "eda.html#popularity-kpop",
    "title": "Exploratory Data Analysis",
    "section": "Popularity: KPOP",
    "text": "Popularity: KPOP\n\nTime Series PlotMoving Average SmoothingLag PlotACF & PACFDickie-Fuller TestStationary\n\n\n\n\nCode\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\n\n# Define KPOP and Western artists\nkpop_artists &lt;- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists &lt;- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\n\nkpop_data &lt;- spotify %&gt;%\n  filter(artist_name %in% kpop_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE) %&gt;%\n  mutate(artist_type = \"KPOP\") %&gt;%\n  select(-artist_type) %&gt;%\n  mutate(album_release_year = as.Date(as.character(album_release_year), format = \"%Y\"))\n\n# Separate data for Western artists\nwestern_data &lt;- spotify %&gt;%\n  filter(artist_name %in% western_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE) %&gt;%\n  mutate(artist_type = \"Western\")%&gt;%\n  select(-artist_type) %&gt;%\n  mutate(album_release_year = as.Date(as.character(album_release_year), format = \"%Y\"))\n\nwrite_csv(kpop_data, \"cleaned_data/kpop_popularity.csv\")\nwrite_csv(western_data, \"cleaned_data/western_popularity.csv\")\n\n#Time series plot: \n\nkpop_ts &lt;- ts(kpop_data$popularity, frequency = 1)\n\nautoplot(kpop_ts)+ggtitle(\"Average Popularity Score of KPOP Artists\") \n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport datetime\nimport plotly \nfrom datetime import datetime \nfrom pandas_datareader import data as pdr\nimport plotly.offline as pyo\nimport plotly.graph_objects as go \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nkpop_data = pd.read_csv('cleaned_data/kpop_popularity.csv')\nkpop_data['album_release_year'] = pd.to_datetime(kpop_data['album_release_year'])\n\nkpop_data['2yma'] = kpop_data['popularity'].rolling(window = 2).mean()\nkpop_data['3yma'] = kpop_data['popularity'].rolling(window = 3).mean()\nkpop_data['4yma'] = kpop_data['popularity'].rolling(window = 4).mean()\n\nfig = px.line(kpop_data, x='album_release_year', y=['popularity', '2yma', '3yma', '4yma'],\n              labels={'value': 'Yearly Data', 'variable': 'Moving Average'},\n              title='Yearly Data and Moving Averages for the Popularity Score of KPOP Artists')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\ngglagplot(kpop_ts, do.lines=FALSE)+\n  ggtitle(\"Lag Plot for Popularity Score of KPOP Artists\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nggAcf(kpop_ts, lag.max = 30)+ggtitle(\"ACF Plot for Popularity Score of KPOP Artists\")\n\n\n\n\n\nCode\nggPacf(kpop_ts, lag.max = 30)+ggtitle(\"PACF Plot for Popularity Score of KPOP Artists\")\n\n\n\n\n\n\n\n\n\nCode\nkpop_test &lt;- adf.test(kpop_ts)\nprint(kpop_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  kpop_ts\nDickey-Fuller = 1.0158, Lag order = 2, p-value = 0.99\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nfit = lm(kpop_ts~time(kpop_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(kpop_ts, lag.max = 30, main=\"Original Data: Popularity Score of KPOP Artists\")\nplot2 &lt;- ggAcf(resid(fit), lag.max = 30, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(kpop_ts), lag.max = 30, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3, ncol=3)\n\n\n\n\n\n\n\n\nAs previously mentioned, the popularity score needed to be augmented due to Spotify’s recent changes regarding their API, resulting in a smaller number of data points.\nOur times series plot shows a steady, yet slightly positive trend in the data until 2023, where the popularity takes a sharp decline. This could be due to BTS, the most popular KPOP group, taking a hiatus this year due to their military enlistments. With that being said, we also cannot see any seasonality or cyclical trends.\nLooking at the sma plot, we can see that as we increase the yearly moving average to 4, we notice a downward trend to the data, clearly due to 2023. Similarly, the lag plots show that while their is some autocorrelation in the first 6 lags, this changes in the 9th lag, mainly due to the few lags available with our data.\nLooking at the ACF & PACF plots, we see that the data is well within the significance bounds, meaning that our data is stationary. Although the Dickie-Fuller test says otherwise, it is often inaccurate. Again, looking at the differenced and detrended data, we realize that the original data is stationary."
  },
  {
    "objectID": "eda.html#popularity-western",
    "href": "eda.html#popularity-western",
    "title": "Exploratory Data Analysis",
    "section": "Popularity: Western",
    "text": "Popularity: Western\n\nTime Series PlotMoving Average SmoothingLag PlotACF & PACFDickie-Fuller TestStationary\n\n\n\n\nCode\n#Time series plot: \n\nwestern_ts &lt;- ts(western_data$popularity, frequency = 1)\n\nautoplot(western_ts)+ggtitle(\"Average Popularity Score of Western Artists\") \n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport datetime\nimport plotly \nfrom datetime import datetime \nfrom pandas_datareader import data as pdr\nimport plotly.offline as pyo\nimport plotly.graph_objects as go \nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nwestern_data = pd.read_csv('cleaned_data/western_popularity.csv')\nwestern_data['album_release_year'] = pd.to_datetime(western_data['album_release_year'])\n\nwestern_data['2yma'] = western_data['popularity'].rolling(window = 2).mean()\nwestern_data['3yma'] = western_data['popularity'].rolling(window = 3).mean()\nwestern_data['4yma'] = western_data['popularity'].rolling(window = 4).mean()\n\nfig = px.line(western_data, x='album_release_year', y=['popularity', '2yma', '3yma', '4yma'],\n              labels={'value': 'Yearly Data', 'variable': 'Moving Average'},\n              title='Yearly Data and Moving Averages for the Popularity Score of Western Artists')\n\n# Show the plot\nfig.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nCode\ngglagplot(western_ts, do.lines=FALSE)+\n  ggtitle(\"Lag Plot for Popularity Score of Western Artists\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nggAcf(western_ts, lag.max = 30)+ggtitle(\"ACF Plot for Popularity Score of Western Artists\")\n\n\n\n\n\nCode\nggPacf(western_ts, lag.max = 30)+ggtitle(\"PACF Plot for Popularity Score of Western Artists\")\n\n\n\n\n\n\n\n\n\nCode\nkpop_test &lt;- adf.test(western_ts)\nprint(kpop_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  western_ts\nDickey-Fuller = -3.194, Lag order = 2, p-value = 0.1175\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nfit = lm(western_ts~time(western_ts), na.action=NULL) \n\nplot1 &lt;- ggAcf(western_ts, lag.max = 30, main=\"Original Data: Popularity Score of Western Artists\")\nplot2 &lt;- ggAcf(resid(fit), lag.max = 30, main=\"Detrended data\") \nplot3 &lt;- ggAcf(diff(western_ts), lag.max = 30, main=\"First Differenced Data\")\n\ngrid.arrange(plot1, plot2, plot3, ncol=3)\n\n\n\n\n\n\n\n\nAs previously mentioned, the popularity score needed to be augmented due to Spotify’s recent changes regarding their API, resulting in a smaller number of data points.\nOur times series plot shows a steady positive trend in the data. Additionally, we can see some signs of seasonal patterns, however, given that the data is minimal, the strength of the seasonality is not strong enough to consider it. Lastly, we do not see any cyclical patterns either.\nLooking at the sma plot, we can see that as we increase the yearly moving average to 4, we notice an upward trend to the data with slight downtrending at the very end of the moving average. Similarly, the lag plots show that while their is some autocorrelation in all lags, but it is very weak.\nLooking at the ACF & PACF plots, we see that the data is well within the significance bounds, meaning that our data is stationary. Although the Dickie-Fuller test says otherwise, it is often inaccurate. Again, looking at the differenced and detrended data, we realize that the original data is stationary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5600: Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Time Series - Shriya Chinthak",
    "section": "",
    "text": "In order to compare the tradition time series models, we’ll next take a look at deep learning models for comparison. The data we’ll take a look at is the KOF globalization index dataset we modeled through ARIMA. In order to compare the predictions of that model with deep learning, we will create 3 different neural network models (RNNs, GRU, LSTM) using the tensorflow package found in python. After we gather the predictions for these models, we’ll compare those predictions with ARIMA’s forecast via RMSE as well as test the forecasting reach of the models.\nFirst, we’ll begin by gathering the data and transforming it such that it is suitable for deep learning.\n\n\nData Transformation\nsk_passengers = pd.read_excel('sk_passenger_arrivals.xlsx')\n\n# Manipulate data\nsk_passengers['date'] = sk_passengers['year'].astype(str) + '-' + sk_passengers['month'].astype(str)\nsk_passengers['date'] = pd.to_datetime(sk_passengers['date'] + '-01')\nsk_passengers = sk_passengers[sk_passengers['date'].dt.year &lt; 2020]\n\ndf = sk_passengers\n\ndf = df.rename(columns={\"date\": \"t\", \"Passengers\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n\n\nCHECK NA:\n t    0\ny    0\ndtype: int64\n\n\n\n\n\n\n\n\n\n(1,) (1,)\n(111, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{python}\n# training\nL=5; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n\n\ntraining: (82, 6, 1) (82, 1)\nvalidation: (17, 6, 1) (17, 1)\n\n\n\nxf = np.arange(112,150)\nxf\n\narray([112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n       125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,\n       138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149])\n\n\n\n\n(82, 6, 1) --&gt; (82, 1)\n(17, 6, 1) --&gt; (17, 1)\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_4 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n3/3 [==============================] - 0s 3ms/step\n1/1 [==============================] - 0s 29ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.009056111857535269\n MAE: 0.06824875608318318\nVALIDATION:\n MSE: 0.15561706540419193\n MAE: 0.3156400454777772\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(2, 7):\n  # training\n  L=i; \n  S=1; \n  D=1\n  Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  # validation\n  Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n  print(\"training:\",Xt.shape,Yt.shape)\n  print(\"validation:\",Xv.shape,Yv.shape)\n\n  # Predictions \n  Ytp=model.predict(Xt)\n  Yvp=model.predict(Xv) \n\n  print(mean_squared_error(Yt,Ytp)**(1/2))\n  print(mean_squared_error(Yv,Yvp)**(1/2))\n\ntraining: (85, 3, 1) (85, 1)\nvalidation: (20, 3, 1) (20, 1)\n3/3 [==============================] - 1s 6ms/step\n1/1 [==============================] - 0s 274ms/step\n0.5679406055875261\n0.5995376140863773\ntraining: (84, 4, 1) (84, 1)\nvalidation: (19, 4, 1) (19, 1)\n3/3 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 74ms/step\n0.6239995142798676\n1.1562064884696703\ntraining: (83, 5, 1) (83, 1)\nvalidation: (18, 5, 1) (18, 1)\n3/3 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 49ms/step\n0.597491731323768\n0.614835713473727\ntraining: (82, 6, 1) (82, 1)\nvalidation: (17, 6, 1) (17, 1)\n3/3 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 83ms/step\n0.09516360574051022\n0.39448328913173486\ntraining: (81, 7, 1) (81, 1)\nvalidation: (16, 7, 1) (16, 1)\n3/3 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 44ms/step\n0.7807362726550906\n0.39488661117339785\n\n\n\n\n(82, 6, 1) --&gt; (82, 1)\n(17, 6, 1) --&gt; (17, 1)\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n3/3 [==============================] - 0s 3ms/step\n1/1 [==============================] - 0s 40ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0418220380895702\n MAE: 0.12986182084202078\nVALIDATION:\n MSE: 0.15584366942611402\n MAE: 0.30081442562794863\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(82, 6, 1) --&gt; (82, 1)\n(17, 6, 1) --&gt; (17, 1)\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n3/3 [==============================] - 0s 4ms/step\n1/1 [==============================] - 0s 49ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.05042644584238158\n MAE: 0.15809287478300865\nVALIDATION:\n MSE: 0.1312755106032139\n MAE: 0.2522199396106808\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "TEST.html",
    "href": "TEST.html",
    "title": "Untitled",
    "section": "",
    "text": "library(quantmod)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(imputeTS)\n\n\nAttaching package: 'imputeTS'\n\nThe following object is masked from 'package:zoo':\n\n    na.locf\n\nlibrary(vars)\n\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nLoading required package: strucchange\nLoading required package: sandwich\n\nAttaching package: 'strucchange'\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nLoading required package: urca\nLoading required package: lmtest\n\nlibrary(forecast)\nlibrary(astsa) \n\n\nAttaching package: 'astsa'\n\nThe following object is masked from 'package:forecast':\n\n    gas\n\nlibrary(xts)\nlibrary(tseries)\n\n\nAttaching package: 'tseries'\n\nThe following object is masked from 'package:imputeTS':\n\n    na.remove\n\nlibrary(fpp2)\n\n── Attaching packages ────────────────────────────────────────────── fpp2 2.5 ──\n✔ fma       2.5     ✔ expsmooth 2.3\n\n\nAttaching package: 'fpp2'\n\nThe following object is masked from 'package:astsa':\n\n    oil\n\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(tidyquant)\n\nLoading required package: PerformanceAnalytics\n\nAttaching package: 'PerformanceAnalytics'\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\n\nAttaching package: 'tidyquant'\n\nThe following object is masked from 'package:vars':\n\n    VAR\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:MASS':\n\n    select\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(ggplot2)\nlibrary(TSA)\n\nRegistered S3 methods overwritten by 'TSA':\n  method       from    \n  fitted.Arima forecast\n  plot.Arima   forecast\n\nAttaching package: 'TSA'\n\nThe following objects are masked from 'package:PerformanceAnalytics':\n\n    kurtosis, skewness\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nThe following objects are masked from 'package:stats':\n\n    acf, arima\n\nThe following object is masked from 'package:utils':\n\n    tar\n\n#install.packages(\"grDevices\")\n#library(grDevices)\nlibrary(fGarch) \n\nNOTE: Packages 'fBasics', 'timeDate', and 'timeSeries' are no longer\nattached to the search() path when 'fGarch' is attached.\n\nIf needed attach them yourself in your R script by e.g.,\n        require(\"timeSeries\")\n\nAttaching package: 'fGarch'\n\nThe following object is masked from 'package:TTR':\n\n    volatility\n\nlibrary(dynlm)\nlibrary(dygraphs)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:TSstudio':\n\n    plot_grid\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\n\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nWarning: 041510.KQ contains missing values. Some functions will not work if\nobjects contain missing values in the middle of the series. Consider using\nna.omit(), na.approx(), na.fill(), etc to remove or replace them.\n\n\nWarning: 122870.KQ contains missing values. Some functions will not work if\nobjects contain missing values in the middle of the series. Consider using\nna.omit(), na.approx(), na.fill(), etc to remove or replace them.\n\n\nWarning: 035900.KQ contains missing values. Some functions will not work if\nobjects contain missing values in the middle of the series. Consider using\nna.omit(), na.approx(), na.fill(), etc to remove or replace them.\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(UMGP_Price = UMGP.Adjusted)\n\nstart_date &lt;- as.Date(min(UMGP$Date))  \nend_date &lt;- as.Date(max(UMGP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nUMGP &lt;- merge(UMGP, date_dataset, by = 'Date', all = TRUE)\n\nimputed_time_series &lt;- imputeTS::na_ma(UMGP, k = 4, weighting = \"exponential\")\nUMGP &lt;- data.frame(imputed_time_series)\n\n#---\n\nSONY &lt;- data.frame(SONY$SONY.Adjusted)\nSONY &lt;- SONY %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SONY_Price = SONY.Adjusted)\n\n\nstart_date &lt;- as.Date(min(SONY$Date))  \nend_date &lt;- as.Date(max(SONY$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSONY &lt;- merge(SONY, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SONY[which(rowSums(is.na(SONY)) &gt; 0),]\ndf_na_cols &lt;- SONY[, which(colSums(is.na(SONY)) &gt; 0)]\nimputed_time_series &lt;- imputeTS::na_ma(SONY, k = 4, weighting = \"exponential\")\nSONY &lt;- data.frame(imputed_time_series)\n\n#---\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(HYBE_Price = X352820.KS.Adjusted) %&gt;%\n  mutate(HYBE_Price = HYBE_Price/1352.60)\n\nstart_date &lt;- as.Date(min(HYBE$Date))  \nend_date &lt;- as.Date(max(HYBE$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nHYBE &lt;- merge(HYBE, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- HYBE[which(rowSums(is.na(HYBE)) &gt; 0),]\ndf_na_cols &lt;- HYBE[, which(colSums(is.na(HYBE)) &gt; 0)]\nimputed_time_series &lt;- imputeTS::na_ma(HYBE, k = 4, weighting = \"exponential\")\nHYBE &lt;- data.frame(imputed_time_series)\n\n#--- \n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(SM_Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(SM_Price = SM_Price/1352.60)\n\nstart_date &lt;- as.Date(min(SM$Date))  \nend_date &lt;- as.Date(max(SM$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nSM &lt;- merge(SM, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- SM[which(rowSums(is.na(SM)) &gt; 0),]\ndf_na_cols &lt;- SM[, which(colSums(is.na(SM)) &gt; 0)]\nimputed_time_series &lt;- imputeTS::na_ma(SM, k = 4, weighting = \"exponential\")\nSM &lt;- data.frame(imputed_time_series)\n\n#---\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(YG_Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(YG_Price = YG_Price/1352.60)\n\nstart_date &lt;- as.Date(min(YG$Date))  \nend_date &lt;- as.Date(max(YG$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nYG &lt;- merge(YG, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- YG[which(rowSums(is.na(YG)) &gt; 0),]\ndf_na_cols &lt;- YG[, which(colSums(is.na(YG)) &gt; 0)]\nimputed_time_series &lt;- imputeTS::na_ma(YG, k = 4, weighting = \"exponential\")\nYG &lt;- data.frame(imputed_time_series)\n\n#---\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(JYP_Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(JYP_Price = JYP_Price/1352.60)\n\nstart_date &lt;- as.Date(min(JYP$Date))  \nend_date &lt;- as.Date(max(JYP$Date))    \ndate_range &lt;- seq(start_date, end_date, by = \"1 day\")\ndate_dataset &lt;- data.frame(Date = date_range)\nJYP &lt;- merge(JYP, date_dataset, by = 'Date', all = TRUE)\ndf_na_rows &lt;- JYP[which(rowSums(is.na(JYP)) &gt; 0),]\ndf_na_cols &lt;- JYP[, which(colSums(is.na(JYP)) &gt; 0)]\nimputed_time_series &lt;- imputeTS::na_ma(JYP, k = 4, weighting = \"exponential\")\nJYP &lt;- data.frame(imputed_time_series)\n\nstock_dataframes &lt;- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\n\nCode for spotify API: (NOT USING FOR NOW)\n\n\nCode\n# access_token &lt;- get_spotify_access_token()\n# artists &lt;- c(\"BTS\", \"Taylor Swift\", \"Twice\")\n# \n# BTS &lt;- get_artist_audio_features(\"BTS\")\n# BTS_A &lt;- data.frame(BTS$artist_name,\n# BTS$instrumentalness,\n# BTS$valence,\n# BTS$danceability,\n# BTS$energy,\n# BTS$loudness,\n# BTS$speechiness,\n# BTS$acousticness,\n# BTS$liveness,\n# BTS$tempo,\n# BTS$track_name,\n# BTS$album_name,\n# BTS$album_release_year,\n# BTS$album_release_date)\n# \n# colnames(BTS_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n# \"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n# \"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n# \n# Taylor_Swift &lt;- get_artist_audio_features(\"Taylor Swift\")\n# Taylor_Swift_A &lt;- data.frame(Taylor_Swift$artist_name,\n# Taylor_Swift$instrumentalness,\n# Taylor_Swift$valence,\n# Taylor_Swift$danceability,\n# Taylor_Swift$energy,\n# Taylor_Swift$loudness,\n# Taylor_Swift$speechiness,\n# Taylor_Swift$acousticness,\n# Taylor_Swift$liveness,\n# Taylor_Swift$tempo,\n# Taylor_Swift$track_name,\n# Taylor_Swift$album_name,\n# Taylor_Swift$album_release_year,\n# Taylor_Swift$album_release_date)\n# \n# colnames(Taylor_Swift_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n# \"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n# \"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n# \n# \n# Twice &lt;- get_artist_audio_features(\"Twice\")\n# Twice_A &lt;- data.frame(Twice$artist_name,\n# Twice$instrumentalness,\n# Twice$valence,\n# Twice$danceability,\n# Twice$energy,\n# Twice$loudness, \n# Twice$speechiness,\n# Twice$acousticness,\n# Twice$liveness,\n# Twice$tempo,\n# Twice$track_name,\n# Twice$album_name,\n# Twice$album_release_year,\n# Twice$album_release_date)\n# \n# colnames(Twice_A) &lt;- c(\"artist_name\",\"instrumentalness\",\"Valence\",\"danceability\",\"energy\",\n# \"loudness\",\"speechiness\",\"acousticness\",\"liveness\",\n# \"tempo\",\"track_name\",\"album_name\",\"album_release_year\",\"date\")\n# \n# artists &lt;- rbind(BTS_A, Taylor_Swift_A, Twice_A)\n# artists$date &lt;- as.Date(artists$date, format = \"%Y-%m-%d\")\n# \n# fig &lt;- plot_ly(artists, x = ~date, y = ~danceability, color = ~artist_name, \n#                type = 'scatter', mode = 'markers', size = ~speechiness) \n# \n# fig &lt;- fig %&gt;% layout(xaxis = list(title = \"Album Released Date\"),\n#                       yaxis = list(title =\"Danceability\"), \n#                       title = \"Danceability - Taylor Swift/BTS/Twice\")\n# \n# fig\n\n\nMOVE TO VIZUALIZATION PAGE:\n\n\nCode\n# basic example of ohlc charts\ndf &lt;- data.frame(Date=index(`352820.KS`),coredata(`352820.KS`))\ndf &lt;- tail(df, 365)\n\nfig &lt;- df %&gt;% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~X352820.KS.Open, close = ~X352820.KS.Close,\n          high = ~X352820.KS.High, low = ~X352820.KS.Low) \nfig &lt;- fig %&gt;% layout(title = \"Universal Music Group Candlestick Chart\")\n\nfig\n\n\n\n\n\n\nSPOTIFY VIZ:\n\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\nglimpse(spotify)\n\nRows: 2,013\nColumns: 17\n$ X                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ artist_name        &lt;chr&gt; \"Drake\", \"Drake\", \"Drake\", \"Drake\", \"Drake\", \"Drake…\n$ instrumentalness   &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0…\n$ valence            &lt;dbl&gt; 0.2000, 0.3090, 0.1040, 0.0658, 0.0667, 0.2450, 0.1…\n$ danceability       &lt;dbl&gt; 0.402, 0.646, 0.571, 0.697, 0.566, 0.475, 0.673, 0.…\n$ energy             &lt;dbl&gt; 0.514, 0.605, 0.550, 0.320, 0.564, 0.655, 0.672, 0.…\n$ loudness           &lt;dbl&gt; -7.322, -7.590, -6.567, -9.347, -7.880, -7.342, -8.…\n$ speechiness        &lt;dbl&gt; 0.0471, 0.1880, 0.1060, 0.2140, 0.2770, 0.3300, 0.2…\n$ acousticness       &lt;dbl&gt; 0.1060, 0.0636, 0.0548, 0.0127, 0.0149, 0.0306, 0.0…\n$ liveness           &lt;dbl&gt; 0.1810, 0.2530, 0.1790, 0.1800, 0.1040, 0.3750, 0.1…\n$ tempo              &lt;dbl&gt; 120.009, 90.156, 120.947, 136.976, 139.920, 163.997…\n$ track_name         &lt;chr&gt; \"Virginia Beach\", \"Amen (feat. Teezo Touchdown)\", \"…\n$ track_id           &lt;chr&gt; \"6YV2AI87l1n2fzqU8Dyo05\", \"59ZmQR4pwCaa5iM3veM7Cs\",…\n$ album_name         &lt;chr&gt; \"For All The Dogs Scary Hours Edition\", \"For All Th…\n$ album_release_year &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202…\n$ album_release_date &lt;chr&gt; \"2023-11-17\", \"2023-11-17\", \"2023-11-17\", \"2023-11-…\n$ popularity         &lt;int&gt; 83, 83, 81, 86, 85, 84, 88, 88, 86, 87, 95, 100, 91…\n\n\n\nspotify &lt;- spotify %&gt;%\n  select(artist_name, album_release_year, popularity) %&gt;%\n  group_by(artist_name, album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE)\n\n\nspotify &lt;- spotify %&gt;%\n  mutate(album_release_year_numeric = as.numeric(as.character(album_release_year)))\n\n# Create a sequence of years from debut to 2023 for each artist\nyearly_data &lt;- spotify %&gt;%\n  group_by(artist_name) %&gt;%\n  complete(album_release_year_numeric = full_seq(album_release_year_numeric, 1)) %&gt;%\n  fill(album_release_year, .direction = \"up\") %&gt;%\n  ungroup()\n\n# Step 3: Convert 'album_release_year_numeric' back to a factor\nyearly_data &lt;- yearly_data %&gt;%\n  mutate(album_release_year = as.factor(album_release_year_numeric)) %&gt;%\n  select(-album_release_year_numeric) %&gt;%\n  fill(popularity)\n\nwrite_csv(yearly_data, \"cleaned_data/artist_popularity_cleaned.csv\")\n\n\nplot &lt;- plot_ly(yearly_data, x = ~album_release_year, y = ~popularity, color = ~artist_name, type = 'scatter', mode = 'lines')\n\n# Combine plot and layout\nplot &lt;- layout(plot, title = \"Average Popularity Over Years by Artist\",\n               xaxis = list(title = \"Album Release Year\"),\n               yaxis = list(title = \"Popularity\"))\n\n# Show the plot\nplot\n\n\n\n\n\n\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\n\nlibrary(plotly)\n\n\n\n# Define KPOP and Western artists\nkpop_artists &lt;- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists &lt;- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\n\n# Group by artist type and calculate the average for each musical characteristic\navg_characteristics &lt;- spotify %&gt;%\n  mutate(artist_type = ifelse(artist_name %in% kpop_artists, \"KPOP\", \"Western\")) %&gt;%\n  group_by(artist_type) %&gt;%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE)`.\nℹ In group 1: `artist_type = \"KPOP\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n# Reshape data for plotting\navg_characteristics_long &lt;- pivot_longer(avg_characteristics, \n                                         cols = -artist_type,\n                                         names_to = \"musical_characteristic\",\n                                         values_to = \"average_value\")\n\nwrite_csv(avg_characteristics_long, \"cleaned_data/artist_avg_characteristics.csv\")\n\n\navg_characteristics_long &lt;- read.csv(\"cleaned_data/artist_avg_characteristics.csv\")\n\nggplot(avg_characteristics_long, aes(x = average_value, y =artist_type, \n                                     fill = artist_type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ musical_characteristic, scales = \"free_x\", nrow = 3, ncol = 3) +\n  labs(title = \"Average Musical Characteristics by Artist Type\",\n       x = \"Average Value\",\n       y = \"Musical Characteristic\") +\n  geom_text(aes(label = round(average_value, 2)), \n            position = position_dodge2(width = 0.9), hjust = 1.2) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(angle = 0, hjust = 1, vjust = -1))\n\n\n\n\n\npassengers &lt;- read.csv(\"cleaned_data/air_passengers_cleaned.csv\")\n\npassengers &lt;- passengers %&gt;%\n  mutate(date = as.Date(date))\n\nline_plot &lt;- ggplot(passengers, aes(x = date, y = Passengers)) +\n  geom_line() +\n  labs(title = \"Air Passengers Over Time\",\n       x = \"Year-Month\",\n       y = \"Number of Passengers\")\n\n# Create the histogram\ndensity_plot &lt;- ggplot(passengers, aes(x = Passengers)) +\n  geom_density(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Density Plot of Air Passengers\",\n       x = \"Number of Passengers\",\n       y = \"Density\")\n\n# Create the box plot\nbox_plot &lt;- ggplot(passengers, aes(x = 1, y = Passengers)) +\n  geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Box Plot of Air Passengers\",\n       x = \"\",\n       y = \"Number of Passengers\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\nfacet_wrap_plot &lt;- plot_grid(line_plot, density_plot, box_plot, ncol = 3)\n\n# Display the facet wrap\nprint(facet_wrap_plot)\n\n\n\n\n\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\n\n\n# Define KPOP and Western artists\nkpop_artists &lt;- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists &lt;- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\n\nkpop_data &lt;- spotify %&gt;%\n  filter(artist_name %in% kpop_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE) %&gt;%\n  mutate(artist_type = \"KPOP\") %&gt;%\n  select(-artist_type) %&gt;%\n  mutate(album_release_year = as.Date(as.character(album_release_year), format = \"%Y\"))\n\n# Separate data for Western artists\nwestern_data &lt;- spotify %&gt;%\n  filter(artist_name %in% western_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE) %&gt;%\n  mutate(artist_type = \"Western\")%&gt;%\n  select(-artist_type) %&gt;%\n  mutate(album_release_year = as.Date(as.character(album_release_year), format = \"%Y\"))\n\n\nglimpse(kpop_data)\n\nRows: 11\nColumns: 2\n$ album_release_year &lt;date&gt; 2013-12-08, 2014-12-08, 2015-12-08, 2016-12-08, 20…\n$ popularity         &lt;dbl&gt; 77.95556, 78.20000, 76.21212, 77.53535, 77.90789, 7…\n\nwrite_csv(kpop_data, \"cleaned_data/kpop_popularity.csv\")\nwrite_csv(western_data, \"cleaned_data/western_popularity.csv\")\n\n\nkpop_artists &lt;- c(\"BLACKPINK\", \"BTS\", \"EXO\", \"Twice\")\nwestern_artists &lt;- c(\"Harry Styles\", \"Beyoncé\", \"Drake\", \"Taylor Swift\")\n\n\nkpop_arimax &lt;- spotify %&gt;%\n  filter(artist_name %in% kpop_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\nwestern_arimax &lt;- spotify %&gt;%\n  filter(artist_name %in% western_artists) %&gt;%\n  group_by(album_release_year) %&gt;%\n  summarise(across(starts_with(\"instrumentalness\"), mean, na.rm = TRUE),\n            across(starts_with(\"valence\"), mean, na.rm = TRUE),\n            across(starts_with(\"danceability\"), mean, na.rm = TRUE),\n            across(starts_with(\"energy\"), mean, na.rm = TRUE),\n            across(starts_with(\"loudness\"), mean, na.rm = TRUE),\n            across(starts_with(\"speechiness\"), mean, na.rm = TRUE),\n            across(starts_with(\"acousticness\"), mean, na.rm = TRUE),\n            across(starts_with(\"liveness\"), mean, na.rm = TRUE),\n            across(starts_with(\"tempo\"), mean, na.rm = TRUE),\n            across(starts_with(\"popularity\"), mean, na.rm = TRUE))\n\n\n\n\n Back to top"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "The Effect of KPOP on the Western Music Industry",
    "section": "",
    "text": "Music as a representation of culture has been defined since the birth of human connection and communities for centuries. In the past few decades, we have seen a transformation in music listening and sharing, from physical records, to digital purchases, to modern day streaming. Specifically in the United States (U.S.), popular music has, for the most part, been contained to English speaking American artists. In recent years, however, the rise of KPOP, through groups like BTS, EXO, BlackPink, and TWICE has become increasingly popular in Western markets, to the point where it is considered part of the Pop genre Wang (2018). With this new found accessibility of KPOP music through streaming, there comes the question of how music listening and cultural exchange has changed in the U.S.\nWe can define such a question as the effect of KPOP on the Western Music Industry and its impacts on globalization as a whole. At the moment, there is literature to discuss both viewpoints of music globalization; the first being that streaming has expanded the average U.S. consumer’s music palette from traditional American music, while the second suggests that streaming has only narrowed the average listener’s taste through algorithmic suggestions and playlists Bello and Garcia (2021). Cultural Divergence in popular music suggests that while clustering in music tastes can be noted for specific regions of the world, we do see an “upward trend in music consumption diversity that started in 2017 and spans across platforms” Bello and Garcia (2021).\nThus, in order to fully realize the question of KPOP’s effect on the western music industry through time, I propose analyzing the following: music charting on the Billboard 100, stock prices for music record companies, musical characterists and popularity, and quantifying globalization over time."
  },
  {
    "objectID": "introduction.html#western-artists",
    "href": "introduction.html#western-artists",
    "title": "The Effect of KPOP on the Western Music Industry",
    "section": "Western Artists",
    "text": "Western Artists\n\n\nDrake\nDrake is a Canadian rapper, singer, and songwriter who rose to fame in the mid-2000s. He initially gained recognition as a child actor on the teen drama “Degrassi: The Next Generation” prior to transitioning to music. Drake’s debut studio album, “Thank Me Later” (2010), was a commercial success, setting the stage for a number of hit records, including “Take Care” (2011) and “Views” (2016). He is know for his versatile style that blending rap and R&B into a new form of comtemporary rap. Additionally, Drake has consistently dominated the Billboard charts. He holds numerous records, including the most charted songs by a solo artist in the history of the Billboard Hot 100. With multiple Grammy Awards and an extensive list of chart-topping singles and albums, Drake stands as one of the most influential and successful artists in contemporary music “Drake (Musician) - Wikipedia” (2023).\n\n\nTaylor Swift\nTaylor Swift is an American singer-songwriter who began her career as a country music artist before evolving into a global pop superstar. Swift released her self-titled debut album in 2006, marking the beginning of her commercial success. However, it was with albums like “Fearless” (2008) and “1989” (2014) that she achieved widespread acclaim. She is knwon for her heartfelt songwriting and relatable lyrics and has won numerous Grammy Awards, including Album of the Year for “Fearless” and “1989.” She holds several Billboard records, including being the first woman to replace herself at the top of the Hot 100 chart with “Blank Space” succeeding “Shake It Off”. More recently, she has once again broken records with her re-recorded albums as a result of legal battle with her previous label, Big Machine, taking full ownership of her masters. She has also had one of the most successful tours in music industry this year with her 2023 tour, the Eras Tour. With an impressive catalog of chart-topping hits, Taylor Swift remains one of the most influential and successful artists in the music industry “Taylor Swift - Wikipedia” (2023).\n\n\nHarry Styles\nHarry Styles is an English singer, songwriter, and actor who initially gained fame as a member of the globally successful boy band, One Direction. After the group’s hiatus in 2016, Styles started his solo career and made a notable shift to a more eclectic and mature musical style. His self-titled debut solo album, released in 2017, showcased his versatility and garnered critical acclaim. Styles continued his success with the release of “Fine Line” in 2019, and “Harry’s House” in 2022. Billboard achievements include chart-topping singles and albums, with “Watermelon Sugar” and “As it Was” earning him number 1’s on the Billboard Hot 100. Recognized for his distinctive voice, charismatic stage presence, and fashion-forward style, Harry Styles has established himself as a prominent figure in the music industry “Harry Styles - Wikipedia” (2023).\n\n\nBeyoncé\nBeyoncé is an iconic American singer, songwriter, actress, and producer who first gained fame as the lead vocalist of Destiny’s Child, one of the world’s best-selling girl groups of the early 2000’s. Embarking on a solo career shortly after, Beyoncé has consistently pushed boundaries with her music, incorporating R&B, pop, and hip-hop influences. Her solo debut album, “Dangerously In Love” (2003), marked the beginning of a highly successful solo career. She has earned numerous Grammy Awards and achieved remarkable Billboard success, holding records for the most No. 1 hits by a female artist in the Hot 100 chart. Recnetly, she continued to break records with the release of her album “Reniassance” and its accompanying tour. Renowned for her powerful vocals, dynamic performances, and impactful cultural influence, Beyoncé stands as one of the greatest entertainers in modern music history “Beyoncé - Wikipedia” (2023).\nThese four artists have transformed the music industry for the past decade and continue to break records in the music industry."
  },
  {
    "objectID": "introduction.html#kpop-artists",
    "href": "introduction.html#kpop-artists",
    "title": "The Effect of KPOP on the Western Music Industry",
    "section": "KPOP Artists",
    "text": "KPOP Artists\n\n\nBTS\nBTS, short for Bangtan Sonyeondan, is a South Korean boy band that has become a global phenomenon. Formed by Big Hit Entertainment in 2013, the group consists of members RM, Jin, Suga, J-Hope, Jimin, V, and Jungkook. BTS has been the trailblazer in the K-pop genre into the Western world, achieving unprecedented success worldwide. Their music blends various genres and addresses diverse themes, resonating with a massive and dedicated fanbase known as the ARMY. BTS has garnered multiple Billboard achievements, including numerous chart-topping albums and singles. Their track “Dynamite” made history by becoming the first song by a Korean act to debut at No. 1 on the Billboard Hot 100. With this success, they transformed the small company, Big Hit Music, into HYBE, a record label that represents both KPOP and Western artists around the globe. Beyond music, BTS has been recognized for their philanthropy, advocacy, and positive influence, solidifying their status as global cultural icons “BTS - Wikipedia” (2023).\n\n\nBlackPink\nBlackPink, a South Korean girl group formed by YG Entertainment (one of the former “Big 3” KPOP record labels), comprises of members Jisoo, Jennie, Rosé, and Lisa. Debuting in 2016, BlackPink quickly gained international prominence with their pop and hip-hop-infused music. Their breakthrough came with hits like “Boombayah” and “Whistle.” The group continued to make waves with successful releases such as “DDU-DU DDU-DU” and “Kill This Love.” Their Billboard achievements include impressive chart positions and record-breaking views on YouTube. Additionally, BlackPink became the first K-pop girl group to perform at the Coachella Music Festival in 2019. BlackPink has solidified their status as one of the most influential and popular K-pop acts worldwide “Blackpink - Wikipedia” (2023).\n\n\nEXO\nEXO, a South Korean boy group formed by SM Entertainment (one of the former “Big 3” KPOP record labels), debuted in 2012 and became a powerhouse in the K-pop industry. The group originally consisted of 12 members but later downsized to nine: Xiumin, Suho, Lay, Baekhyun, Chen, Chanyeol, D.O., Kai, and Sehun. EXO achieved widespread success with albums like “XOXO” and “EXODUS.” While their primary success has been in the Asian market, with numerous chart-topping hits, they also made a mark internationally. While their presence on Billboard may not be as extensive as some Western artists, EXO has garnered notable achievements, solidifying their status as one of K-pop’s most influential groups “EXO - Wikipedia” (2023).\n\n\nTwice\nTwice, a South Korean girl group formed by JYP Entertainment (one of the former “Big 3” KPOP record labels), made their debut in 2015 and quickly became one of the leading acts in the K-pop industry. Comprising members Nayeon, Jeongyeon, Momo, Sana, Jihyo, Mina, Dahyun, Chaeyoung, and Tzuyu, Twice gained recognition for their infectious pop tunes and engaging performances. They secured their position as one of the best-selling girl groups in South Korea. While Twice has predominantly found success in the Asian market, they have also made notable strides globally, amassing a dedicated fanbase called “Once.” Although they may not have extensive Billboard achievements compared to some Western counterparts, Twice has undeniably left a significant impact on the global pop scene “Twice - Wikipedia” (2023)."
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization serves the purpose of uncovering potential trends, identifying patterns, and detecting anomalies present in datasets. By transforming cleaned data into graphical representations, it enables a more intuitive understanding of complex information, aiding in data-driven decision-making processes. Below are the visualization for our datasets."
  },
  {
    "objectID": "data-visualization.html#kof-globalization",
    "href": "data-visualization.html#kof-globalization",
    "title": "Data Visualization",
    "section": "KOF Globalization",
    "text": "KOF Globalization\nThe KOF Globalization index quantifies globalization within a country through a multitude of lenses. We will be looking at the general globalization index values as well as technology, culture, and TV and Media, since it all correlates to sphere of music and music streaming in the modern world.\nThrough R, I cleaning the dataset from KOF and used Tableau to create interative global maps for each respective index system.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\n\nglobal &lt;- read_excel(\"raw_data/KOFGI_2022_public.xlsx\")\nglobal &lt;- global %&gt;%\n  select(code, country, year, KOFGI, KOFInGIdf, KOFInGIdj, KOFCuGIdf) %&gt;%\n  drop_na(c(KOFGI, KOFInGIdf, KOFInGIdj, KOFCuGIdf))\n\nwrite.csv(global, \"cleaned_data/globalization.csv\", row.names=FALSE)\n\n\nFrom this cleaned data, we can visualize the globalization index over time for general globalization as well as indexes specifc to culture and entertainment.\nPlease allow for the visualization to load when moving the slider as it handles a large amount of data\n\n\n\n\n\n\n   \n\n\n\n\nFrom the graphs, we can a gradual increase in the globalization index over the past 50 years. Specifically, North America, Western Europe, and Australia have the highest globalization indexes across all index types throughout the 50 years. Additionally, we can see that in 1990, Russia’s globalization index was added, resulting in the world’s largest populous countries to be added to the index. Once we get to 2000, we continue to see the highest globalization values in North America, Western Europe (specifically UK and France), and Australia, with new additions like South Korea and Japan with higher values. By 2020, the aforementioned countries have even higher index values, most in the 80’s and 90’s out of 100. China, Russia, Brazil, and India are also most notable countries with very high globalization values, especially in the technology category."
  },
  {
    "objectID": "data-visualization.html#record-label-stocks",
    "href": "data-visualization.html#record-label-stocks",
    "title": "Data Visualization",
    "section": "Record Label Stocks",
    "text": "Record Label Stocks\nNext, in order to better understand globalization within the music industry, we will take a look at large record label stock prices over time. I chose the most popular record labels who house some of the largest musicians in the industry.\n\nUniversal Music Group Inc. (UMGP)\nSONY Group Corporation (SONY)\nHYBE Co. (352820.KS)\nSM Entertainment (041510.KQ)\nYG Entertainment (122870.KQ)\nJYP Entertainment (035900.KQ)\n\nI used plotly in R to show this interactive line plot.\n\n\nCode\nlibrary(plotly)\nlibrary(quantmod)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UMGP\", \"SONY\", \"352820.KS\", \"041510.KQ\", '122870.KQ', '035900.KQ')\n\nfor (i in tickers){\n  getSymbols(i, from = \"2000-01-01\", to = \"2023-11-01\")\n}\n\nUMGP &lt;- data.frame(UMGP$UMGP.Adjusted)\nUMGP &lt;- UMGP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = UMGP.Adjusted)\nwrite.csv(UMGP, \"raw_data/umgp_stock.csv\", row.names=FALSE)\n\nSONY &lt;- data.frame(SONY$SONY.Adjusted)\nSONY &lt;- SONY %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = SONY.Adjusted)\nwrite.csv(SONY, \"raw_data/sony_stock.csv\", row.names=FALSE)\n\nHYBE &lt;- data.frame(`352820.KS`$`352820.KS.Adjusted`)\nHYBE &lt;- HYBE %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X352820.KS.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\nwrite.csv(HYBE, \"raw_data/hybe_stock.csv\", row.names=FALSE)\n\nSM &lt;- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)\nSM &lt;- SM %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X041510.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\nwrite.csv(SM, \"raw_data/sm_stock.csv\", row.names=FALSE)\n\nYG &lt;- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)\nYG &lt;- YG %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X122870.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\nwrite.csv(YG, \"raw_data/yg_stock.csv\", row.names=FALSE)\n\nJYP &lt;- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)\nJYP &lt;- JYP %&gt;%\n  rownames_to_column(var = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  rename(Price = X035900.KQ.Adjusted) %&gt;%\n  mutate(Price = Price/1352.60)\nwrite.csv(JYP, \"raw_data/jyp_stock.csv\", row.names=FALSE)\n\nstock_dataframes &lt;- list(UMGP, SONY, HYBE, SM, YG, JYP)\nstock_names &lt;- list(\"UMGP\", \"SONY\", \"HYBE\", \"SM\", \"YG\", \"JYP\")\np &lt;- plot_ly()\n\n# Add traces for each stock to the plot\nfor (i in 1:length(stock_dataframes)) {\n  stock_df &lt;- stock_dataframes[[i]]\n  p &lt;- add_trace(p, x = stock_df$Date, y = stock_df$Price, type = 'scatter', mode = 'lines', name = stock_names[i])\n}\n\n# Customize the layout if needed\np &lt;- layout(p, title = \"Stock Prices of Music Record Label\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Price (USD)\"))\n\n# Show the plot\np\n\n\n\n\n\n\nFrom the line plot, we can analyze these record labels as they enter the market to today. Starting with the oldest public record label, SONY seems to have a seasonal downward trend from 2000 - 2001, however that quickly ends to a more irregular and constant movement. We can see a large spike again in the early 2022, possibly due to SONY’s press release for the vison-s suv, a self driving car Lawler (2022). Of the other stocks, a notable one to note is the introduction of HYBE Co. in late 2020. HYBE, a South Korean Entertainment company, founded by Bang Shi Hyuk, is notable for its creation of the current largest boy band in the industy, BTS. With their international Billboard Hot 100 #1 song, Dynamite, releasing in August of 2020, the band a stirred enough interest for the company’s entrance in the market to be a success at $188 USD. The price reached a high in November of 2021 at $306 USD, most likely due to BTS’s Artist of the Year award at the 2021 American Music Awards and their Grammy nomination. However, since that peak, the price has gone down significantly in June of 2022, likely a result of the company’s largest artists, BTS, announcing a hiatus of group activities due to military enlistment “HYBE Shares Rise as World Contemplates Company Sans BTS” (2022). Amonst the remaining tickers, we can a gradual positive trend with no seasonality and a slight spike around early 2022, as artists were announcing in person activities after the Covid-19 pandemic. Thus, we can see through these stock prices that while American record companies seem to a large and consistent history in the market, new companies like HYBE and SM from South Korea have started to take space in the public markets."
  },
  {
    "objectID": "data-visualization.html#billboard-charts",
    "href": "data-visualization.html#billboard-charts",
    "title": "Data Visualization",
    "section": "Billboard Charts:",
    "text": "Billboard Charts:\nThe Billboard Hot 100 is a chart created by billboard used to rank the top 100 songs in the United States per week. In order to see the trends in musical globalization, I will be taking a look artists who acheived number 1’s on the Billboard Hot 100 and the number of weeks those songs stayed on the chart from 2010 - 2021. I used Tableau to show this interactive bubble graph.\n\n\nCode\ncharts &lt;- read_csv('cleaned_data/charts.csv')\ncharts &lt;- charts %&gt;%\n  filter(rank == 1) %&gt;%\n  select(-`last-week`, -`peak-rank`)\n\nwrite.csv(charts, \"cleaned_data/number1_charts.csv\", row.names=FALSE)\n\n\n\n\n\n\n\n\n    \n\n\n\n\nFrom this visualization we notice that first instance of a non-Western artist debuting at number 1 on the Billboard Hot 100 is in 2017 with Luis Fonsi, Daddy Yankee, and Justin Bieber with Despacito. The song stayed on the charts for 78 weeks, being song with the most weeks on the Billboard Hot 100 in this timeframe. The next non-Western artist would be BTS, as mentioned previously, with their song Dynamite, first debuting number one in September of 2020. BTS appears again in June 2021, with their song Butter which stayed on the charts for 15 weeks. Latin Artists Bad Bunny and J Balvin also made a debut on the charts at number 1 with the song I Like It along side Cardi B for a total of 12 weeks on the charts. Thus, we can see that after 2015, we saw an increaing number of number 1’s from non-Western artists on the America charts, a sign of general positive feedback from the public regarding the globalization of music.\n\n\nCode\nselected_artists &lt;- c(\"bts\", \"exo\", \"twice\", \"blackpink\", 'drake', \"beyoncé\", \"beyonce\", \"taylor swift\", \"harry styles\")\n\n# Create a regex pattern to match any of the selected artists\npattern &lt;- paste0(\"\\\\b\", paste(selected_artists, collapse = \"\\\\b|\\\\b\"), \"\\\\b\")\n\n# Subset the dataframe based on selected artists\ncharts &lt;- charts %&gt;%\nmutate(artist = tolower(artist)) %&gt;%\nfilter(str_detect(artist, pattern)) %&gt;%\nmutate(artist = str_extract(artist, paste(selected_artists, collapse = \"|\")))\n\nwrite.csv(charts, \"cleaned_data/subset_artist_charts.csv\", row.names=FALSE)\n\n\n\n\n\n\n\n\n   \n\n\n\n\nHere, we can clearly see that, in terms of the billboard charts, Taylor Swift and Drake dominate in terms of the quantity of songs on the chart per year. Additionally, the only KPOP artist of the four I’ve selected that has made it onto the Billboard Hot 100, as of 2021, was BTS. Other artists like Twice and Blackpink both had 2 etries on the charts in 2022 and 2023 combined."
  },
  {
    "objectID": "data-visualization.html#spotify-data",
    "href": "data-visualization.html#spotify-data",
    "title": "Data Visualization",
    "section": "Spotify Data",
    "text": "Spotify Data\nWhen looking at popularity for all KPOP acts, we can see that throughout the past 10 years, BTS has maintained the highest popularity score, with BlackPink catching up in recent years. Comparatively, with the Western artists, we can see that Beyoncé has consistently been the most popular, with Drake, Taylor Swift, and Harry Styles reaching peaks around the same level in the past two years.\nWhen looking at musical characteristics, we can see that most characteristics between KPOP and Western artists are approximately the same, with the exception of energy, instrumentalness, liveness, loudness, and valence. We can see that in all these values, KPOP seems to have a higher average, indicating that larger, more powerful songs seems to be more common in KPOP than in Western music.\n\nPopularityMusical Characteristics\n\n\n\n\nCode\nspotify &lt;- read.csv(\"cleaned_data/spotify_data_cleaned.csv\")\nspotify &lt;- spotify %&gt;%\n  select(artist_name, album_release_year, popularity) %&gt;%\n  group_by(artist_name, album_release_year) %&gt;%\n  summarise_at(vars(popularity), mean, na.rm = TRUE)\n\nspotify &lt;- spotify %&gt;%\n  mutate(album_release_year_numeric = as.numeric(as.character(album_release_year)))\n\n# Create a sequence of years from debut to 2023 for each artist\nyearly_data &lt;- spotify %&gt;%\n  group_by(artist_name) %&gt;%\n  complete(album_release_year_numeric = full_seq(album_release_year_numeric, 1)) %&gt;%\n  fill(album_release_year, .direction = \"up\") %&gt;%\n  ungroup()\n\n# Step 3: Convert 'album_release_year_numeric' back to a factor\nyearly_data &lt;- yearly_data %&gt;%\n  mutate(album_release_year = as.factor(album_release_year_numeric)) %&gt;%\n  select(-album_release_year_numeric) %&gt;%\n  fill(popularity)\n\nwrite_csv(yearly_data, \"cleaned_data/artist_popularity_cleaned.csv\")\n\nplot &lt;- plot_ly(yearly_data, x = ~album_release_year, y = ~popularity, color = ~artist_name, type = 'scatter', mode = 'lines')\n\n# Combine plot and layout\nplot &lt;- layout(plot, title = \"Average Popularity Over Years by Artist\",\n               xaxis = list(title = \"Album Release Year\"),\n               yaxis = list(title = \"Popularity\"))\n\n# Show the plot\nplot\n\n\n\n\n\n\n\n\n\n\nCode\navg_characteristics_long &lt;- read.csv(\"cleaned_data/artist_avg_characteristics.csv\")\n\nggplot(avg_characteristics_long, aes(x = average_value, y =artist_type, \n                                     fill = artist_type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ musical_characteristic, scales = \"free_x\", nrow = 3, ncol = 3) +\n  labs(title = \"Average Musical Characteristics by Artist Type\",\n       x = \"Average Value\",\n       y = \"Musical Characteristic\") +\n  geom_text(aes(label = round(average_value, 2)), \n            position = position_dodge2(width = 0.9), hjust = 1.2) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(angle = 0, hjust = 1, vjust = -1))"
  },
  {
    "objectID": "data-visualization.html#korean-tourism",
    "href": "data-visualization.html#korean-tourism",
    "title": "Data Visualization",
    "section": "Korean Tourism",
    "text": "Korean Tourism\nFrom the 3 visualizations below we can note the following. There appears to be seasonality within the data, however, we’ll cover that more in the EDA section. For our density plot and boxplot, we can note that there seems to have been more flights in our dataset with fewer passengers, however, the overall trend points to greater travel into the nation.\n\n\nCode\npassengers &lt;- read.csv(\"cleaned_data/air_passengers_cleaned.csv\")\n\npassengers &lt;- passengers %&gt;%\n  mutate(date = as.Date(date))\n\nline_plot &lt;- ggplot(passengers, aes(x = date, y = Passengers)) +\n  geom_line() +\n  labs(title = \"Air Passengers Over Time\",\n       x = \"Year-Month\",\n       y = \"Number of Passengers\")\n\n# Create the histogram\ndensity_plot &lt;- ggplot(passengers, aes(x = Passengers)) +\n  geom_density(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Density Plot of Air Passengers\",\n       x = \"Number of Passengers\",\n       y = \"Density\")\n\n# Create the box plot\nbox_plot &lt;- ggplot(passengers, aes(x = 1, y = Passengers)) +\n  geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Box Plot of Air Passengers\",\n       x = \"\",\n       y = \"Number of Passengers\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\nfacet_wrap_plot &lt;- cowplot::plot_grid(line_plot, density_plot, box_plot)\n\n# Display the facet wrap\nprint(facet_wrap_plot)"
  }
]