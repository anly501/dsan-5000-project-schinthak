---
title: "ARIMAX, SARIMAX, and VAR"
bibliography: intro_reference.bib
---

In order to understand the relationships between the Western Music industry and KPOP, we must take a look at their relationships between the artists. Focusing on KPOP, the biggest record label as of 2023 within the KPOP music industry is HYBE, now an international music company housing the biggest KPOP group, BTS. However, the other notable groups which we'll be focusing on are EXO, Twice, and Black Pink, all of which are signed to other record labels known as SM, JYP, and YG respectfully. In terms of sales and popularity, BTS seems to be far above the other noted groups in their reach into the western music industry, especially of of recent with their total of 5 Grammy nominations (@grammy). Thus, we will use a an ARIMAX model in order to discover what the relationship between other KPOP groups have with BTS and forecast the stock prices of these record labels using this information.

The next relationship we'll analyze is between HYBE and the Western record labels Universal Music and Warner music. These two massive conglomerates make up the majority of the music industry within the west. However, with the recent merger of HYBE with Ithaca Holdings in 2021, there is reason to believe there is now overlap between HYBE and the western industry. Thus, we'll see Universal Music Group and Warner's relationship on HYBE and whether it's significant.

Lastly, we'll take a look at the relationship between globalization and tourism inbound in Korea in order to see whether foreign travel into Korea has a direct correlation within cultural globalization worldwide. This allows us to better understand the significance of KPOP and Korean culture onto other countries, specifically the western market and music industry.

### Key Questions:

1.  What is the relationship between KPOP groups?
2.  What is the relationship between HYBE and the Western industry?
3.  What is the relationship between cultural globalization and Korean tourism?

## (1) The KPOP Record Labels - VAR:

```{r}
#| echo: false
#| warning: false
library(quantmod)
library(tidyverse)
library(imputeTS)
library(vars)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(TSstudio)
library(tidyquant)
library(plotly)
library(ggplot2)
library(TSA)
#install.packages("grDevices")
#library(grDevices)
library(fGarch) 
library(dynlm)
library(dygraphs)
library(readxl)
library(dplyr)
```

Firstly, let's gather the stock data for HYBE, SM Entertainment, YG, and JYP. Once gathered, we will be cleaning the data in order to impute weekends or holidays throughout the year where the stock market is closed. 

```{r}
#| code-fold: true
#| warning: false


options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)

tickers = c("UMGP", "SONY", "352820.KS", "041510.KQ", '122870.KQ', '035900.KQ')

for (i in tickers){
  getSymbols(i, from = "2000-01-01", to = "2023-11-01")
}

UMGP <- data.frame(UMGP$UMGP.Adjusted)
UMGP <- UMGP %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(UMGP_Price = UMGP.Adjusted)

start_date <- as.Date(min(UMGP$Date))  
end_date <- as.Date(max(UMGP$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
UMGP <- merge(UMGP, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- UMGP[which(rowSums(is.na(UMGP)) > 0),]
df_na_cols <- UMGP[, which(colSums(is.na(UMGP)) > 0)]
imputed_time_series <- na_ma(UMGP, k = 4, weighting = "exponential")
UMGP <- data.frame(imputed_time_series)

#---

SONY <- data.frame(SONY$SONY.Adjusted)
SONY <- SONY %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(SONY_Price = SONY.Adjusted)


start_date <- as.Date(min(SONY$Date))  
end_date <- as.Date(max(SONY$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
SONY <- merge(SONY, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- SONY[which(rowSums(is.na(SONY)) > 0),]
df_na_cols <- SONY[, which(colSums(is.na(SONY)) > 0)]
imputed_time_series <- na_ma(SONY, k = 4, weighting = "exponential")
SONY <- data.frame(imputed_time_series)

#---

HYBE <- data.frame(`352820.KS`$`352820.KS.Adjusted`)
HYBE <- HYBE %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(HYBE_Price = X352820.KS.Adjusted) %>%
  mutate(HYBE_Price = HYBE_Price/1352.60)

start_date <- as.Date(min(HYBE$Date))  
end_date <- as.Date(max(HYBE$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
HYBE <- merge(HYBE, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- HYBE[which(rowSums(is.na(HYBE)) > 0),]
df_na_cols <- HYBE[, which(colSums(is.na(HYBE)) > 0)]
imputed_time_series <- na_ma(HYBE, k = 4, weighting = "exponential")
HYBE <- data.frame(imputed_time_series)

#--- 

SM <- data.frame(`041510.KQ`$`041510.KQ.Adjusted`)
SM <- SM %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(SM_Price = X041510.KQ.Adjusted) %>%
  mutate(SM_Price = SM_Price/1352.60)

start_date <- as.Date(min(SM$Date))  
end_date <- as.Date(max(SM$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
SM <- merge(SM, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- SM[which(rowSums(is.na(SM)) > 0),]
df_na_cols <- SM[, which(colSums(is.na(SM)) > 0)]
imputed_time_series <- na_ma(SM, k = 4, weighting = "exponential")
SM <- data.frame(imputed_time_series)

#---

YG <- data.frame(`122870.KQ`$`122870.KQ.Adjusted`)
YG <- YG %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(YG_Price = X122870.KQ.Adjusted) %>%
  mutate(YG_Price = YG_Price/1352.60)

start_date <- as.Date(min(YG$Date))  
end_date <- as.Date(max(YG$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
YG <- merge(YG, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- YG[which(rowSums(is.na(YG)) > 0),]
df_na_cols <- YG[, which(colSums(is.na(YG)) > 0)]
imputed_time_series <- na_ma(YG, k = 4, weighting = "exponential")
YG <- data.frame(imputed_time_series)

#---

JYP <- data.frame(`035900.KQ`$`035900.KQ.Adjusted`)
JYP <- JYP %>%
  rownames_to_column(var = "Date") %>%
  mutate(Date = as.Date(Date)) %>%
  rename(JYP_Price = X035900.KQ.Adjusted) %>%
  mutate(JYP_Price = JYP_Price/1352.60)

start_date <- as.Date(min(JYP$Date))  
end_date <- as.Date(max(JYP$Date))    
date_range <- seq(start_date, end_date, by = "1 day")
date_dataset <- data.frame(Date = date_range)
JYP <- merge(JYP, date_dataset, by = 'Date', all = TRUE)
df_na_rows <- JYP[which(rowSums(is.na(JYP)) > 0),]
df_na_cols <- JYP[, which(colSums(is.na(JYP)) > 0)]
imputed_time_series <- na_ma(JYP, k = 4, weighting = "exponential")
JYP <- data.frame(imputed_time_series)

stock_dataframes <- list(UMGP, SONY, HYBE, SM, YG, JYP)
stock_names <- list("UMGP", "SONY", "HYBE", "SM", "YG", "JYP")

#Creating a subset of only Korean Record label stock data
df <- HYBE %>%
  left_join(SM, by = 'Date') %>%
  left_join(YG, by = 'Date') %>%
  left_join(JYP, by = 'Date')
```

### Converting to Time Series

Next, we'll take all the KPOP entertainment companies' stock prices and convert them into time series objects. 
```{r}
#| code-fold: true
hybe <- ts(df$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)
sm <- ts(df$SM_Price, start = as.Date('2020-10-15'), freq = 365.25)
yg <- ts(df$YG_Price, start = as.Date('2020-10-15'), freq = 365.25)
jyp <- ts(df$JYP_Price, start = as.Date('2020-10-15'), freq = 365.25)

df_ts <- cbind(hybe, sm, yg, jyp)
colnames(df_ts) <- c("hybe", "sm", "yg", "jyp")
```

### Visualizing the data:
```{r}
#| code-fold: true
autoplot(df_ts)
```

As we previously mentioned in data visualization, HYBE seems to have a much larger impact in comparison to the other three record companies on the stock market overall. However, what we can see is that several of the positive trends shown through all stock prices, thus we can note some initial correlation. Let's continue with the VAR model to see what the multivariate relationship is. 

### VARselect
```{r}
#| code-fold: true
VARselect(df_ts, lag.max=10, type="both")
```

We can see that the p-values detected from VARselect() are 5 and 1. 

### Initial selection: 
```{r}
#| code-fold: true
#| warning: false
summary(vars::VAR(df_ts, p=1, type='both'))
summary(vars::VAR(df_ts, p=5, type='both'))
```


We can see that based on the residual standard error and number of significant variables in the model, we can say that the model when p=5 performs better than when p=1. We can also notice that while HYBE and SM don't see to have much correlation with other agencies, JYP and YG seem to be heavily correlated with each other and SM. 

Thus, before we continue with the model, we will also verify through a CV test. 

### Cross Validation: 
```{r}
#| code-fold: true
#| warning: true


folds = 5 
best_model <- NULL
best_performance <- Inf 

fold_s <- floor(nrow(df_ts)/folds)

for(fold in 1:folds){
  start <- (fold-1)*fold_s+1
  end <- fold*fold_s
  
  train_model <- df_ts[-(start:end), ]
  test_model <- df_ts[start:end, ]
  
  sel <- VARselect(train_model, lag.max = 10, type = "both")
  best_lag <- sel$selection[1]
  
  fit <- vars::VAR(train_model, p=best_lag, type= "both", season = NULL, exog = NULL)
  
  h <- nrow(test_model)
  pred <- predict(fit, n.ahead = h)
  
  pred_hybe <- pred$fcst$hybe[,1]
  mse <- mean((pred_hybe - test_model[, "hybe"])^2)
  
  if(mse < best_performance){
    best_model <- fit
    best_performance <- mse
  }
}

print("The best model is: ")
print(best_model)
```

The results from the CV test show that a model of p = 2 is the best at predicting HYBE stock prices in relation to SM, YG, and JYP. Since cross validation is a more accurate model selection technique, we will create both models where p=5,2. 

### Model Creation: 
```{r}
#| code-fold: true

var_model_1 <- vars::VAR(df_ts, p=2, type= "both", season = NULL, exog = NULL)
gu.serial <- serial.test(var_model_1, lags.pt = 12, type = "PT.asymptotic") 
gu.serial
plot(gu.serial, names = "hybe") 
plot(gu.serial, names = "sm")
plot(gu.serial, names = "jyp") 
plot(gu.serial, names = "yg")

#--

var_model_2 <- vars::VAR(df_ts, p=5, type= "both", season = NULL, exog = NULL)
gu.serial <- serial.test(var_model_2, lags.pt = 12, type = "PT.asymptotic") 
gu.serial
plot(gu.serial, names = "hybe") 
plot(gu.serial, names = "sm")
plot(gu.serial, names = "jyp") 
plot(gu.serial, names = "yg")

```

Based on the p-values, we can say that the model where p=2 has a much lower p-value, indicating that there is no serial correlation within the model. Thus, we will choose this model to forecast HYBE prices in relation to other KPOP agencies. 

### Forecasting: 
```{r}
#| cold-fold: true
par(mar=c(1,2,3,1))
var_model_1 <- vars::VAR(df_ts, p=2, type= "both", season = NULL, exog = NULL)

fit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)
fanchart(fit.pr)
```

Thus, from our forecasting, we can see that SM, JYP, and YG all are similar in that the so a contextually upward trend of similar magnitude. Additionally, we see a downward trend for HYBE in the next year with a larger variance within the prediction. 


## (2) KPOP and the Western industry - VAR:

Similarly, we'll take look now at how or if the Western music industry has had a relation with the growth and sucess of HYBE entertainment. As we see the blend of the two industries within HYBE's artist roster, we will also need to use the techinques of VAR models to identify correlations between all three entertainment companies in order to properly forecast all three. 

We'll follow the same steps as before the get some initial p values from VARselect(). 

```{r}
#| code-fold: true
#| warning: false


#Creating a subset of only Korean Record label stock data
df2 <- HYBE %>%
  left_join(UMGP, by = 'Date') %>%
  left_join(SONY, by = 'Date') %>%
  drop_na()

hybe <- ts(df2$HYBE_Price, start = as.Date('2020-10-15'), freq = 365.25)
umgp <- ts(df2$UMGP_Price, start = as.Date('2020-10-15'), freq = 365.25)
sony <- ts(df2$SONY_Price, start = as.Date('2020-10-15'), freq = 365.25)

df2_ts <- cbind(hybe, umgp, sony)
colnames(df2_ts) <- c("hybe", "umgp", "sony")

autoplot(df2_ts)
```

From an initial visualization, it doesn't appear that there is correlation between any of these stock prices, simply because the trends are so vastly different. HYBE, compared to the other stock prices, seems much more volatile, which makes it difficult to predict its forecasted prices. Thus, we'll continue with the VAR model to work on forecasting.

### VARselect
```{r}
#| code-fold: true
VARselect(df2_ts, lag.max=10, type="both")
```

Here, we can see that VARselect() chose p=5,1, similar to the relation between KPOP agencies. Let's continue by analyzing the residuals squared errors. 

### Initial selection: 
```{r}
#| code-fold: true
#| warning: false
summary(vars::VAR(df2_ts, p=1, type='both'))
summary(vars::VAR(df2_ts, p=5, type='both'))
```

From the residual squared errors and significance values, we can see that both models are very similar. The error on UMGP and SONY are very low, however the error for HYBE is larger at at approximately 4. Thus, we'll continue model selection through cross validation. 

### Cross Validation: 
```{r}
folds = 5 
best_model <- NULL
best_performance <- Inf 

fold_s <- floor(nrow(df2_ts)/folds)

for(fold in 1:folds){
  start <- (fold-1)*fold_s+1
  end <- fold*fold_s
  
  train_model <- df2_ts[-(start:end), ]
  test_model <- df2_ts[start:end, ]
  
  sel <- VARselect(train_model, lag.max = 10, type = "both")
  best_lag <- sel$selection[1]
  
  fit <- vars::VAR(train_model, p=best_lag, type= "both", season = NULL, exog = NULL)
  
  h <- nrow(test_model)
  pred <- predict(fit, n.ahead = h)
  
  pred_hybe <- pred$fcst$hybe[,1]
  mse <- mean((pred_hybe - test_model[, "hybe"])^2)
  
  if(mse < best_performance){
    best_model <- fit
    best_performance <- mse
  }
}

print("The best model is: ")
print(best_model)
```

CV seems to have chosen a different model where p=8. Thus, we'll create models for p=1,5,8. 

### Model Creation: 
```{r}
#| code-fold: true

var_model_1 <- vars::VAR(df2_ts, p=1, type= "both", season = NULL, exog = NULL)
gu.serial <- serial.test(var_model_1, lags.pt = 12, type = "PT.asymptotic") 
gu.serial
plot(gu.serial, names = "hybe") 
plot(gu.serial, names = "umgp")
plot(gu.serial, names = "sony") 

#--

var_model_2 <- vars::VAR(df2_ts, p=5, type= "both", season = NULL, exog = NULL)
gu.serial <- serial.test(var_model_2, lags.pt = 12, type = "PT.asymptotic") 
gu.serial
plot(gu.serial, names = "hybe") 
plot(gu.serial, names = "umgp")
plot(gu.serial, names = "sony")

#--

var_model_3 <- vars::VAR(df2_ts, p=8, type= "both", season = NULL, exog = NULL)
gu.serial <- serial.test(var_model_3, lags.pt = 12, type = "PT.asymptotic") 
gu.serial
plot(gu.serial, names = "hybe") 
plot(gu.serial, names = "umgp")
plot(gu.serial, names = "sony")

```

Based on the p-values and ACF plots of the residuals, the model where p=5 seems to be the best model for forecasting. the residuals are not correlated and the p-value is significant as it is 0.01918 < 0.05. 

### Forecasting:
```{r}
#| cold-fold: true
par(mar=c(1,2,3,1))
var_model_1 <- vars::VAR(df2_ts, p=5, type= "both", season = NULL, exog = NULL)

fit.pr <- predict(var_model_1, n.ahead = 365, ci = 0.95)
fanchart(fit.pr)
```
From this forecasting into the next year, we can see a strong negative trend for both HYBE and SONY, while UMGP's stock price remains approximately constant. This prediction is similar to what we found from the previous model, such that HYBE will be experiencing a downward trend in prices for the upcoming year. This may be due to a number of reasons, however, most notably would be that their most successful artist, BTS, are continuing their hiatus as the members of the group complete their mandatory military service in South Korea. 

Knowing this downward trend in the stock prices of the biggest performing KPOP music agency, we can start to see a downward shift in KPOP among investors globally. Thus, we may need to discuss the direction of cultural globalization in relation to South Korea. 


## (3) - Foreign tourism in Korea on Cultural Globalization in the USA 

Let's see if the cultural globalization index in relation to tourism in South Korea will be trending downward in relation to our previous forecasting. 

### Gathering the Data

We'll combine the globalization index data from KOF with the South Korean tourism data from Statistica. 
```{r}
#| code-fold: true
#| warning: false
#| echo: false

library(readxl)
library(dplyr)

global <- read_csv('globalization.csv')
global <- global %>%
  filter(code == "USA") %>%
  dplyr::select(year, KOFCuGIdf)

tourism <- read_xlsx('tourism.xlsx', sheet = 'Data')


by <- join_by(Year == year)
df3 <- tourism %>%
  left_join(global, by = by) %>%
  rename(tourists = `Number of visitor arrivals in South Korea`) %>%
  mutate(tourists = 1000000*tourists) %>%
  drop_na()
```
As discussed previously, we will be modeling the cultural globalization index quantified by KOF within the United States in conjunction with tourism with South Korea throughout the 21st century. As we are focusing on KPOP's influence within the United States, an integral part of globalization and cultural exchange is through tourism. Thus, looking at the relationship between tourism into South Korea and global culture in the United States will further help to understand this exchange in culture. 

```{r}
#| code-fold: true
global_ts <-ts(df3, start = 2000, frequency = 1)

autoplot(global_ts[,c(2:3)], facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Cultural Globalization in USA and Tourism in South Korea")
```

From the graphs above, we can see a similar positive trend between both the globalization index and tourists entering South Korea. However, tourism takes a sharp downward trend in 2020. This is, of course, due to the COVID-19 global pandemic that prevented all travel into South Korea from foreigners. Since this data point is an anomaly to determine cultural trends, will continue this model without 2020. 

```{r}
#| echo: false
df3 <- df3 %>% slice(-n())
global_ts <-ts(df3, start = 2000, frequency = 1)
```

### Using Auto.Arima()

Now, let's move on with the ARIMAX/ARMAX model. First, we'll create a model using auto.arima(). 

```{r}
#| code-fold: true
fit <- auto.arima(global_ts[, "KOFCuGIdf"], xreg = global_ts[, "tourists"])
summary(fit)
checkresiduals(fit)
```

Based on the summary statistics of the model created, auto.arima() created the model ARMA(2,0). Additionally, there is no cross correlation in the residuals and the p-value based in the Ljung-Box test is significant. 


### Manually Finding the Model: 

We'll move now to find the ARMAX model manually. Let's start by taking creating a regression model of tourism on cultural globalization. Using that model, we'll take the residuals and test multiple Arima models in order to find the one with the lowest AIC and BIC values. From there, after analyzing the residuals and significance of the variables, we'll validate the model through cross validation. 

```{r}
#| code-fold: true
#| warning: false

df3$tourists <-ts(df3$tourists, start= 2000, frequency = 1)
df3$KOFCuGIdf <-ts(df3$KOFCuGIdf, start= 2000, frequency = 1)

############# First fit the linear model##########
fit.reg <- lm(KOFCuGIdf ~ tourists, data = df3)
summary(fit.reg)
```

```{r}
res.fit<-ts(residuals(fit.reg), start= 2000, frequency = 1)
ggAcf(res.fit)
ggPacf(res.fit)
```
From the residuals, we can see that there is no cross correlation between the residuals within the ACF plot. Thus, we can move on to manually simulating ARMA models, since we do not need to difference the data. 

```{r}
#| code-fold: true
#| warning: false
d=0
i=1
temp= data.frame()
ls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)


for (p in 0:4)# p=0,1,2,3,4 : 5
{
  for(q in 0:4)# q=0,1,2,3,4 :5
  {
    model<- Arima(res.fit, order=c(p,d,q),include.drift=TRUE) 
    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

output= as.data.frame(ls)
names(output)= c("p","d","q","AIC","BIC","AICc")

#temp
knitr::kable(output)
```

```{r}
#| code-fold: true
#| warning: false
output[which.min(output$AIC),] 
output[which.min(output$BIC),] 
output[which.min(output$AICc),]
```

From the manual process, we can see the models produced with the lowest AIC and BIC values are ARMA(2,2) and ARMA(1,0). Thus, we'll take a look at the residuals of the following models:

```{r}
#| code-fold: true
#| warning: false
capture.output(sarima(res.fit, 1,0,0)) 
capture.output(sarima(res.fit, 2,0,2)) 
```
From the following residual plots, we can say that model ARMA(1,0) is the better of the two models due to the lack of cross correlation between the residuals. However, we'll move onto cross validation in order to determine which of the ARMAX models are the best for forecasting. 


### CV

```{r}
#| code-fold: true
#| warning: false
n <- length(res.fit)
k <- 5  # Assuming 5 is the maximum number of observations for testing

rmse1 <- matrix(NA, 15)
rmse2 <- matrix(NA, 15)
rmse3 <- matrix(NA, 15)

st <- tsp(res.fit)[1] + (k - 1)

for (i in 1:15) {
  # Define the training set
  train_end <- st + i - 1
  xtrain <- window(res.fit, end = train_end)

  # Define the testing set
  test_start <- train_end + 1
  test_end <- min(st + i, tsp(res.fit)[2])
  xtest <- window(res.fit, start = test_start, end = test_end)

  fit <- Arima(xtrain, order = c(1, 0, 0), include.drift = TRUE, method = "ML")
  fcast <- forecast(fit, h = 4)

  fit2 <- Arima(xtrain, order = c(2, 0, 0), include.drift = TRUE, method = "ML")
  fcast2 <- forecast(fit2, h = 4)

  fit3 <- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = "ML")
  fcast3 <- forecast(fit3, h = 4)

  rmse1[i] <- sqrt((fcast$mean - xtest)^2)
  rmse2[i] <- sqrt((fcast2$mean - xtest)^2)
  rmse3[i] <- sqrt((fcast3$mean - xtest)^2)
}

plot(1:15, rmse2, type = "l", col = 2, xlab = "horizon", ylab = "RMSE")
lines(1:15, rmse1, type = "l", col = 3)
lines(1:15, rmse3, type = "l", col = 4)
legend("topleft", legend = c("fit2", "fit1", "fit3"), col = 2:4, lty = 1)

```
From the cross validation function, we can see that model ARMA(1, 0) is the best model given that the RMSE values are the lowest across the cross folds. Thus, we'll choose to forecast Korean tourism on cultural globalization in the US via model 1. 

```{r}
#| code-fold: true
#| warning: false
fit <- Arima(global_ts[, "KOFCuGIdf"], order=c(1,0,0), xreg = global_ts[, "tourists"])
summary(fit)
```

### Forecasting: 

```{r}
#| code-fold: true
#| warning: false
tourists_fit <-auto.arima(global_ts[, "tourists"]) 
summary(tourists_fit)

ft<-forecast(tourists_fit)

fcast <- forecast(fit, xreg=ft$mean)
autoplot(fcast) + xlab("Year") +
  ylab("Globalization")
```

We can see that in the next 10 years, globalization within the US with regards to Korea's tourism of foreigners will see a slight decrease. As we've observed in out previous VAR models, this may be due to an incoming disinterest in KPOP as famous groups such as BTS step away from music in the near future and new groups unable to make a significant impact on the Western music industry as BTS has done. 